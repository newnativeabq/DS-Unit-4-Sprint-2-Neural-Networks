diff --git a/LS_DS_Unit_4_Sprint_Challenge_2.ipynb b/LS_DS_Unit_4_Sprint_Challenge_2.ipynb
index 74a5653..455d13f 100644
--- a/LS_DS_Unit_4_Sprint_Challenge_2.ipynb
+++ b/LS_DS_Unit_4_Sprint_Challenge_2.ipynb
@@ -1,431 +1,1480 @@
 {
-  "cells": [
-    {
-      "cell_type": "markdown",
-      "source": [
-        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
-        "<br></br>\n",
-        "<br></br>\n",
-        "\n",
-        "## *Data Science Unit 4 Sprint 2*\n",
-        "\n",
-        "# Sprint Challenge - Neural Network Foundations\n",
-        "\n",
-        "Table of Problems\n",
-        "\n",
-        "1. [Defining Neural Networks](#Q1)\n",
-        "2. [Chocolate Gummy Bears](#Q2)\n",
-        "    - Perceptron\n",
-        "    - Multilayer Perceptron\n",
-        "4. [Keras MMP](#Q3)"
-      ],
-      "metadata": {}
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
+    "<br></br>\n",
+    "<br></br>\n",
+    "\n",
+    "## *Data Science Unit 4 Sprint 2*\n",
+    "\n",
+    "# Sprint Challenge - Neural Network Foundations\n",
+    "\n",
+    "Table of Problems\n",
+    "\n",
+    "1. [Defining Neural Networks](#Q1)\n",
+    "2. [Chocolate Gummy Bears](#Q2)\n",
+    "    - Perceptron\n",
+    "    - Multilayer Perceptron\n",
+    "4. [Keras MMP](#Q3)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "<a id=\"Q1\"></a>\n",
+    "## 1. Define the following terms:\n",
+    "\n",
+    "- **Neuron:  A node in network consisting of a set of weights to be applied to inputs and optionally a transform (activation function) dictating output.\n",
+    "- **Input Layer:  The processed data layer - often an unweighted ingest of normalized/scaled data to the first hidden layer.\n",
+    "- **Hidden Layer:  A layer that lies between the input and output layers.\n",
+    "- **Output Layer:  A final set of nodes in the shape of desired output.  May use alternate activation functions to get explicity output ranges.\n",
+    "- **Activation:  Output transformation at the neuron level - f(x) where x = sum(weights*inputs). \n",
+    "- **Backpropagation:  Propogation of errors from the output layer backward through the network calculating the partial dependence on weights at each layer/node.  The backbone of neural networks. \n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
+    "\n",
+    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
+    "\n",
+    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
+    "\n",
+    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
+    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {
+    "collapsed": false,
+    "inputHidden": false,
+    "jupyter": {
+     "outputs_hidden": false
     },
-    {
-      "cell_type": "markdown",
-      "source": [
-        "<a id=\"Q1\"></a>\n",
-        "## 1. Define the following terms:\n",
-        "\n",
-        "- **Neuron:**\n",
-        "- **Input Layer:**\n",
-        "- **Hidden Layer:**\n",
-        "- **Output Layer:**\n",
-        "- **Activation:**\n",
-        "- **Backpropagation:**\n"
-      ],
-      "metadata": {}
+    "outputHidden": false
+   },
+   "outputs": [],
+   "source": [
+    "import pandas as pd\n",
+    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {
+    "collapsed": false,
+    "inputHidden": false,
+    "jupyter": {
+     "outputs_hidden": false
     },
+    "outputHidden": false
+   },
+   "outputs": [
     {
-      "cell_type": "markdown",
-      "source": [
-        "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
-        "\n",
-        "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
-        "\n",
-        "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
-        "\n",
-        "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
-        "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>chocolate</th>\n",
+       "      <th>gummy</th>\n",
+       "      <th>ate</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>3</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>4</td>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
       ],
-      "metadata": {}
-    },
+      "text/plain": [
+       "   chocolate  gummy  ate\n",
+       "0          0      1    1\n",
+       "1          1      0    1\n",
+       "2          0      1    1\n",
+       "3          0      0    0\n",
+       "4          1      1    0"
+      ]
+     },
+     "execution_count": 2,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "candy.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 43,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "code",
-      "source": [
-        "import pandas as pd\n",
-        "candy = pd.read_csv('chocolate_gummy_bears.csv')"
-      ],
-      "outputs": [],
-      "execution_count": 3,
-      "metadata": {
-        "collapsed": false,
-        "inputHidden": false,
-        "outputHidden": false
-      }
+     "data": {
+      "text/plain": [
+       "(0.5,           chocolate         gummy           ate\n",
+       " count  10000.000000  10000.000000  10000.000000\n",
+       " mean       0.499100      0.499300      0.500000\n",
+       " std        0.500024      0.500025      0.500025\n",
+       " min        0.000000      0.000000      0.000000\n",
+       " 25%        0.000000      0.000000      0.000000\n",
+       " 50%        0.000000      0.000000      0.500000\n",
+       " 75%        1.000000      1.000000      1.000000\n",
+       " max        1.000000      1.000000      1.000000)"
+      ]
+     },
+     "execution_count": 43,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "## Majority Class\n",
+    "\n",
+    "candy.ate.mean(), candy.describe()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Perceptron\n",
+    "\n",
+    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
+    "\n",
+    "Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 31,
+   "metadata": {
+    "collapsed": false,
+    "inputHidden": false,
+    "jupyter": {
+     "outputs_hidden": false
     },
+    "outputHidden": false
+   },
+   "outputs": [],
+   "source": [
+    "# Start your candy perceptron here\n",
+    "\n",
+    "X = candy[['chocolate', 'gummy']].values\n",
+    "\n",
+    "y = candy['ate'].values.reshape(-1,1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 32,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "code",
-      "source": [
-        "candy.head()"
-      ],
-      "outputs": [
-        {
-          "output_type": "execute_result",
-          "execution_count": 4,
-          "data": {
-            "text/plain": [
-              "   chocolate  gummy  ate\n",
-              "0          0      1    1\n",
-              "1          1      0    1\n",
-              "2          0      1    1\n",
-              "3          0      0    0\n",
-              "4          1      1    0"
-            ],
-            "text/html": [
-              "<div>\n",
-              "<style scoped>\n",
-              "    .dataframe tbody tr th:only-of-type {\n",
-              "        vertical-align: middle;\n",
-              "    }\n",
-              "\n",
-              "    .dataframe tbody tr th {\n",
-              "        vertical-align: top;\n",
-              "    }\n",
-              "\n",
-              "    .dataframe thead th {\n",
-              "        text-align: right;\n",
-              "    }\n",
-              "</style>\n",
-              "<table border=\"1\" class=\"dataframe\">\n",
-              "  <thead>\n",
-              "    <tr style=\"text-align: right;\">\n",
-              "      <th></th>\n",
-              "      <th>chocolate</th>\n",
-              "      <th>gummy</th>\n",
-              "      <th>ate</th>\n",
-              "    </tr>\n",
-              "  </thead>\n",
-              "  <tbody>\n",
-              "    <tr>\n",
-              "      <th>0</th>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>1</th>\n",
-              "      <td>1</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>2</th>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>3</th>\n",
-              "      <td>0</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>4</th>\n",
-              "      <td>1</td>\n",
-              "      <td>1</td>\n",
-              "      <td>0</td>\n",
-              "    </tr>\n",
-              "  </tbody>\n",
-              "</table>\n",
-              "</div>"
-            ]
-          },
-          "metadata": {}
-        }
-      ],
-      "execution_count": 4,
-      "metadata": {
-        "collapsed": false,
-        "outputHidden": false,
-        "inputHidden": false
-      }
-    },
+     "data": {
+      "text/plain": [
+       "(numpy.ndarray, (10000, 2), (10000, 1))"
+      ]
+     },
+     "execution_count": 32,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "type(X), X.shape, y.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 90,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Custom implementation of flexible dense network with numpy\n",
+    "import pandas as pd\n",
+    "import numpy as np\n",
+    "\n",
+    "def gen_random_matrix(shape):\n",
+    "    np.random.seed()\n",
+    "    return np.random.rand(shape[0], shape[1])\n",
+    "\n",
+    "# A lot of thought needs to go setting up the nodes.  Creating their size programatically\n",
+    "#  makes a lot of assumptions about hidden layer size (could be random, arbitrary, etc.).\n",
+    "#  One option would be to add a 'auto' feature that created hidden layers of X.shape + C\n",
+    "#  weights. Another would be to create pass a distribution and have the layer be a generator \n",
+    "#  of sorts whose parameters can be optimized.\n",
+    "\n",
+    "example_network_description = (\n",
+    "    ('input', X),  # row 0 must be input\n",
+    "    ('hidden_1', (X.shape[1], 3), 'simple random'),  # hidden vectors must match input vec\n",
+    "    ('hidden_2', (3, 7), 'simple random'),\n",
+    "    ('output', (7,1), 'simple random'),  # final active row must be output vector. match last hidden vec\n",
+    "    ('target', y) # last row in description must be the target vector\n",
+    ")\n",
+    "\n",
+    "\n",
+    "class LayerFactory():\n",
+    "    def __init__(self):\n",
+    "        return\n",
+    "    \n",
+    "    def initialize_weights(self, shape, algorithm):\n",
+    "        \"\"\"\n",
+    "        Lookup available weight formulas and generate pseudo-random numbers for initial weights\n",
+    "        of specified shape.\n",
+    "        \n",
+    "        \"\"\"\n",
+    "        available_generators = {\n",
+    "            'simple random': gen_random_matrix,\n",
+    "        }\n",
+    "        \n",
+    "        return available_generators[algorithm](shape)\n",
+    "    \n",
+    "    def generate_layers(self, description):\n",
+    "        \"\"\"\n",
+    "        Generate layers based on network description.\n",
+    "        \n",
+    "        ====Parameters====\n",
+    "        description: tuple or list object of layer descriptions ('name', shape)\n",
+    "        \"\"\"\n",
+    "        layers = {}\n",
+    "        layers[0] = Layer()\n",
+    "        layers[0].activated_values = description[0][1]\n",
+    "        for count, row in enumerate(description):\n",
+    "            if row[0] == 'target':\n",
+    "                layers[count] = Layer()\n",
+    "                layers[count].activated_values = description[len(description)-1][1]\n",
+    "                \n",
+    "            elif row[0] != 'input':\n",
+    "                layers[count] = Layer()\n",
+    "                layers[count].weights = self.initialize_weights(shape=row[1], algorithm=row[2])\n",
+    "                layers[count].weighted_sum = 0\n",
+    "                layers[count].activated_values = 0\n",
+    "            \n",
+    "        \n",
+    "        return layers\n",
+    "    \n",
+    "    \n",
+    "class Layer():\n",
+    "    pass\n",
+    "    \n",
+    "\n",
+    "class NeuralNetwork(LayerFactory):\n",
+    "    def __init__(self, description):\n",
+    "        # Set up Architecture of Neural Network\n",
+    "        self.description = description\n",
+    "        self.layers = self.generate_layers(description)\n",
+    "        \n",
+    "    def sigmoid(self, weighted_sum):\n",
+    "        return 1 / (1+np.exp(-weighted_sum))\n",
+    "    \n",
+    "    def sigmoidPrime(self, weighted_sum):\n",
+    "        return weighted_sum * (1 - weighted_sum)\n",
+    "    \n",
+    "    def feed_forward(self, X):\n",
+    "        \"\"\"\n",
+    "        Calculate the NN inference using feed forward.\n",
+    "        aka \"predict\"\n",
+    "        \"\"\"\n",
+    "        for i in range(1, len(self.layers)-1):\n",
+    "            # Weighted sum of inputs\n",
+    "            #  Check if first layer (required to use feed_forward method as Predict)\n",
+    "            if i == 1:\n",
+    "                self.layers[i].weighted_sum = np.dot(X, self.layers[i].weights)\n",
+    "                # Activated values (local outputs)\n",
+    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
+    "            else:\n",
+    "                self.layers[i].weighted_sum = np.dot(self.layers[i-1].activated_values, self.layers[i].weights)\n",
+    "                # Activated values (local outputs)\n",
+    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
+    "\n",
+    "        return self.layers[len(self.layers)-2].activated_values\n",
+    "        \n",
+    "    def backward(self, X, y, net_output, learning_rate):\n",
+    "        \"\"\"\n",
+    "        Backward propagate through the network\n",
+    "        \"\"\"\n",
+    "        # Step 1: Calculate errors and delta shifts for each layer (backward)\n",
+    "        back_prop_pos = 0\n",
+    "        for i in range(len(self.layers)-2, 0, -1):\n",
+    "            # Error in local output\n",
+    "            #   Check if first backprop\n",
+    "            if back_prop_pos == 0:\n",
+    "                self.layers[i].error = y - net_output\n",
+    "                # Apply Derivative of Sigmoid to error\n",
+    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(net_output) * learning_rate\n",
+    "            else:\n",
+    "                self.layers[i].error = self.layers[i+1].delta.dot(self.layers[i+1].weights.T)\n",
+    "                # Apply Derivative of Sigmoid to error\n",
+    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(\n",
+    "                    self.layers[i].activated_values)\n",
+    "                \n",
+    "            back_prop_pos += 1\n",
+    "            \n",
+    "        # Step 2: Calculate adjustments and apply to each layer (forward)\n",
+    "        for i in range(1, len(self.layers)-1):\n",
+    "            self.layers[i].weights += self.layers[i-1].activated_values.T.dot(self.layers[i].delta)\n",
+    "        \n",
+    "    def train(self, X, y, learning_rate):\n",
+    "        net_output = self.feed_forward(X)\n",
+    "        self.backward(X, y, net_output, learning_rate)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 49,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "markdown",
-      "source": [
-        "### Perceptron\n",
-        "\n",
-        "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
-        "\n",
-        "Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "+---------EPOCH 1---------+\n",
+      "Weights: \n",
+      " [[0.47432446]\n",
+      " [0.16792762]]\n",
+      "Loss: \n",
+      " 0.2590575177989399\n",
+      "+---------EPOCH 5---------+\n",
+      "Weights: \n",
+      " [[-13.89552077]\n",
+      " [ -8.49463178]]\n",
+      "Loss: \n",
+      " 0.5487530406363103\n",
+      "+---------EPOCH 1000---------+\n",
+      "Weights: \n",
+      " [[-19.54241453]\n",
+      " [  2.8912209 ]]\n",
+      "Loss: \n",
+      " 0.32526107670787996\n",
+      "+---------EPOCH 2000---------+\n",
+      "Weights: \n",
+      " [[-19.54082341]\n",
+      " [  2.89122091]]\n",
+      "Loss: \n",
+      " 0.32526107670281657\n",
+      "+---------EPOCH 3000---------+\n",
+      "Weights: \n",
+      " [[-19.53922975]\n",
+      " [  2.89122091]]\n",
+      "Loss: \n",
+      " 0.3252610766977371\n",
+      "+---------EPOCH 4000---------+\n",
+      "Weights: \n",
+      " [[-19.53763355]\n",
+      " [  2.89122091]]\n",
+      "Loss: \n",
+      " 0.32526107669264137\n",
+      "+---------EPOCH 5000---------+\n",
+      "Weights: \n",
+      " [[-19.5360348 ]\n",
+      " [  2.89122091]]\n",
+      "Loss: \n",
+      " 0.3252610766875294\n",
+      "+---------EPOCH 6000---------+\n",
+      "Weights: \n",
+      " [[-19.53443349]\n",
+      " [  2.89122091]]\n",
+      "Loss: \n",
+      " 0.325261076682401\n",
+      "+---------EPOCH 7000---------+\n",
+      "Weights: \n",
+      " [[-19.53282961]\n",
+      " [  2.89122092]]\n",
+      "Loss: \n",
+      " 0.3252610766772561\n",
+      "+---------EPOCH 8000---------+\n",
+      "Weights: \n",
+      " [[-19.53122315]\n",
+      " [  2.89122092]]\n",
+      "Loss: \n",
+      " 0.32526107667209464\n",
+      "+---------EPOCH 9000---------+\n",
+      "Weights: \n",
+      " [[-19.5296141 ]\n",
+      " [  2.89122092]]\n",
+      "Loss: \n",
+      " 0.3252610766669166\n",
+      "+---------EPOCH 10000---------+\n",
+      "Weights: \n",
+      " [[-19.52800247]\n",
+      " [  2.89122092]]\n",
+      "Loss: \n",
+      " 0.3252610766617218\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Describe single perceptron network\n",
+    "single_layer =(\n",
+    "    ('input', X),  # row 0 must be input\n",
+    "    ('output', (X.shape[1], 1), 'simple random'),  # shape of first layer of (input_dim, #nodes)\n",
+    "    ('target', y) # last row in description must be the target vector\n",
+    ")\n",
+    "\n",
+    "nn_singlet = NeuralNetwork(single_layer)\n",
+    "\n",
+    "# Number of Epochs / Iterations\n",
+    "for i in range(10000):\n",
+    "    if (i+1 in [1, 5]) or ((i+1) % 1000 ==0):\n",
+    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
+    "        print('Weights: \\n', nn_singlet.layers[1].weights)\n",
+    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn_singlet.feed_forward(X)))))\n",
+    "    nn_singlet.train(X, y, 0.1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 52,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "0.7229"
+      ]
+     },
+     "execution_count": 52,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# Check actual accuracy\n",
+    "from sklearn.metrics import accuracy_score\n",
+    "\n",
+    "accuracy_score(nn_singlet.feed_forward(X).round(), y)\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### No Better Than Majority"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "**A single Perceptron** in with only two weights and a sigmoid activation function is highly limited.  Translating these values back to binary via simple round can vary error significantly.  Simply adding more nodes to allow for cooperative interpretation of weights might be helpful, but another layer is required to transform the output into single probability.  "
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
+    "\n",
+    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
+    "Your network must have one hidden layer.\n",
+    "\n",
+    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 95,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "+---------EPOCH 1---------+\n",
+      "Loss: \n",
+      " 0.262119387525705\n",
+      "+---------EPOCH 5---------+\n",
+      "Loss: \n",
+      " 0.2537678148345916\n",
+      "+---------EPOCH 1000---------+\n",
+      "Loss: \n",
+      " 0.11284325264785669\n",
+      "+---------EPOCH 2000---------+\n",
+      "Loss: \n",
+      " 0.11016799791348786\n",
+      "+---------EPOCH 3000---------+\n",
+      "Loss: \n",
+      " 0.10124714221423707\n",
+      "+---------EPOCH 4000---------+\n",
+      "Loss: \n",
+      " 0.1011770248536761\n",
+      "+---------EPOCH 5000---------+\n",
+      "Loss: \n",
+      " 0.10115736297390696\n",
+      "+---------EPOCH 6000---------+\n",
+      "Loss: \n",
+      " 0.10114816527332042\n",
+      "+---------EPOCH 7000---------+\n",
+      "Loss: \n",
+      " 0.10114284685639229\n",
+      "+---------EPOCH 8000---------+\n",
+      "Loss: \n",
+      " 0.10113938474650397\n",
+      "+---------EPOCH 9000---------+\n",
+      "Loss: \n",
+      " 0.10113695284348583\n",
+      "+---------EPOCH 10000---------+\n",
+      "Loss: \n",
+      " 0.10113515142281249\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Build a simple multi layer network with one node/input in the first hidden layer and an output layer to convert back to single probability of ate/not ate\n",
+    "multi_layer =(\n",
+    "    ('input', X),  # row 0 must be input\n",
+    "    ('hidden_1', (X.shape[1], 2), 'simple random'),  # shape of first layer of (input_dim, #nodes)\n",
+    "    ('hidden_2', (2, 2), 'simple random'),\n",
+    "    ('output', (2, 1), 'simple random'),\n",
+    "    ('target', y) # last row in description must be the target vector\n",
+    ")\n",
+    "\n",
+    "nn_multi = NeuralNetwork(multi_layer)\n",
+    "\n",
+    "# Number of Epochs / Iterations\n",
+    "for i in range(10000):\n",
+    "    if (i+1 in [1, 5]) or ((i+1) % 1000 ==0):\n",
+    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
+    "#         print('Weights: \\n', nn_multi.layers[1].weights)\n",
+    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn_multi.feed_forward(X)))))\n",
+    "    nn_multi.train(X, y, 0.01)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 102,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>y_pred</th>\n",
+       "      <th>y_test</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>0.499946</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>0.947680</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>2</td>\n",
+       "      <td>0.499946</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>3</td>\n",
+       "      <td>0.056076</td>\n",
+       "      <td>0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>4</td>\n",
+       "      <td>0.056416</td>\n",
+       "      <td>0</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
       ],
-      "metadata": {}
+      "text/plain": [
+       "     y_pred  y_test\n",
+       "0  0.499946       1\n",
+       "1  0.947680       1\n",
+       "2  0.499946       1\n",
+       "3  0.056076       0\n",
+       "4  0.056416       0"
+      ]
+     },
+     "execution_count": 102,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "compat = pd.DataFrame.from_dict({\n",
+    "    'y_pred': nn_multi.feed_forward(X).reshape(1,-1)[0],\n",
+    "    'y_test': y.reshape(1,-1)[0],\n",
+    "})\n",
+    "compat.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 103,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "0.7229"
+      ]
+     },
+     "execution_count": 103,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# Check actual accuracy\n",
+    "from sklearn.metrics import accuracy_score\n",
+    "\n",
+    "accuracy_score(nn_multi.feed_forward(X).round(), y)\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 104,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "0.9458"
+      ]
+     },
+     "execution_count": 104,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# We can give a little more confidence manually with a stop function at ~50%\n",
+    "\n",
+    "def give_confidence(x):\n",
+    "    if x > 0.45:\n",
+    "        return 1\n",
+    "    return 0\n",
+    "\n",
+    "compat.y_pred = compat.y_pred.apply(give_confidence)\n",
+    "accuracy_score(compat.y_pred, compat.y_test)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "inputHidden": false,
+    "jupyter": {
+     "outputs_hidden": false
     },
+    "outputHidden": false
+   },
+   "source": [
+    "**Boom!** ~95%.  For some reason, the network isn't able to push values further away from 0.5 for true positives.  Maybe more layers, bias optimization, or other techniques could help with that.  With a little confidence, this spunky network is doing ok!"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "P.S. Don't try candy gummy bears. They're disgusting. "
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
+    "\n",
+    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
+    "Use the Heart Disease Dataset (binary classification)\n",
+    "Use an appropriate loss function for a binary classification task\n",
+    "Use an appropriate activation function on the final layer of your network.\n",
+    "Train your model using verbose output for ease of grading.\n",
+    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
+    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
+    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
+    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 105,
+   "metadata": {
+    "collapsed": false,
+    "inputHidden": false,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "outputHidden": false
+   },
+   "outputs": [
     {
-      "cell_type": "code",
-      "source": [
-        "# Start your candy perceptron here\n",
-        "\n",
-        "X = candy[['chocolate', 'gummy']].values\n",
-        "y = candy['ate'].values"
-      ],
-      "outputs": [],
-      "execution_count": 6,
-      "metadata": {
-        "collapsed": false,
-        "outputHidden": false,
-        "inputHidden": false
-      }
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "(303, 14)\n"
+     ]
     },
     {
-      "cell_type": "markdown",
-      "source": [
-        "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
-        "\n",
-        "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
-        "Your network must have one hidden layer.\n",
-        "\n",
-        "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>age</th>\n",
+       "      <th>sex</th>\n",
+       "      <th>cp</th>\n",
+       "      <th>trestbps</th>\n",
+       "      <th>chol</th>\n",
+       "      <th>fbs</th>\n",
+       "      <th>restecg</th>\n",
+       "      <th>thalach</th>\n",
+       "      <th>exang</th>\n",
+       "      <th>oldpeak</th>\n",
+       "      <th>slope</th>\n",
+       "      <th>ca</th>\n",
+       "      <th>thal</th>\n",
+       "      <th>target</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>155</td>\n",
+       "      <td>58</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>130</td>\n",
+       "      <td>197</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>131</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.6</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>149</td>\n",
+       "      <td>42</td>\n",
+       "      <td>1</td>\n",
+       "      <td>2</td>\n",
+       "      <td>130</td>\n",
+       "      <td>180</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>150</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>135</td>\n",
+       "      <td>49</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>130</td>\n",
+       "      <td>269</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>163</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>80</td>\n",
+       "      <td>41</td>\n",
+       "      <td>1</td>\n",
+       "      <td>2</td>\n",
+       "      <td>112</td>\n",
+       "      <td>250</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>179</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>109</td>\n",
+       "      <td>50</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>110</td>\n",
+       "      <td>254</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>159</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
       ],
-      "metadata": {}
-    },
+      "text/plain": [
+       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
+       "155   58    0   0       130   197    0        1      131      0      0.6   \n",
+       "149   42    1   2       130   180    0        1      150      0      0.0   \n",
+       "135   49    0   0       130   269    0        1      163      0      0.0   \n",
+       "80    41    1   2       112   250    0        1      179      0      0.0   \n",
+       "109   50    0   0       110   254    0        0      159      0      0.0   \n",
+       "\n",
+       "     slope  ca  thal  target  \n",
+       "155      1   0     2       1  \n",
+       "149      2   0     2       1  \n",
+       "135      2   0     2       1  \n",
+       "80       2   0     2       1  \n",
+       "109      2   0     2       1  "
+      ]
+     },
+     "execution_count": 105,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "import pandas as pd\n",
+    "from sklearn.preprocessing import StandardScaler\n",
+    "\n",
+    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
+    "df = df.sample(frac=1)\n",
+    "print(df.shape)\n",
+    "df.head()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Preprocessing\n",
+    "\n",
+    "Some of these variables are continuous, others categorical and already encoded.\n",
+    "\n",
+    "Summary:\n",
+    "* Categorical: sex, cp, fbs, restecg, exang, slope, ca, thal\n",
+    "* Continuous: everything else\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 126,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "X = df.drop(columns='target')\n",
+    "y = np.array(df.target).reshape(-1,1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 131,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "code",
-      "source": [],
-      "outputs": [],
-      "execution_count": null,
-      "metadata": {
-        "collapsed": false,
-        "outputHidden": false,
-        "inputHidden": false
-      }
-    },
+     "data": {
+      "text/plain": [
+       "(303, 13)"
+      ]
+     },
+     "execution_count": 131,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "X.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 130,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "markdown",
-      "source": [
-        "P.S. Don't try candy gummy bears. They're disgusting. "
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>age</th>\n",
+       "      <th>sex</th>\n",
+       "      <th>cp</th>\n",
+       "      <th>trestbps</th>\n",
+       "      <th>chol</th>\n",
+       "      <th>fbs</th>\n",
+       "      <th>restecg</th>\n",
+       "      <th>thalach</th>\n",
+       "      <th>exang</th>\n",
+       "      <th>oldpeak</th>\n",
+       "      <th>slope</th>\n",
+       "      <th>ca</th>\n",
+       "      <th>thal</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>155</td>\n",
+       "      <td>0.604167</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.339623</td>\n",
+       "      <td>0.162100</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0.458015</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.096774</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>149</td>\n",
+       "      <td>0.270833</td>\n",
+       "      <td>1</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0.339623</td>\n",
+       "      <td>0.123288</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0.603053</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>135</td>\n",
+       "      <td>0.416667</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.339623</td>\n",
+       "      <td>0.326484</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0.702290</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>80</td>\n",
+       "      <td>0.250000</td>\n",
+       "      <td>1</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0.169811</td>\n",
+       "      <td>0.283105</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0.824427</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>109</td>\n",
+       "      <td>0.437500</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.150943</td>\n",
+       "      <td>0.292237</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.671756</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
       ],
-      "metadata": {}
-    },
+      "text/plain": [
+       "          age  sex  cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
+       "155  0.604167    0   0  0.339623  0.162100    0        1  0.458015      0   \n",
+       "149  0.270833    1   2  0.339623  0.123288    0        1  0.603053      0   \n",
+       "135  0.416667    0   0  0.339623  0.326484    0        1  0.702290      0   \n",
+       "80   0.250000    1   2  0.169811  0.283105    0        1  0.824427      0   \n",
+       "109  0.437500    0   0  0.150943  0.292237    0        0  0.671756      0   \n",
+       "\n",
+       "      oldpeak  slope  ca  thal  \n",
+       "155  0.096774      1   0     2  \n",
+       "149  0.000000      2   0     2  \n",
+       "135  0.000000      2   0     2  \n",
+       "80   0.000000      2   0     2  \n",
+       "109  0.000000      2   0     2  "
+      ]
+     },
+     "execution_count": 130,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "from sklearn.preprocessing import MinMaxScaler\n",
+    "\n",
+    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
+    "continuous_vars = list(set(X.columns) - set(categorical_vars))\n",
+    "\n",
+    "scaler = MinMaxScaler()\n",
+    "\n",
+    "X_transformed = X.copy()\n",
+    "X_transformed[continuous_vars] = scaler.fit_transform(X_transformed[continuous_vars]) \n",
+    "\n",
+    "X_transformed.head()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Define Network"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 138,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "markdown",
-      "source": [
-        "## 3. Keras MMP <a id=\"Q3\"></a>\n",
-        "\n",
-        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
-        "Use the Heart Disease Dataset (binary classification)\n",
-        "Use an appropriate loss function for a binary classification task\n",
-        "Use an appropriate activation function on the final layer of your network.\n",
-        "Train your model using verbose output for ease of grading.\n",
-        "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
-        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
-        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
-        "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
+     "data": {
+      "text/html": [
+       "\n",
+       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/vincent-a-brandon/lambda-ds-424/runs/tg0q560i\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
+       "            in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
+       "        "
       ],
-      "metadata": {}
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
     },
     {
-      "cell_type": "code",
-      "source": [
-        "import pandas as pd\n",
-        "from sklearn.preprocessing import StandardScaler\n",
-        "\n",
-        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
-        "df = df.sample(frac=1)\n",
-        "print(df.shape)\n",
-        "df.head()"
-      ],
-      "outputs": [
-        {
-          "output_type": "stream",
-          "name": "stdout",
-          "text": [
-            "(303, 14)\n"
-          ]
-        },
-        {
-          "output_type": "execute_result",
-          "execution_count": 1,
-          "data": {
-            "text/html": [
-              "<div>\n",
-              "<style scoped>\n",
-              "    .dataframe tbody tr th:only-of-type {\n",
-              "        vertical-align: middle;\n",
-              "    }\n",
-              "\n",
-              "    .dataframe tbody tr th {\n",
-              "        vertical-align: top;\n",
-              "    }\n",
-              "\n",
-              "    .dataframe thead th {\n",
-              "        text-align: right;\n",
-              "    }\n",
-              "</style>\n",
-              "<table border=\"1\" class=\"dataframe\">\n",
-              "  <thead>\n",
-              "    <tr style=\"text-align: right;\">\n",
-              "      <th></th>\n",
-              "      <th>age</th>\n",
-              "      <th>sex</th>\n",
-              "      <th>cp</th>\n",
-              "      <th>trestbps</th>\n",
-              "      <th>chol</th>\n",
-              "      <th>fbs</th>\n",
-              "      <th>restecg</th>\n",
-              "      <th>thalach</th>\n",
-              "      <th>exang</th>\n",
-              "      <th>oldpeak</th>\n",
-              "      <th>slope</th>\n",
-              "      <th>ca</th>\n",
-              "      <th>thal</th>\n",
-              "      <th>target</th>\n",
-              "    </tr>\n",
-              "  </thead>\n",
-              "  <tbody>\n",
-              "    <tr>\n",
-              "      <th>97</th>\n",
-              "      <td>52</td>\n",
-              "      <td>1</td>\n",
-              "      <td>0</td>\n",
-              "      <td>108</td>\n",
-              "      <td>233</td>\n",
-              "      <td>1</td>\n",
-              "      <td>1</td>\n",
-              "      <td>147</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0.1</td>\n",
-              "      <td>2</td>\n",
-              "      <td>3</td>\n",
-              "      <td>3</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>1</th>\n",
-              "      <td>37</td>\n",
-              "      <td>1</td>\n",
-              "      <td>2</td>\n",
-              "      <td>130</td>\n",
-              "      <td>250</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>187</td>\n",
-              "      <td>0</td>\n",
-              "      <td>3.5</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0</td>\n",
-              "      <td>2</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>18</th>\n",
-              "      <td>43</td>\n",
-              "      <td>1</td>\n",
-              "      <td>0</td>\n",
-              "      <td>150</td>\n",
-              "      <td>247</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>171</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1.5</td>\n",
-              "      <td>2</td>\n",
-              "      <td>0</td>\n",
-              "      <td>2</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>160</th>\n",
-              "      <td>56</td>\n",
-              "      <td>1</td>\n",
-              "      <td>1</td>\n",
-              "      <td>120</td>\n",
-              "      <td>240</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>169</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0.0</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0</td>\n",
-              "      <td>2</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>290</th>\n",
-              "      <td>61</td>\n",
-              "      <td>1</td>\n",
-              "      <td>0</td>\n",
-              "      <td>148</td>\n",
-              "      <td>203</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>161</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0.0</td>\n",
-              "      <td>2</td>\n",
-              "      <td>1</td>\n",
-              "      <td>3</td>\n",
-              "      <td>0</td>\n",
-              "    </tr>\n",
-              "  </tbody>\n",
-              "</table>\n",
-              "</div>"
-            ],
-            "text/plain": [
-              "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
-              "97    52    1   0       108   233    1        1      147      0      0.1   \n",
-              "1     37    1   2       130   250    0        1      187      0      3.5   \n",
-              "18    43    1   0       150   247    0        1      171      0      1.5   \n",
-              "160   56    1   1       120   240    0        1      169      0      0.0   \n",
-              "290   61    1   0       148   203    0        1      161      0      0.0   \n",
-              "\n",
-              "     slope  ca  thal  target  \n",
-              "97       2   3     3       1  \n",
-              "1        0   0     2       1  \n",
-              "18       2   0     2       1  \n",
-              "160      0   0     2       1  \n",
-              "290      2   1     3       0  "
-            ]
-          },
-          "metadata": {}
-        }
-      ],
-      "execution_count": 1,
-      "metadata": {
-        "collapsed": false,
-        "inputHidden": false,
-        "outputHidden": false
-      }
+     "data": {
+      "text/plain": [
+       "W&B Run: https://app.wandb.ai/vincent-a-brandon/lambda-ds-424/runs/tg0q560i"
+      ]
+     },
+     "execution_count": 138,
+     "metadata": {},
+     "output_type": "execute_result"
     }
-  ],
-  "metadata": {
-    "kernel_info": {
-      "name": "python3"
-    },
-    "kernelspec": {
-      "name": "python3",
-      "language": "python",
-      "display_name": "Python 3"
-    },
-    "language_info": {
-      "name": "python",
-      "version": "3.7.3",
-      "mimetype": "text/x-python",
-      "codemirror_mode": {
-        "name": "ipython",
-        "version": 3
-      },
-      "pygments_lexer": "ipython3",
-      "nbconvert_exporter": "python",
-      "file_extension": ".py"
+   ],
+   "source": [
+    "from tensorflow import keras\n",
+    "from tensorflow.keras.models import Sequential\n",
+    "from tensorflow.keras.layers import Dense\n",
+    "\n",
+    "# Initialize WANDB\n",
+    "import wandb\n",
+    "from wandb.keras import WandbCallback\n",
+    "wandb.init(project=\"lambda-ds-424\")\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 139,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
+      "Train on 303 samples\n",
+      "Epoch 1/100\n",
+      "303/303 [==============================] - 0s 323us/sample - loss: 0.7789 - binary_accuracy: 0.4554\n",
+      "Epoch 2/100\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.5929 - binary_accuracy: 0.4554\n",
+      "Epoch 3/100\n",
+      "303/303 [==============================] - 0s 96us/sample - loss: 0.5443 - binary_accuracy: 0.4488\n",
+      "Epoch 4/100\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.5141 - binary_accuracy: 0.4851\n",
+      "Epoch 5/100\n",
+      "303/303 [==============================] - 0s 97us/sample - loss: 0.4877 - binary_accuracy: 0.5314\n",
+      "Epoch 6/100\n",
+      "303/303 [==============================] - 0s 97us/sample - loss: 0.4602 - binary_accuracy: 0.5677\n",
+      "Epoch 7/100\n",
+      "303/303 [==============================] - 0s 95us/sample - loss: 0.4352 - binary_accuracy: 0.5677\n",
+      "Epoch 8/100\n",
+      "303/303 [==============================] - 0s 97us/sample - loss: 0.4191 - binary_accuracy: 0.5743\n",
+      "Epoch 9/100\n",
+      "303/303 [==============================] - 0s 786us/sample - loss: 0.4044 - binary_accuracy: 0.6502\n",
+      "Epoch 10/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.3897 - binary_accuracy: 0.6997\n",
+      "Epoch 11/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.3740 - binary_accuracy: 0.7162\n",
+      "Epoch 12/100\n",
+      "303/303 [==============================] - 0s 96us/sample - loss: 0.3592 - binary_accuracy: 0.7525\n",
+      "Epoch 13/100\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.3452 - binary_accuracy: 0.7657\n",
+      "Epoch 14/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.3322 - binary_accuracy: 0.7888\n",
+      "Epoch 15/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.3169 - binary_accuracy: 0.7888\n",
+      "Epoch 16/100\n",
+      "303/303 [==============================] - 0s 96us/sample - loss: 0.3056 - binary_accuracy: 0.7822\n",
+      "Epoch 17/100\n",
+      "303/303 [==============================] - 0s 95us/sample - loss: 0.2956 - binary_accuracy: 0.7921\n",
+      "Epoch 18/100\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.2912 - binary_accuracy: 0.7954\n",
+      "Epoch 19/100\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.2925 - binary_accuracy: 0.7855\n",
+      "Epoch 20/100\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.2896 - binary_accuracy: 0.7987\n",
+      "Epoch 21/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2845 - binary_accuracy: 0.7954\n",
+      "Epoch 22/100\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.2825 - binary_accuracy: 0.7954\n",
+      "Epoch 23/100\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.2844 - binary_accuracy: 0.8152\n",
+      "Epoch 24/100\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.2812 - binary_accuracy: 0.8086\n",
+      "Epoch 25/100\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.2769 - binary_accuracy: 0.8020\n",
+      "Epoch 26/100\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.2752 - binary_accuracy: 0.8119\n",
+      "Epoch 27/100\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.2751 - binary_accuracy: 0.8053\n",
+      "Epoch 28/100\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.2738 - binary_accuracy: 0.8152\n",
+      "Epoch 29/100\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.2714 - binary_accuracy: 0.8086\n",
+      "Epoch 30/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.2701 - binary_accuracy: 0.8086\n",
+      "Epoch 31/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.2695 - binary_accuracy: 0.8119\n",
+      "Epoch 32/100\n",
+      "303/303 [==============================] - 0s 97us/sample - loss: 0.2680 - binary_accuracy: 0.8119\n",
+      "Epoch 33/100\n",
+      "303/303 [==============================] - 0s 95us/sample - loss: 0.2671 - binary_accuracy: 0.8185\n",
+      "Epoch 34/100\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.2664 - binary_accuracy: 0.8119\n",
+      "Epoch 35/100\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.2677 - binary_accuracy: 0.8119\n",
+      "Epoch 36/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.2669 - binary_accuracy: 0.8218\n",
+      "Epoch 37/100\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.2641 - binary_accuracy: 0.8251\n",
+      "Epoch 38/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2652 - binary_accuracy: 0.8218\n",
+      "Epoch 39/100\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.2618 - binary_accuracy: 0.8251\n",
+      "Epoch 40/100\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.2616 - binary_accuracy: 0.8218\n",
+      "Epoch 41/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.2612 - binary_accuracy: 0.8284\n",
+      "Epoch 42/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.2602 - binary_accuracy: 0.8317\n",
+      "Epoch 43/100\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.2592 - binary_accuracy: 0.8317\n",
+      "Epoch 44/100\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.2586 - binary_accuracy: 0.8218\n",
+      "Epoch 45/100\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.2604 - binary_accuracy: 0.8284\n",
+      "Epoch 46/100\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.2603 - binary_accuracy: 0.8185\n",
+      "Epoch 47/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2575 - binary_accuracy: 0.8350\n",
+      "Epoch 48/100\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.2564 - binary_accuracy: 0.8218\n",
+      "Epoch 49/100\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.2561 - binary_accuracy: 0.8251\n",
+      "Epoch 50/100\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.2574 - binary_accuracy: 0.8218\n",
+      "Epoch 51/100\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.2571 - binary_accuracy: 0.8383\n",
+      "Epoch 52/100\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.2553 - binary_accuracy: 0.8218\n",
+      "Epoch 53/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.2551 - binary_accuracy: 0.8218\n",
+      "Epoch 54/100\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.2537 - binary_accuracy: 0.8317\n",
+      "Epoch 55/100\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.2527 - binary_accuracy: 0.8218\n",
+      "Epoch 56/100\n",
+      "303/303 [==============================] - 0s 130us/sample - loss: 0.2529 - binary_accuracy: 0.8251\n",
+      "Epoch 57/100\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.2535 - binary_accuracy: 0.8218\n",
+      "Epoch 58/100\n",
+      "303/303 [==============================] - 0s 126us/sample - loss: 0.2518 - binary_accuracy: 0.8185\n",
+      "Epoch 59/100\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.2513 - binary_accuracy: 0.8218\n",
+      "Epoch 60/100\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.2520 - binary_accuracy: 0.8350\n",
+      "Epoch 61/100\n",
+      "303/303 [==============================] - 0s 141us/sample - loss: 0.2510 - binary_accuracy: 0.8218\n",
+      "Epoch 62/100\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.2509 - binary_accuracy: 0.8251\n",
+      "Epoch 63/100\n",
+      "303/303 [==============================] - 0s 125us/sample - loss: 0.2501 - binary_accuracy: 0.8284\n",
+      "Epoch 64/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2487 - binary_accuracy: 0.8218\n",
+      "Epoch 65/100\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.2487 - binary_accuracy: 0.8218\n",
+      "Epoch 66/100\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.2487 - binary_accuracy: 0.8218\n",
+      "Epoch 67/100\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.2472 - binary_accuracy: 0.8317\n",
+      "Epoch 68/100\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.2470 - binary_accuracy: 0.8284\n",
+      "Epoch 69/100\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.2481 - binary_accuracy: 0.8284\n",
+      "Epoch 70/100\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.2464 - binary_accuracy: 0.8218\n",
+      "Epoch 71/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.2454 - binary_accuracy: 0.8350\n",
+      "Epoch 72/100\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.2449 - binary_accuracy: 0.8284\n",
+      "Epoch 73/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2457 - binary_accuracy: 0.8284\n",
+      "Epoch 74/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.2447 - binary_accuracy: 0.8251\n",
+      "Epoch 75/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2447 - binary_accuracy: 0.8284\n",
+      "Epoch 76/100\n",
+      "303/303 [==============================] - 0s 95us/sample - loss: 0.2439 - binary_accuracy: 0.8251\n",
+      "Epoch 77/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2430 - binary_accuracy: 0.8284\n",
+      "Epoch 78/100\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.2427 - binary_accuracy: 0.8251\n",
+      "Epoch 79/100\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.2423 - binary_accuracy: 0.8350\n",
+      "Epoch 80/100\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.2426 - binary_accuracy: 0.8218\n",
+      "Epoch 81/100\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.2411 - binary_accuracy: 0.8317\n",
+      "Epoch 82/100\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.2407 - binary_accuracy: 0.8251\n",
+      "Epoch 83/100\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.2401 - binary_accuracy: 0.8317\n",
+      "Epoch 84/100\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.2397 - binary_accuracy: 0.8317\n",
+      "Epoch 85/100\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.2396 - binary_accuracy: 0.8284\n",
+      "Epoch 86/100\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.2412 - binary_accuracy: 0.8350\n",
+      "Epoch 87/100\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.2395 - binary_accuracy: 0.8251\n",
+      "Epoch 88/100\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.2385 - binary_accuracy: 0.8317\n",
+      "Epoch 89/100\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.2382 - binary_accuracy: 0.8284\n",
+      "Epoch 90/100\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.2371 - binary_accuracy: 0.8284\n",
+      "Epoch 91/100\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.2379 - binary_accuracy: 0.8350\n",
+      "Epoch 92/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2369 - binary_accuracy: 0.8284\n",
+      "Epoch 93/100\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.2361 - binary_accuracy: 0.8317\n",
+      "Epoch 94/100\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2368 - binary_accuracy: 0.8284\n",
+      "Epoch 95/100\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.2360 - binary_accuracy: 0.8317\n",
+      "Epoch 96/100\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.2348 - binary_accuracy: 0.8317\n",
+      "Epoch 97/100\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.2347 - binary_accuracy: 0.8284\n",
+      "Epoch 98/100\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.2357 - binary_accuracy: 0.8251\n",
+      "Epoch 99/100\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.2338 - binary_accuracy: 0.8317\n",
+      "Epoch 100/100\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.2345 - binary_accuracy: 0.8251\n"
+     ]
     },
-    "nteract": {
-      "version": "0.15.0"
+    {
+     "data": {
+      "text/plain": [
+       "<tensorflow.python.keras.callbacks.History at 0x7fee5866db70>"
+      ]
+     },
+     "execution_count": 139,
+     "metadata": {},
+     "output_type": "execute_result"
     }
+   ],
+   "source": [
+    "# Static Parameters\n",
+    "inputs = X_transformed.shape[1]\n",
+    "wandb.config.epochs = 1000\n",
+    "\n",
+    "# Define\n",
+    "model = Sequential()\n",
+    "model.add(Dense(13, input_shape=(inputs,)))\n",
+    "model.add(Dense(26, ac))\n",
+    "model.add(Dense(15, activation='relu'))\n",
+    "model.add(Dense(1))\n",
+    "# Compile Model\n",
+    "model.compile(optimizer='adam', loss='mae', metrics=['binary_accuracy'])\n",
+    "\n",
+    "# Fit Model\n",
+    "\n",
+    "model.fit(X_transformed, y, \n",
+    "          epochs=wandb.config.epochs, \n",
+    "          callbacks=[WandbCallback()],\n",
+    "          verbose=1\n",
+    "         )"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernel_info": {
+   "name": "python3"
+  },
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.6.8"
   },
-  "nbformat": 4,
-  "nbformat_minor": 2
-}
\ No newline at end of file
+  "nteract": {
+   "version": "0.15.0"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
diff --git a/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb b/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
index 559d58f..097fa19 100644
--- a/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
+++ b/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
@@ -31,7 +31,97 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[0, 0, 1],\n",
+       "       [0, 1, 1],\n",
+       "       [1, 0, 1],\n",
+       "       [0, 1, 0],\n",
+       "       [1, 0, 0],\n",
+       "       [1, 1, 1],\n",
+       "       [0, 0, 0]])"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/plain": [
+       "array([[0],\n",
+       "       [1],\n",
+       "       [1],\n",
+       "       [1],\n",
+       "       [1],\n",
+       "       [0],\n",
+       "       [0]])"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "# Load Data\n",
+    "import numpy as np\n",
+    "\n",
+    "\n",
+    "X = np.array((\n",
+    "    [0, 0, 1],\n",
+    "    [0, 1, 1],\n",
+    "    [1, 0, 1],\n",
+    "    [0, 1, 0],\n",
+    "    [1, 0, 0],\n",
+    "    [1, 1, 1],\n",
+    "    [0, 0, 0]\n",
+    "))\n",
+    "\n",
+    "y = np.array((\n",
+    "    [0], [1], [1], [1], [1], [0], [0]\n",
+    "))\n",
+    "\n",
+    "display(X, y)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "NameError",
+     "evalue": "name 'NeuralNetwork' is not defined",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-6-53ea8d73d50b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test given network class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Number of Epochs / Iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mNameError\u001b[0m: name 'NeuralNetwork' is not defined"
+     ]
+    }
+   ],
+   "source": [
+    "# Test given network class\n",
+    "nn = NeuralNetwork()\n",
+    "\n",
+    "# Number of Epochs / Iterations\n",
+    "for i in range(10000):\n",
+    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
+    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
+    "        print('Input: \\n', X)\n",
+    "        print('Actual Output: \\n', y)\n",
+    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
+    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
+    "    nn.train(X,y)\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -39,9 +129,686 @@
    },
    "outputs": [],
    "source": [
-    "##### Your Code Here #####"
+    "def gen_random_matrix(shape):\n",
+    "    return np.random.rand(shape[0], shape[1])\n",
+    "\n",
+    "# A lot of thought needs to go setting up the nodes.  Creating their size programatically\n",
+    "#  makes a lot of assumptions about hidden layer size (could be random, arbitrary, etc.).\n",
+    "#  One option would be to add a 'auto' feature that created hidden layers of X.shape + C\n",
+    "#  weights. Another would be to create pass a distribution and have the layer be a generator \n",
+    "#  of sorts whose parameters can be optimized.\n",
+    "\n",
+    "example_network_description = (\n",
+    "    ('input', X),  # row 0 must be input\n",
+    "    ('hidden_1', (X.shape[1], 3), 'simple random'),  # hidden vectors must match input vec\n",
+    "    ('hidden_2', (3, 7), 'simple random'),\n",
+    "    ('output', (7,1), 'simple random'),  # final active row must be output vector. match last hidden vec\n",
+    "    ('target', y) # last row in description must be the target vector\n",
+    ")\n",
+    "\n",
+    "\n",
+    "class LayerFactory():\n",
+    "    def __init__(self):\n",
+    "        return\n",
+    "    \n",
+    "    def initialize_weights(self, shape, algorithm):\n",
+    "        \"\"\"\n",
+    "        Lookup available weight formulas and generate pseudo-random numbers for initial weights\n",
+    "        of specified shape.\n",
+    "        \n",
+    "        \"\"\"\n",
+    "        available_generators = {\n",
+    "            'simple random': gen_random_matrix,\n",
+    "        }\n",
+    "        \n",
+    "        return available_generators[algorithm](shape)\n",
+    "    \n",
+    "    def generate_layers(self, description):\n",
+    "        \"\"\"\n",
+    "        Generate layers based on network description.\n",
+    "        \n",
+    "        ====Parameters====\n",
+    "        description: tuple or list object of layer descriptions ('name', shape)\n",
+    "        \"\"\"\n",
+    "        layers = {}\n",
+    "        layers[0] = Layer()\n",
+    "        layers[0].activated_values = description[0][1]\n",
+    "        for count, row in enumerate(description):\n",
+    "            if row[0] == 'target':\n",
+    "                layers[count] = Layer()\n",
+    "                layers[count].activated_values = description[len(description)-1][1]\n",
+    "                \n",
+    "            elif row[0] != 'input':\n",
+    "                layers[count] = Layer()\n",
+    "                layers[count].weights = self.initialize_weights(shape=row[1], algorithm=row[2])\n",
+    "                layers[count].weighted_sum = 0\n",
+    "                layers[count].activated_values = 0\n",
+    "            \n",
+    "        \n",
+    "        return layers\n",
+    "    \n",
+    "    \n",
+    "class Layer():\n",
+    "    pass\n",
+    "    \n",
+    "\n",
+    "class NeuralNetwork(LayerFactory):\n",
+    "    def __init__(self, description):\n",
+    "        # Set up Architecture of Neural Network\n",
+    "        self.description = description\n",
+    "        self.layers = self.generate_layers(description)\n",
+    "        \n",
+    "    def sigmoid(self, weighted_sum):\n",
+    "        return 1 / (1+np.exp(-weighted_sum))\n",
+    "    \n",
+    "    def sigmoidPrime(self, weighted_sum):\n",
+    "        return weighted_sum * (1 - weighted_sum)\n",
+    "    \n",
+    "    def feed_forward(self, X):\n",
+    "        \"\"\"\n",
+    "        Calculate the NN inference using feed forward.\n",
+    "        aka \"predict\"\n",
+    "        \"\"\"\n",
+    "        for i in range(1, len(self.layers)-1):\n",
+    "            # Weighted sum of inputs\n",
+    "            #  Check if first layer (required to use feed_forward method as Predict)\n",
+    "            if i == 1:\n",
+    "                self.layers[i].weighted_sum = np.dot(X, self.layers[i].weights)\n",
+    "                # Activated values (local outputs)\n",
+    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
+    "            else:\n",
+    "                self.layers[i].weighted_sum = np.dot(self.layers[i-1].activated_values, self.layers[i].weights)\n",
+    "                # Activated values (local outputs)\n",
+    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
+    "\n",
+    "        return self.layers[len(self.layers)-2].activated_values\n",
+    "        \n",
+    "    def backward(self, X, y, net_output, learning_rate):\n",
+    "        \"\"\"\n",
+    "        Backward propagate through the network\n",
+    "        \"\"\"\n",
+    "        # Step 1: Calculate errors and delta shifts for each layer (backward)\n",
+    "        back_prop_pos = 0\n",
+    "        for i in range(len(self.layers)-2, 0, -1):\n",
+    "            # Error in local output\n",
+    "            #   Check if first backprop\n",
+    "            if back_prop_pos == 0:\n",
+    "                self.layers[i].error = self.layers[i+1].activated_values - net_output\n",
+    "                # Apply Derivative of Sigmoid to error\n",
+    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(net_output)\n",
+    "            else:\n",
+    "                self.layers[i].error = self.layers[i+1].delta.dot(self.layers[i+1].weights.T)\n",
+    "                # Apply Derivative of Sigmoid to error\n",
+    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(\n",
+    "                    self.layers[i].activated_values)*learning_rate\n",
+    "                \n",
+    "            back_prop_pos += 1\n",
+    "            \n",
+    "        # Step 2: Calculate adjustments and apply to each layer (forward)\n",
+    "        for i in range(1, len(self.layers)-1):\n",
+    "            self.layers[i].weights += self.layers[i-1].activated_values.T.dot(self.layers[i].delta)\n",
+    "        \n",
+    "    def train(self, X, y, learning_rate):\n",
+    "        net_output = self.feed_forward(X)\n",
+    "        self.backward(X, y, net_output, learning_rate)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "NameError",
+     "evalue": "name 'layers' is not defined",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-9-1bdf468a0e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Testing block.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
+     ]
+    }
+   ],
+   "source": [
+    "# Testing block.\n",
+    "for i in range(len(layers)-2, 0, -1):\n",
+    "    print(i)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 10,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "{0: <__main__.Layer at 0x7f5a3cef6470>,\n",
+       " 1: <__main__.Layer at 0x7f5a3cef66d8>,\n",
+       " 2: <__main__.Layer at 0x7f5a3cef65c0>,\n",
+       " 3: <__main__.Layer at 0x7f5a3cef6c18>,\n",
+       " 4: <__main__.Layer at 0x7f5a3cef6630>}"
+      ]
+     },
+     "execution_count": 10,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# Test LayerFactory\n",
+    "factory = LayerFactory()\n",
+    "layers = factory.generate_layers(description=example_network_description)\n",
+    "layers"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "metadata": {
+    "scrolled": true
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[0.62710878, 0.95262447, 0.80739135, 0.37436599, 0.42188375,\n",
+       "        0.02954279, 0.76865005],\n",
+       "       [0.778027  , 0.51607266, 0.87361569, 0.35606138, 0.00713691,\n",
+       "        0.77170409, 0.34532938],\n",
+       "       [0.10597956, 0.44483607, 0.55169943, 0.90002817, 0.33284319,\n",
+       "        0.26650898, 0.40674867]])"
+      ]
+     },
+     "execution_count": 11,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "layers[2].weights"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "+---------EPOCH 1---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.90001542]\n",
+      " [0.90943558]\n",
+      " [0.90684305]\n",
+      " [0.8997136 ]\n",
+      " [0.89520821]\n",
+      " [0.91293224]\n",
+      " [0.88283682]]\n",
+      "Loss: \n",
+      " 0.35154181530121204\n",
+      "+---------EPOCH 2---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.77460858]\n",
+      " [0.78528419]\n",
+      " [0.78241465]\n",
+      " [0.77425127]\n",
+      " [0.76964302]\n",
+      " [0.78951931]\n",
+      " [0.75671251]]\n",
+      "Loss: \n",
+      " 0.2847780148880342\n",
+      "+---------EPOCH 3---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.52184464]\n",
+      " [0.5233401 ]\n",
+      " [0.52332309]\n",
+      " [0.52171843]\n",
+      " [0.52196283]\n",
+      " [0.52432127]\n",
+      " [0.52013619]]\n",
+      "Loss: \n",
+      " 0.24706780520505797\n",
+      "+---------EPOCH 4---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.62776169]\n",
+      " [0.63412271]\n",
+      " [0.63262421]\n",
+      " [0.62750785]\n",
+      " [0.62533207]\n",
+      " [0.63691294]\n",
+      " [0.61787375]]\n",
+      "Loss: \n",
+      " 0.2470669193516853\n",
+      "+---------EPOCH 5---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.52870734]\n",
+      " [0.53053361]\n",
+      " [0.53042199]\n",
+      " [0.52857262]\n",
+      " [0.52866415]\n",
+      " [0.531642  ]\n",
+      " [0.52644707]]\n",
+      "Loss: \n",
+      " 0.24637494842869323\n",
+      "+---------EPOCH 1000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.57265846]\n",
+      " [0.57510173]\n",
+      " [0.57995036]\n",
+      " [0.57328448]\n",
+      " [0.58087405]\n",
+      " [0.57863114]\n",
+      " [0.56706002]]\n",
+      "Loss: \n",
+      " 0.24272024326609046\n",
+      "+---------EPOCH 2000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.4040769 ]\n",
+      " [0.59320448]\n",
+      " [0.65186825]\n",
+      " [0.61962146]\n",
+      " [0.69956202]\n",
+      " [0.69478392]\n",
+      " [0.37953314]]\n",
+      "Loss: \n",
+      " 0.18738248179055378\n",
+      "+---------EPOCH 3000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.10092761]\n",
+      " [0.48940541]\n",
+      " [0.56865713]\n",
+      " [0.8634187 ]\n",
+      " [0.8746409 ]\n",
+      " [0.34653972]\n",
+      " [0.0452725 ]]\n",
+      "Loss: \n",
+      " 0.08763694524949416\n",
+      "+---------EPOCH 4000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.08091188]\n",
+      " [0.8734761 ]\n",
+      " [0.88304562]\n",
+      " [0.97397462]\n",
+      " [0.95350244]\n",
+      " [0.1628851 ]\n",
+      " [0.0017191 ]]\n",
+      "Loss: \n",
+      " 0.009372458605793227\n",
+      "+---------EPOCH 5000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[4.30170919e-02]\n",
+      " [9.31355568e-01]\n",
+      " [9.38678693e-01]\n",
+      " [9.83203035e-01]\n",
+      " [9.66586221e-01]\n",
+      " [8.92664319e-02]\n",
+      " [4.02783372e-04]]\n",
+      "Loss: \n",
+      " 0.0028128725309214094\n",
+      "+---------EPOCH 6000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[2.99132380e-02]\n",
+      " [9.51022930e-01]\n",
+      " [9.57444951e-01]\n",
+      " [9.87180890e-01]\n",
+      " [9.73258204e-01]\n",
+      " [6.42122687e-02]\n",
+      " [2.07938663e-04]]\n",
+      "Loss: \n",
+      " 0.0014438856107908897\n",
+      "+---------EPOCH 7000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[2.34180313e-02]\n",
+      " [9.60906009e-01]\n",
+      " [9.66690859e-01]\n",
+      " [9.89452644e-01]\n",
+      " [9.77320645e-01]\n",
+      " [5.15125696e-02]\n",
+      " [1.38852037e-04]]\n",
+      "Loss: \n",
+      " 0.00092362959351063\n",
+      "+---------EPOCH 8000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[1.95161844e-02]\n",
+      " [9.66933204e-01]\n",
+      " [9.72232462e-01]\n",
+      " [9.90937205e-01]\n",
+      " [9.80075351e-01]\n",
+      " [4.37160773e-02]\n",
+      " [1.04445311e-04]]\n",
+      "Loss: \n",
+      " 0.0006622232624017801\n",
+      "+---------EPOCH 9000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[1.68933903e-02]\n",
+      " [9.71038848e-01]\n",
+      " [9.75953927e-01]\n",
+      " [9.91991258e-01]\n",
+      " [9.82082707e-01]\n",
+      " [3.83793639e-02]\n",
+      " [8.40121026e-05]]\n",
+      "Loss: \n",
+      " 0.0005086429369579763\n",
+      "+---------EPOCH 10000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[1.49971830e-02]\n",
+      " [9.74041114e-01]\n",
+      " [9.78643390e-01]\n",
+      " [9.92783070e-01]\n",
+      " [9.83621338e-01]\n",
+      " [3.44625973e-02]\n",
+      " [7.05106756e-05]]\n",
+      "Loss: \n",
+      " 0.00040898633108132074\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Ensure layers being created correctly in NN\n",
+    "nn = NeuralNetwork(description=example_network_description)\n",
+    "# Number of Epochs / Iterations\n",
+    "for i in range(10000):\n",
+    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
+    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
+    "        print('Input: \\n', X)\n",
+    "        print('Actual Output: \\n', y)\n",
+    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
+    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
+    "    nn.train(X,y,0.1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 128,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[0.57969291, 0.88623463, 0.82354341, 0.28966033, 0.25362587,\n",
+       "        0.90120771, 0.7845741 ],\n",
+       "       [0.67906782, 0.29905237, 0.27333864, 0.60150628, 0.25300745,\n",
+       "        0.6969853 , 0.02095351],\n",
+       "       [0.16736435, 0.54543392, 0.53045375, 0.86785617, 0.26756957,\n",
+       "        0.06520021, 0.35142303]])"
+      ]
+     },
+     "execution_count": 128,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "nn.layers[1].weights"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Building Flexible Perceptron Network"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 158,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# SEE ABOVE!! WOOT!!"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
   {
    "cell_type": "markdown",
    "metadata": {
@@ -73,7 +840,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 39,
+   "execution_count": 146,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -84,7 +851,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 147,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -94,31 +861,31 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 31,
+   "execution_count": 148,
    "metadata": {},
    "outputs": [],
    "source": [
     "# the data, split between train and test sets\n",
-    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
+    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 32,
+   "execution_count": 149,
    "metadata": {},
    "outputs": [],
    "source": [
-    "x_train = x_train.reshape(x_train.shape[0], img_rows * img_cols)\n",
-    "x_test = x_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
+    "X_train = X_train.reshape(X_train.shape[0], img_rows * img_cols)\n",
+    "X_test = X_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
     "\n",
     "# Normalize Our Data\n",
-    "x_train = x_train / 255\n",
-    "x_test = x_test / 255"
+    "X_train = X_train / 255\n",
+    "X_test = X_test / 255"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 16,
+   "execution_count": 150,
    "metadata": {},
    "outputs": [
     {
@@ -127,7 +894,7 @@
        "(60000, 784)"
       ]
      },
-     "execution_count": 16,
+     "execution_count": 150,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -147,7 +914,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 33,
+   "execution_count": 151,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -164,35 +931,682 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 40,
+   "execution_count": 152,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
-       "array([0., 1., 0., ..., 0., 0., 0.])"
+       "array([[0.],\n",
+       "       [1.],\n",
+       "       [0.],\n",
+       "       ...,\n",
+       "       [0.],\n",
+       "       [0.],\n",
+       "       [0.]])"
       ]
      },
-     "execution_count": 40,
+     "execution_count": 152,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "# A Nice Binary target for ya to work with\n",
+    "y_train = y_train.reshape(-1,1)\n",
     "y_train"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 161,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
+       "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
+       "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
+       "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
+       "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
+       "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
+       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
+       "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
+       "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
+       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
+       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
+       "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
+       "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
+       "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
+       "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
+       "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
+       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
+       "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
+       "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
+       "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
+       "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
+       "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
+       "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
+       "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
+       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
+       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
+       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
+       "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
+       "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
+       "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
+       "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        ])"
+      ]
+     },
+     "execution_count": 161,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "X_train[0]"
+   ]
+  },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "### Estimating Your `net"
+    "### Balance the class"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 268,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>0</th>\n",
+       "      <th>1</th>\n",
+       "      <th>2</th>\n",
+       "      <th>3</th>\n",
+       "      <th>4</th>\n",
+       "      <th>5</th>\n",
+       "      <th>6</th>\n",
+       "      <th>7</th>\n",
+       "      <th>8</th>\n",
+       "      <th>9</th>\n",
+       "      <th>...</th>\n",
+       "      <th>775</th>\n",
+       "      <th>776</th>\n",
+       "      <th>777</th>\n",
+       "      <th>778</th>\n",
+       "      <th>779</th>\n",
+       "      <th>780</th>\n",
+       "      <th>781</th>\n",
+       "      <th>782</th>\n",
+       "      <th>783</th>\n",
+       "      <th>target</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "<p>2 rows  785 columns</p>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "     0    1    2    3    4    5    6    7    8    9  ...  775  776  777  778  \\\n",
+       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
+       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
+       "\n",
+       "   779  780  781  782  783  target  \n",
+       "0  0.0  0.0  0.0  0.0  0.0     0.0  \n",
+       "1  0.0  0.0  0.0  0.0  0.0     1.0  \n",
+       "\n",
+       "[2 rows x 785 columns]"
+      ]
+     },
+     "execution_count": 268,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "import pandas as pd\n",
+    "\n",
+    "# concatenate the data\n",
+    "df_train = pd.DataFrame(X_train)\n",
+    "df_train['target'] = y_train\n",
+    "df_train.head(2)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 274,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>0</th>\n",
+       "      <th>1</th>\n",
+       "      <th>2</th>\n",
+       "      <th>3</th>\n",
+       "      <th>4</th>\n",
+       "      <th>5</th>\n",
+       "      <th>6</th>\n",
+       "      <th>7</th>\n",
+       "      <th>8</th>\n",
+       "      <th>9</th>\n",
+       "      <th>...</th>\n",
+       "      <th>775</th>\n",
+       "      <th>776</th>\n",
+       "      <th>777</th>\n",
+       "      <th>778</th>\n",
+       "      <th>779</th>\n",
+       "      <th>780</th>\n",
+       "      <th>781</th>\n",
+       "      <th>782</th>\n",
+       "      <th>783</th>\n",
+       "      <th>target</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>count</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>11846.000000</td>\n",
+       "      <td>11846.000000</td>\n",
+       "      <td>11846.000000</td>\n",
+       "      <td>11846.000000</td>\n",
+       "      <td>11846.000000</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.000000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>mean</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.000185</td>\n",
+       "      <td>0.000030</td>\n",
+       "      <td>0.000014</td>\n",
+       "      <td>0.000081</td>\n",
+       "      <td>0.000021</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.500000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>std</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.011510</td>\n",
+       "      <td>0.003279</td>\n",
+       "      <td>0.001549</td>\n",
+       "      <td>0.008792</td>\n",
+       "      <td>0.002234</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.500021</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>min</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.000000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>25%</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.000000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>50%</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.500000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>75%</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.000000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>max</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.996078</td>\n",
+       "      <td>0.356863</td>\n",
+       "      <td>0.168627</td>\n",
+       "      <td>0.956863</td>\n",
+       "      <td>0.243137</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.000000</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "<p>8 rows  785 columns</p>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "             0        1        2        3        4        5        6        7  \\\n",
+       "count  11846.0  11846.0  11846.0  11846.0  11846.0  11846.0  11846.0  11846.0   \n",
+       "mean       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "std        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "min        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "25%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "50%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "75%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "max        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "\n",
+       "             8        9  ...           775           776           777  \\\n",
+       "count  11846.0  11846.0  ...  11846.000000  11846.000000  11846.000000   \n",
+       "mean       0.0      0.0  ...      0.000185      0.000030      0.000014   \n",
+       "std        0.0      0.0  ...      0.011510      0.003279      0.001549   \n",
+       "min        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
+       "25%        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
+       "50%        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
+       "75%        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
+       "max        0.0      0.0  ...      0.996078      0.356863      0.168627   \n",
+       "\n",
+       "                778           779      780      781      782      783  \\\n",
+       "count  11846.000000  11846.000000  11846.0  11846.0  11846.0  11846.0   \n",
+       "mean       0.000081      0.000021      0.0      0.0      0.0      0.0   \n",
+       "std        0.008792      0.002234      0.0      0.0      0.0      0.0   \n",
+       "min        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
+       "25%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
+       "50%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
+       "75%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
+       "max        0.956863      0.243137      0.0      0.0      0.0      0.0   \n",
+       "\n",
+       "             target  \n",
+       "count  11846.000000  \n",
+       "mean       0.500000  \n",
+       "std        0.500021  \n",
+       "min        0.000000  \n",
+       "25%        0.000000  \n",
+       "50%        0.500000  \n",
+       "75%        1.000000  \n",
+       "max        1.000000  \n",
+       "\n",
+       "[8 rows x 785 columns]"
+      ]
+     },
+     "execution_count": 274,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "from sklearn.utils import resample\n",
+    "# try to get a balanced sample\n",
+    "\n",
+    "# Separate majority/minority classes\n",
+    "df_majority = df_train[df_train.target == 0]\n",
+    "df_minority = df_train[df_train.target == 1]\n",
+    "\n",
+    "\n",
+    "# Downsample majority class\n",
+    "df_majority_downsampled = resample(\n",
+    "                                    df_majority,\n",
+    "                                    replace=False,\n",
+    "                                    n_samples=5923,\n",
+    "                                    random_state=42\n",
+    "                                  )\n",
+    "\n",
+    "df_downsampled = pd.concat([\n",
+    "    df_majority_downsampled,\n",
+    "    df_minority\n",
+    "])\n",
+    "\n",
+    "df_downsampled.describe()"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Estimating Your  Net"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 283,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -201,7 +1615,222 @@
    "outputs": [],
    "source": [
     "##### Your Code Here #####\n",
-    "\n"
+    "\n",
+    "# Use balanced training set\n",
+    "X = df_downsampled.drop(columns='target')\n",
+    "y = df_downsampled.target.to_numpy().reshape(-1,1)\n",
+    "\n",
+    "network_description = (\n",
+    "    ('input', X),  # row 0 must be input\n",
+    "    ('hidden_1', (X.shape[1], 768), 'simple random'),  # hidden vectors must match input vec\n",
+    "    ('hidden_2', (768, 50), 'simple random'),\n",
+    "    ('hidden_3', (50, 10), 'simple random'),\n",
+    "    ('output', (10,1), 'simple random'),  # final active row must be output vector. match last hidden vec\n",
+    "    ('target', y) # last row in description must be the target vector\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 284,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "(11846, 784)"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/plain": [
+       "(11846, 1)"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "display(X.shape, y.shape)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 286,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "+---------EPOCH 1---------+\n",
+      "Loss: \n",
+      " 0.4796212892485052\n",
+      "+---------EPOCH 2---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 3---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 4---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 5---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 100---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 200---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 300---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 400---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 500---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 600---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 700---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 800---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 900---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 1000---------+\n",
+      "Loss: \n",
+      " 0.5\n"
+     ]
+    }
+   ],
+   "source": [
+    "network = NeuralNetwork(description=network_description)\n",
+    "# Number of Epochs / Iterations\n",
+    "losses = []\n",
+    "for i in range(1000):\n",
+    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 100 ==0):\n",
+    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
+    "        loss = np.mean(np.square(y - network.feed_forward(X)))\n",
+    "        losses.append(loss)\n",
+    "        print(\"Loss: \\n\", str(np.mean(np.square(y - network.feed_forward(X)))))\n",
+    "    network.train(X, y, 0.1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 253,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "1.0"
+      ]
+     },
+     "execution_count": 253,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "np.mean(network.layers[len(network.layers)-1].activated_values == y_train)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 254,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "image/png": "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\n",
+      "text/plain": [
+       "<Figure size 432x288 with 1 Axes>"
+      ]
+     },
+     "metadata": {
+      "needs_background": "light"
+     },
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "# Visualize losses\n",
+    "import matplotlib.pyplot as plt\n",
+    "\n",
+    "plt.plot(losses)\n",
+    "plt.show();"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 255,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "0.902"
+      ]
+     },
+     "execution_count": 255,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# Calculate on test set\n",
+    "np.mean(network.feed_forward(X_test) == y_test.reshape(-1,1))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 259,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "0.09871666666666666"
+      ]
+     },
+     "execution_count": 259,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# What is the majority class?\n",
+    "np.mean(y_train)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# It's probably guessing everything is a one.  Would yield similar efficiency.  Does not\n",
+    "#  Beat majority classifier."
    ]
   },
   {
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
index ca65dc6..2eefc23 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
@@ -91,9 +91,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.6.8"
   }
  },
  "nbformat": 4,
- "nbformat_minor": 2
+ "nbformat_minor": 4
 }
