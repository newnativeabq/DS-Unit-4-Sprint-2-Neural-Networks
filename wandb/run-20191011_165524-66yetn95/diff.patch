diff --git a/LS_DS_Unit_4_Sprint_Challenge_2.ipynb b/LS_DS_Unit_4_Sprint_Challenge_2.ipynb
index 74a5653..a6ad1ca 100644
--- a/LS_DS_Unit_4_Sprint_Challenge_2.ipynb
+++ b/LS_DS_Unit_4_Sprint_Challenge_2.ipynb
@@ -1,431 +1,3282 @@
 {
-  "cells": [
-    {
-      "cell_type": "markdown",
-      "source": [
-        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
-        "<br></br>\n",
-        "<br></br>\n",
-        "\n",
-        "## *Data Science Unit 4 Sprint 2*\n",
-        "\n",
-        "# Sprint Challenge - Neural Network Foundations\n",
-        "\n",
-        "Table of Problems\n",
-        "\n",
-        "1. [Defining Neural Networks](#Q1)\n",
-        "2. [Chocolate Gummy Bears](#Q2)\n",
-        "    - Perceptron\n",
-        "    - Multilayer Perceptron\n",
-        "4. [Keras MMP](#Q3)"
-      ],
-      "metadata": {}
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
+    "<br></br>\n",
+    "<br></br>\n",
+    "\n",
+    "## *Data Science Unit 4 Sprint 2*\n",
+    "\n",
+    "# Sprint Challenge - Neural Network Foundations\n",
+    "\n",
+    "Table of Problems\n",
+    "\n",
+    "1. [Defining Neural Networks](#Q1)\n",
+    "2. [Chocolate Gummy Bears](#Q2)\n",
+    "    - Perceptron\n",
+    "    - Multilayer Perceptron\n",
+    "4. [Keras MMP](#Q3)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "<a id=\"Q1\"></a>\n",
+    "## 1. Define the following terms:\n",
+    "\n",
+    "- **Neuron:  A node in network consisting of a set of weights to be applied to inputs and optionally a transform (activation function) dictating output.\n",
+    "- **Input Layer:  The processed data layer - often an unweighted ingest of normalized/scaled data to the first hidden layer.\n",
+    "- **Hidden Layer:  A layer that lies between the input and output layers.\n",
+    "- **Output Layer:  A final set of nodes in the shape of desired output.  May use alternate activation functions to get explicity output ranges.\n",
+    "- **Activation:  Output transformation at the neuron level - f(x) where x = sum(weights*inputs). \n",
+    "- **Backpropagation:  Propogation of errors from the output layer backward through the network calculating the partial dependence on weights at each layer/node.  The backbone of neural networks. \n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
+    "\n",
+    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
+    "\n",
+    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
+    "\n",
+    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
+    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {
+    "collapsed": false,
+    "inputHidden": false,
+    "jupyter": {
+     "outputs_hidden": false
     },
-    {
-      "cell_type": "markdown",
-      "source": [
-        "<a id=\"Q1\"></a>\n",
-        "## 1. Define the following terms:\n",
-        "\n",
-        "- **Neuron:**\n",
-        "- **Input Layer:**\n",
-        "- **Hidden Layer:**\n",
-        "- **Output Layer:**\n",
-        "- **Activation:**\n",
-        "- **Backpropagation:**\n"
-      ],
-      "metadata": {}
+    "outputHidden": false
+   },
+   "outputs": [],
+   "source": [
+    "import pandas as pd\n",
+    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {
+    "collapsed": false,
+    "inputHidden": false,
+    "jupyter": {
+     "outputs_hidden": false
     },
+    "outputHidden": false
+   },
+   "outputs": [
     {
-      "cell_type": "markdown",
-      "source": [
-        "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
-        "\n",
-        "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
-        "\n",
-        "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
-        "\n",
-        "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
-        "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>chocolate</th>\n",
+       "      <th>gummy</th>\n",
+       "      <th>ate</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>3</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>4</td>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
       ],
-      "metadata": {}
-    },
+      "text/plain": [
+       "   chocolate  gummy  ate\n",
+       "0          0      1    1\n",
+       "1          1      0    1\n",
+       "2          0      1    1\n",
+       "3          0      0    0\n",
+       "4          1      1    0"
+      ]
+     },
+     "execution_count": 2,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "candy.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 43,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "code",
-      "source": [
-        "import pandas as pd\n",
-        "candy = pd.read_csv('chocolate_gummy_bears.csv')"
-      ],
-      "outputs": [],
-      "execution_count": 3,
-      "metadata": {
-        "collapsed": false,
-        "inputHidden": false,
-        "outputHidden": false
-      }
+     "data": {
+      "text/plain": [
+       "(0.5,           chocolate         gummy           ate\n",
+       " count  10000.000000  10000.000000  10000.000000\n",
+       " mean       0.499100      0.499300      0.500000\n",
+       " std        0.500024      0.500025      0.500025\n",
+       " min        0.000000      0.000000      0.000000\n",
+       " 25%        0.000000      0.000000      0.000000\n",
+       " 50%        0.000000      0.000000      0.500000\n",
+       " 75%        1.000000      1.000000      1.000000\n",
+       " max        1.000000      1.000000      1.000000)"
+      ]
+     },
+     "execution_count": 43,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "## Majority Class\n",
+    "\n",
+    "candy.ate.mean(), candy.describe()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Perceptron\n",
+    "\n",
+    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
+    "\n",
+    "Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 31,
+   "metadata": {
+    "collapsed": false,
+    "inputHidden": false,
+    "jupyter": {
+     "outputs_hidden": false
     },
+    "outputHidden": false
+   },
+   "outputs": [],
+   "source": [
+    "# Start your candy perceptron here\n",
+    "\n",
+    "X = candy[['chocolate', 'gummy']].values\n",
+    "\n",
+    "y = candy['ate'].values.reshape(-1,1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 32,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "code",
-      "source": [
-        "candy.head()"
-      ],
-      "outputs": [
-        {
-          "output_type": "execute_result",
-          "execution_count": 4,
-          "data": {
-            "text/plain": [
-              "   chocolate  gummy  ate\n",
-              "0          0      1    1\n",
-              "1          1      0    1\n",
-              "2          0      1    1\n",
-              "3          0      0    0\n",
-              "4          1      1    0"
-            ],
-            "text/html": [
-              "<div>\n",
-              "<style scoped>\n",
-              "    .dataframe tbody tr th:only-of-type {\n",
-              "        vertical-align: middle;\n",
-              "    }\n",
-              "\n",
-              "    .dataframe tbody tr th {\n",
-              "        vertical-align: top;\n",
-              "    }\n",
-              "\n",
-              "    .dataframe thead th {\n",
-              "        text-align: right;\n",
-              "    }\n",
-              "</style>\n",
-              "<table border=\"1\" class=\"dataframe\">\n",
-              "  <thead>\n",
-              "    <tr style=\"text-align: right;\">\n",
-              "      <th></th>\n",
-              "      <th>chocolate</th>\n",
-              "      <th>gummy</th>\n",
-              "      <th>ate</th>\n",
-              "    </tr>\n",
-              "  </thead>\n",
-              "  <tbody>\n",
-              "    <tr>\n",
-              "      <th>0</th>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>1</th>\n",
-              "      <td>1</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>2</th>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>3</th>\n",
-              "      <td>0</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>4</th>\n",
-              "      <td>1</td>\n",
-              "      <td>1</td>\n",
-              "      <td>0</td>\n",
-              "    </tr>\n",
-              "  </tbody>\n",
-              "</table>\n",
-              "</div>"
-            ]
-          },
-          "metadata": {}
-        }
-      ],
-      "execution_count": 4,
-      "metadata": {
-        "collapsed": false,
-        "outputHidden": false,
-        "inputHidden": false
-      }
-    },
+     "data": {
+      "text/plain": [
+       "(numpy.ndarray, (10000, 2), (10000, 1))"
+      ]
+     },
+     "execution_count": 32,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "type(X), X.shape, y.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 90,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Custom implementation of flexible dense network with numpy\n",
+    "import pandas as pd\n",
+    "import numpy as np\n",
+    "\n",
+    "def gen_random_matrix(shape):\n",
+    "    np.random.seed()\n",
+    "    return np.random.rand(shape[0], shape[1])\n",
+    "\n",
+    "# A lot of thought needs to go setting up the nodes.  Creating their size programatically\n",
+    "#  makes a lot of assumptions about hidden layer size (could be random, arbitrary, etc.).\n",
+    "#  One option would be to add a 'auto' feature that created hidden layers of X.shape + C\n",
+    "#  weights. Another would be to create pass a distribution and have the layer be a generator \n",
+    "#  of sorts whose parameters can be optimized.\n",
+    "\n",
+    "example_network_description = (\n",
+    "    ('input', X),  # row 0 must be input\n",
+    "    ('hidden_1', (X.shape[1], 3), 'simple random'),  # hidden vectors must match input vec\n",
+    "    ('hidden_2', (3, 7), 'simple random'),\n",
+    "    ('output', (7,1), 'simple random'),  # final active row must be output vector. match last hidden vec\n",
+    "    ('target', y) # last row in description must be the target vector\n",
+    ")\n",
+    "\n",
+    "\n",
+    "class LayerFactory():\n",
+    "    def __init__(self):\n",
+    "        return\n",
+    "    \n",
+    "    def initialize_weights(self, shape, algorithm):\n",
+    "        \"\"\"\n",
+    "        Lookup available weight formulas and generate pseudo-random numbers for initial weights\n",
+    "        of specified shape.\n",
+    "        \n",
+    "        \"\"\"\n",
+    "        available_generators = {\n",
+    "            'simple random': gen_random_matrix,\n",
+    "        }\n",
+    "        \n",
+    "        return available_generators[algorithm](shape)\n",
+    "    \n",
+    "    def generate_layers(self, description):\n",
+    "        \"\"\"\n",
+    "        Generate layers based on network description.\n",
+    "        \n",
+    "        ====Parameters====\n",
+    "        description: tuple or list object of layer descriptions ('name', shape)\n",
+    "        \"\"\"\n",
+    "        layers = {}\n",
+    "        layers[0] = Layer()\n",
+    "        layers[0].activated_values = description[0][1]\n",
+    "        for count, row in enumerate(description):\n",
+    "            if row[0] == 'target':\n",
+    "                layers[count] = Layer()\n",
+    "                layers[count].activated_values = description[len(description)-1][1]\n",
+    "                \n",
+    "            elif row[0] != 'input':\n",
+    "                layers[count] = Layer()\n",
+    "                layers[count].weights = self.initialize_weights(shape=row[1], algorithm=row[2])\n",
+    "                layers[count].weighted_sum = 0\n",
+    "                layers[count].activated_values = 0\n",
+    "            \n",
+    "        \n",
+    "        return layers\n",
+    "    \n",
+    "    \n",
+    "class Layer():\n",
+    "    pass\n",
+    "    \n",
+    "\n",
+    "class NeuralNetwork(LayerFactory):\n",
+    "    def __init__(self, description):\n",
+    "        # Set up Architecture of Neural Network\n",
+    "        self.description = description\n",
+    "        self.layers = self.generate_layers(description)\n",
+    "        \n",
+    "    def sigmoid(self, weighted_sum):\n",
+    "        return 1 / (1+np.exp(-weighted_sum))\n",
+    "    \n",
+    "    def sigmoidPrime(self, weighted_sum):\n",
+    "        return weighted_sum * (1 - weighted_sum)\n",
+    "    \n",
+    "    def feed_forward(self, X):\n",
+    "        \"\"\"\n",
+    "        Calculate the NN inference using feed forward.\n",
+    "        aka \"predict\"\n",
+    "        \"\"\"\n",
+    "        for i in range(1, len(self.layers)-1):\n",
+    "            # Weighted sum of inputs\n",
+    "            #  Check if first layer (required to use feed_forward method as Predict)\n",
+    "            if i == 1:\n",
+    "                self.layers[i].weighted_sum = np.dot(X, self.layers[i].weights)\n",
+    "                # Activated values (local outputs)\n",
+    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
+    "            else:\n",
+    "                self.layers[i].weighted_sum = np.dot(self.layers[i-1].activated_values, self.layers[i].weights)\n",
+    "                # Activated values (local outputs)\n",
+    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
+    "\n",
+    "        return self.layers[len(self.layers)-2].activated_values\n",
+    "        \n",
+    "    def backward(self, X, y, net_output, learning_rate):\n",
+    "        \"\"\"\n",
+    "        Backward propagate through the network\n",
+    "        \"\"\"\n",
+    "        # Step 1: Calculate errors and delta shifts for each layer (backward)\n",
+    "        back_prop_pos = 0\n",
+    "        for i in range(len(self.layers)-2, 0, -1):\n",
+    "            # Error in local output\n",
+    "            #   Check if first backprop\n",
+    "            if back_prop_pos == 0:\n",
+    "                self.layers[i].error = y - net_output\n",
+    "                # Apply Derivative of Sigmoid to error\n",
+    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(net_output) * learning_rate\n",
+    "            else:\n",
+    "                self.layers[i].error = self.layers[i+1].delta.dot(self.layers[i+1].weights.T)\n",
+    "                # Apply Derivative of Sigmoid to error\n",
+    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(\n",
+    "                    self.layers[i].activated_values)\n",
+    "                \n",
+    "            back_prop_pos += 1\n",
+    "            \n",
+    "        # Step 2: Calculate adjustments and apply to each layer (forward)\n",
+    "        for i in range(1, len(self.layers)-1):\n",
+    "            self.layers[i].weights += self.layers[i-1].activated_values.T.dot(self.layers[i].delta)\n",
+    "        \n",
+    "    def train(self, X, y, learning_rate):\n",
+    "        net_output = self.feed_forward(X)\n",
+    "        self.backward(X, y, net_output, learning_rate)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 49,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "markdown",
-      "source": [
-        "### Perceptron\n",
-        "\n",
-        "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
-        "\n",
-        "Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "+---------EPOCH 1---------+\n",
+      "Weights: \n",
+      " [[0.47432446]\n",
+      " [0.16792762]]\n",
+      "Loss: \n",
+      " 0.2590575177989399\n",
+      "+---------EPOCH 5---------+\n",
+      "Weights: \n",
+      " [[-13.89552077]\n",
+      " [ -8.49463178]]\n",
+      "Loss: \n",
+      " 0.5487530406363103\n",
+      "+---------EPOCH 1000---------+\n",
+      "Weights: \n",
+      " [[-19.54241453]\n",
+      " [  2.8912209 ]]\n",
+      "Loss: \n",
+      " 0.32526107670787996\n",
+      "+---------EPOCH 2000---------+\n",
+      "Weights: \n",
+      " [[-19.54082341]\n",
+      " [  2.89122091]]\n",
+      "Loss: \n",
+      " 0.32526107670281657\n",
+      "+---------EPOCH 3000---------+\n",
+      "Weights: \n",
+      " [[-19.53922975]\n",
+      " [  2.89122091]]\n",
+      "Loss: \n",
+      " 0.3252610766977371\n",
+      "+---------EPOCH 4000---------+\n",
+      "Weights: \n",
+      " [[-19.53763355]\n",
+      " [  2.89122091]]\n",
+      "Loss: \n",
+      " 0.32526107669264137\n",
+      "+---------EPOCH 5000---------+\n",
+      "Weights: \n",
+      " [[-19.5360348 ]\n",
+      " [  2.89122091]]\n",
+      "Loss: \n",
+      " 0.3252610766875294\n",
+      "+---------EPOCH 6000---------+\n",
+      "Weights: \n",
+      " [[-19.53443349]\n",
+      " [  2.89122091]]\n",
+      "Loss: \n",
+      " 0.325261076682401\n",
+      "+---------EPOCH 7000---------+\n",
+      "Weights: \n",
+      " [[-19.53282961]\n",
+      " [  2.89122092]]\n",
+      "Loss: \n",
+      " 0.3252610766772561\n",
+      "+---------EPOCH 8000---------+\n",
+      "Weights: \n",
+      " [[-19.53122315]\n",
+      " [  2.89122092]]\n",
+      "Loss: \n",
+      " 0.32526107667209464\n",
+      "+---------EPOCH 9000---------+\n",
+      "Weights: \n",
+      " [[-19.5296141 ]\n",
+      " [  2.89122092]]\n",
+      "Loss: \n",
+      " 0.3252610766669166\n",
+      "+---------EPOCH 10000---------+\n",
+      "Weights: \n",
+      " [[-19.52800247]\n",
+      " [  2.89122092]]\n",
+      "Loss: \n",
+      " 0.3252610766617218\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Describe single perceptron network\n",
+    "single_layer =(\n",
+    "    ('input', X),  # row 0 must be input\n",
+    "    ('output', (X.shape[1], 1), 'simple random'),  # shape of first layer of (input_dim, #nodes)\n",
+    "    ('target', y) # last row in description must be the target vector\n",
+    ")\n",
+    "\n",
+    "nn_singlet = NeuralNetwork(single_layer)\n",
+    "\n",
+    "# Number of Epochs / Iterations\n",
+    "for i in range(10000):\n",
+    "    if (i+1 in [1, 5]) or ((i+1) % 1000 ==0):\n",
+    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
+    "        print('Weights: \\n', nn_singlet.layers[1].weights)\n",
+    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn_singlet.feed_forward(X)))))\n",
+    "    nn_singlet.train(X, y, 0.1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 52,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "0.7229"
+      ]
+     },
+     "execution_count": 52,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# Check actual accuracy\n",
+    "from sklearn.metrics import accuracy_score\n",
+    "\n",
+    "accuracy_score(nn_singlet.feed_forward(X).round(), y)\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### No Better Than Majority"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "**A single Perceptron** in with only two weights and a sigmoid activation function is highly limited.  Translating these values back to binary via simple round can vary error significantly.  Simply adding more nodes to allow for cooperative interpretation of weights might be helpful, but another layer is required to transform the output into single probability.  "
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
+    "\n",
+    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
+    "Your network must have one hidden layer.\n",
+    "\n",
+    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 95,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "+---------EPOCH 1---------+\n",
+      "Loss: \n",
+      " 0.262119387525705\n",
+      "+---------EPOCH 5---------+\n",
+      "Loss: \n",
+      " 0.2537678148345916\n",
+      "+---------EPOCH 1000---------+\n",
+      "Loss: \n",
+      " 0.11284325264785669\n",
+      "+---------EPOCH 2000---------+\n",
+      "Loss: \n",
+      " 0.11016799791348786\n",
+      "+---------EPOCH 3000---------+\n",
+      "Loss: \n",
+      " 0.10124714221423707\n",
+      "+---------EPOCH 4000---------+\n",
+      "Loss: \n",
+      " 0.1011770248536761\n",
+      "+---------EPOCH 5000---------+\n",
+      "Loss: \n",
+      " 0.10115736297390696\n",
+      "+---------EPOCH 6000---------+\n",
+      "Loss: \n",
+      " 0.10114816527332042\n",
+      "+---------EPOCH 7000---------+\n",
+      "Loss: \n",
+      " 0.10114284685639229\n",
+      "+---------EPOCH 8000---------+\n",
+      "Loss: \n",
+      " 0.10113938474650397\n",
+      "+---------EPOCH 9000---------+\n",
+      "Loss: \n",
+      " 0.10113695284348583\n",
+      "+---------EPOCH 10000---------+\n",
+      "Loss: \n",
+      " 0.10113515142281249\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Build a simple multi layer network with one node/input in the first hidden layer and an output layer to convert back to single probability of ate/not ate\n",
+    "multi_layer =(\n",
+    "    ('input', X),  # row 0 must be input\n",
+    "    ('hidden_1', (X.shape[1], 2), 'simple random'),  # shape of first layer of (input_dim, #nodes)\n",
+    "    ('hidden_2', (2, 2), 'simple random'),\n",
+    "    ('output', (2, 1), 'simple random'),\n",
+    "    ('target', y) # last row in description must be the target vector\n",
+    ")\n",
+    "\n",
+    "nn_multi = NeuralNetwork(multi_layer)\n",
+    "\n",
+    "# Number of Epochs / Iterations\n",
+    "for i in range(10000):\n",
+    "    if (i+1 in [1, 5]) or ((i+1) % 1000 ==0):\n",
+    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
+    "#         print('Weights: \\n', nn_multi.layers[1].weights)\n",
+    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn_multi.feed_forward(X)))))\n",
+    "    nn_multi.train(X, y, 0.01)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 102,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>y_pred</th>\n",
+       "      <th>y_test</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>0.499946</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>0.947680</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>2</td>\n",
+       "      <td>0.499946</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>3</td>\n",
+       "      <td>0.056076</td>\n",
+       "      <td>0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>4</td>\n",
+       "      <td>0.056416</td>\n",
+       "      <td>0</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
       ],
-      "metadata": {}
+      "text/plain": [
+       "     y_pred  y_test\n",
+       "0  0.499946       1\n",
+       "1  0.947680       1\n",
+       "2  0.499946       1\n",
+       "3  0.056076       0\n",
+       "4  0.056416       0"
+      ]
+     },
+     "execution_count": 102,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "compat = pd.DataFrame.from_dict({\n",
+    "    'y_pred': nn_multi.feed_forward(X).reshape(1,-1)[0],\n",
+    "    'y_test': y.reshape(1,-1)[0],\n",
+    "})\n",
+    "compat.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 103,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "0.7229"
+      ]
+     },
+     "execution_count": 103,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# Check actual accuracy\n",
+    "from sklearn.metrics import accuracy_score\n",
+    "\n",
+    "accuracy_score(nn_multi.feed_forward(X).round(), y)\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 104,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "0.9458"
+      ]
+     },
+     "execution_count": 104,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# We can give a little more confidence manually with a stop function at ~50%\n",
+    "\n",
+    "def give_confidence(x):\n",
+    "    if x > 0.45:\n",
+    "        return 1\n",
+    "    return 0\n",
+    "\n",
+    "compat.y_pred = compat.y_pred.apply(give_confidence)\n",
+    "accuracy_score(compat.y_pred, compat.y_test)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "inputHidden": false,
+    "jupyter": {
+     "outputs_hidden": false
     },
+    "outputHidden": false
+   },
+   "source": [
+    "**Boom!** ~95%.  For some reason, the network isn't able to push values further away from 0.5 for true positives.  Maybe more layers, bias optimization, or other techniques could help with that.  With a little confidence, this spunky network is doing ok!"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "P.S. Don't try candy gummy bears. They're disgusting. "
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
+    "\n",
+    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
+    "Use the Heart Disease Dataset (binary classification)\n",
+    "Use an appropriate loss function for a binary classification task\n",
+    "Use an appropriate activation function on the final layer of your network.\n",
+    "Train your model using verbose output for ease of grading.\n",
+    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
+    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
+    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
+    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 105,
+   "metadata": {
+    "collapsed": false,
+    "inputHidden": false,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "outputHidden": false
+   },
+   "outputs": [
     {
-      "cell_type": "code",
-      "source": [
-        "# Start your candy perceptron here\n",
-        "\n",
-        "X = candy[['chocolate', 'gummy']].values\n",
-        "y = candy['ate'].values"
-      ],
-      "outputs": [],
-      "execution_count": 6,
-      "metadata": {
-        "collapsed": false,
-        "outputHidden": false,
-        "inputHidden": false
-      }
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "(303, 14)\n"
+     ]
     },
     {
-      "cell_type": "markdown",
-      "source": [
-        "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
-        "\n",
-        "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
-        "Your network must have one hidden layer.\n",
-        "\n",
-        "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>age</th>\n",
+       "      <th>sex</th>\n",
+       "      <th>cp</th>\n",
+       "      <th>trestbps</th>\n",
+       "      <th>chol</th>\n",
+       "      <th>fbs</th>\n",
+       "      <th>restecg</th>\n",
+       "      <th>thalach</th>\n",
+       "      <th>exang</th>\n",
+       "      <th>oldpeak</th>\n",
+       "      <th>slope</th>\n",
+       "      <th>ca</th>\n",
+       "      <th>thal</th>\n",
+       "      <th>target</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>155</td>\n",
+       "      <td>58</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>130</td>\n",
+       "      <td>197</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>131</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.6</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>149</td>\n",
+       "      <td>42</td>\n",
+       "      <td>1</td>\n",
+       "      <td>2</td>\n",
+       "      <td>130</td>\n",
+       "      <td>180</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>150</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>135</td>\n",
+       "      <td>49</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>130</td>\n",
+       "      <td>269</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>163</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>80</td>\n",
+       "      <td>41</td>\n",
+       "      <td>1</td>\n",
+       "      <td>2</td>\n",
+       "      <td>112</td>\n",
+       "      <td>250</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>179</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>109</td>\n",
+       "      <td>50</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>110</td>\n",
+       "      <td>254</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>159</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
       ],
-      "metadata": {}
-    },
+      "text/plain": [
+       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
+       "155   58    0   0       130   197    0        1      131      0      0.6   \n",
+       "149   42    1   2       130   180    0        1      150      0      0.0   \n",
+       "135   49    0   0       130   269    0        1      163      0      0.0   \n",
+       "80    41    1   2       112   250    0        1      179      0      0.0   \n",
+       "109   50    0   0       110   254    0        0      159      0      0.0   \n",
+       "\n",
+       "     slope  ca  thal  target  \n",
+       "155      1   0     2       1  \n",
+       "149      2   0     2       1  \n",
+       "135      2   0     2       1  \n",
+       "80       2   0     2       1  \n",
+       "109      2   0     2       1  "
+      ]
+     },
+     "execution_count": 105,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "import pandas as pd\n",
+    "from sklearn.preprocessing import StandardScaler\n",
+    "\n",
+    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
+    "df = df.sample(frac=1)\n",
+    "print(df.shape)\n",
+    "df.head()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Preprocessing\n",
+    "\n",
+    "Some of these variables are continuous, others categorical and already encoded.\n",
+    "\n",
+    "Summary:\n",
+    "* Categorical: sex, cp, fbs, restecg, exang, slope, ca, thal\n",
+    "* Continuous: everything else\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 126,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "X = df.drop(columns='target')\n",
+    "y = np.array(df.target).reshape(-1,1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 131,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "code",
-      "source": [],
-      "outputs": [],
-      "execution_count": null,
-      "metadata": {
-        "collapsed": false,
-        "outputHidden": false,
-        "inputHidden": false
-      }
-    },
+     "data": {
+      "text/plain": [
+       "(303, 13)"
+      ]
+     },
+     "execution_count": 131,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "X.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 130,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "markdown",
-      "source": [
-        "P.S. Don't try candy gummy bears. They're disgusting. "
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>age</th>\n",
+       "      <th>sex</th>\n",
+       "      <th>cp</th>\n",
+       "      <th>trestbps</th>\n",
+       "      <th>chol</th>\n",
+       "      <th>fbs</th>\n",
+       "      <th>restecg</th>\n",
+       "      <th>thalach</th>\n",
+       "      <th>exang</th>\n",
+       "      <th>oldpeak</th>\n",
+       "      <th>slope</th>\n",
+       "      <th>ca</th>\n",
+       "      <th>thal</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>155</td>\n",
+       "      <td>0.604167</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.339623</td>\n",
+       "      <td>0.162100</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0.458015</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.096774</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>149</td>\n",
+       "      <td>0.270833</td>\n",
+       "      <td>1</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0.339623</td>\n",
+       "      <td>0.123288</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0.603053</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>135</td>\n",
+       "      <td>0.416667</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.339623</td>\n",
+       "      <td>0.326484</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0.702290</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>80</td>\n",
+       "      <td>0.250000</td>\n",
+       "      <td>1</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0.169811</td>\n",
+       "      <td>0.283105</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0.824427</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>109</td>\n",
+       "      <td>0.437500</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.150943</td>\n",
+       "      <td>0.292237</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.671756</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>2</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
       ],
-      "metadata": {}
-    },
+      "text/plain": [
+       "          age  sex  cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
+       "155  0.604167    0   0  0.339623  0.162100    0        1  0.458015      0   \n",
+       "149  0.270833    1   2  0.339623  0.123288    0        1  0.603053      0   \n",
+       "135  0.416667    0   0  0.339623  0.326484    0        1  0.702290      0   \n",
+       "80   0.250000    1   2  0.169811  0.283105    0        1  0.824427      0   \n",
+       "109  0.437500    0   0  0.150943  0.292237    0        0  0.671756      0   \n",
+       "\n",
+       "      oldpeak  slope  ca  thal  \n",
+       "155  0.096774      1   0     2  \n",
+       "149  0.000000      2   0     2  \n",
+       "135  0.000000      2   0     2  \n",
+       "80   0.000000      2   0     2  \n",
+       "109  0.000000      2   0     2  "
+      ]
+     },
+     "execution_count": 130,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "from sklearn.preprocessing import MinMaxScaler\n",
+    "\n",
+    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
+    "continuous_vars = list(set(X.columns) - set(categorical_vars))\n",
+    "\n",
+    "scaler = MinMaxScaler()\n",
+    "\n",
+    "X_transformed = X.copy()\n",
+    "X_transformed[continuous_vars] = scaler.fit_transform(X_transformed[continuous_vars]) \n",
+    "\n",
+    "X_transformed.head()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Define Network"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 141,
+   "metadata": {},
+   "outputs": [
     {
-      "cell_type": "markdown",
-      "source": [
-        "## 3. Keras MMP <a id=\"Q3\"></a>\n",
-        "\n",
-        "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
-        "Use the Heart Disease Dataset (binary classification)\n",
-        "Use an appropriate loss function for a binary classification task\n",
-        "Use an appropriate activation function on the final layer of your network.\n",
-        "Train your model using verbose output for ease of grading.\n",
-        "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
-        "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
-        "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
-        "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
+     "data": {
+      "text/html": [
+       "\n",
+       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/vincent-a-brandon/lambda-ds-424/runs/kt210tgo\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
+       "            in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
+       "        "
       ],
-      "metadata": {}
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
     },
     {
-      "cell_type": "code",
-      "source": [
-        "import pandas as pd\n",
-        "from sklearn.preprocessing import StandardScaler\n",
-        "\n",
-        "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
-        "df = df.sample(frac=1)\n",
-        "print(df.shape)\n",
-        "df.head()"
-      ],
-      "outputs": [
-        {
-          "output_type": "stream",
-          "name": "stdout",
-          "text": [
-            "(303, 14)\n"
-          ]
-        },
-        {
-          "output_type": "execute_result",
-          "execution_count": 1,
-          "data": {
-            "text/html": [
-              "<div>\n",
-              "<style scoped>\n",
-              "    .dataframe tbody tr th:only-of-type {\n",
-              "        vertical-align: middle;\n",
-              "    }\n",
-              "\n",
-              "    .dataframe tbody tr th {\n",
-              "        vertical-align: top;\n",
-              "    }\n",
-              "\n",
-              "    .dataframe thead th {\n",
-              "        text-align: right;\n",
-              "    }\n",
-              "</style>\n",
-              "<table border=\"1\" class=\"dataframe\">\n",
-              "  <thead>\n",
-              "    <tr style=\"text-align: right;\">\n",
-              "      <th></th>\n",
-              "      <th>age</th>\n",
-              "      <th>sex</th>\n",
-              "      <th>cp</th>\n",
-              "      <th>trestbps</th>\n",
-              "      <th>chol</th>\n",
-              "      <th>fbs</th>\n",
-              "      <th>restecg</th>\n",
-              "      <th>thalach</th>\n",
-              "      <th>exang</th>\n",
-              "      <th>oldpeak</th>\n",
-              "      <th>slope</th>\n",
-              "      <th>ca</th>\n",
-              "      <th>thal</th>\n",
-              "      <th>target</th>\n",
-              "    </tr>\n",
-              "  </thead>\n",
-              "  <tbody>\n",
-              "    <tr>\n",
-              "      <th>97</th>\n",
-              "      <td>52</td>\n",
-              "      <td>1</td>\n",
-              "      <td>0</td>\n",
-              "      <td>108</td>\n",
-              "      <td>233</td>\n",
-              "      <td>1</td>\n",
-              "      <td>1</td>\n",
-              "      <td>147</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0.1</td>\n",
-              "      <td>2</td>\n",
-              "      <td>3</td>\n",
-              "      <td>3</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>1</th>\n",
-              "      <td>37</td>\n",
-              "      <td>1</td>\n",
-              "      <td>2</td>\n",
-              "      <td>130</td>\n",
-              "      <td>250</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>187</td>\n",
-              "      <td>0</td>\n",
-              "      <td>3.5</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0</td>\n",
-              "      <td>2</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>18</th>\n",
-              "      <td>43</td>\n",
-              "      <td>1</td>\n",
-              "      <td>0</td>\n",
-              "      <td>150</td>\n",
-              "      <td>247</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>171</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1.5</td>\n",
-              "      <td>2</td>\n",
-              "      <td>0</td>\n",
-              "      <td>2</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>160</th>\n",
-              "      <td>56</td>\n",
-              "      <td>1</td>\n",
-              "      <td>1</td>\n",
-              "      <td>120</td>\n",
-              "      <td>240</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>169</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0.0</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0</td>\n",
-              "      <td>2</td>\n",
-              "      <td>1</td>\n",
-              "    </tr>\n",
-              "    <tr>\n",
-              "      <th>290</th>\n",
-              "      <td>61</td>\n",
-              "      <td>1</td>\n",
-              "      <td>0</td>\n",
-              "      <td>148</td>\n",
-              "      <td>203</td>\n",
-              "      <td>0</td>\n",
-              "      <td>1</td>\n",
-              "      <td>161</td>\n",
-              "      <td>0</td>\n",
-              "      <td>0.0</td>\n",
-              "      <td>2</td>\n",
-              "      <td>1</td>\n",
-              "      <td>3</td>\n",
-              "      <td>0</td>\n",
-              "    </tr>\n",
-              "  </tbody>\n",
-              "</table>\n",
-              "</div>"
-            ],
-            "text/plain": [
-              "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
-              "97    52    1   0       108   233    1        1      147      0      0.1   \n",
-              "1     37    1   2       130   250    0        1      187      0      3.5   \n",
-              "18    43    1   0       150   247    0        1      171      0      1.5   \n",
-              "160   56    1   1       120   240    0        1      169      0      0.0   \n",
-              "290   61    1   0       148   203    0        1      161      0      0.0   \n",
-              "\n",
-              "     slope  ca  thal  target  \n",
-              "97       2   3     3       1  \n",
-              "1        0   0     2       1  \n",
-              "18       2   0     2       1  \n",
-              "160      0   0     2       1  \n",
-              "290      2   1     3       0  "
-            ]
-          },
-          "metadata": {}
-        }
-      ],
-      "execution_count": 1,
-      "metadata": {
-        "collapsed": false,
-        "inputHidden": false,
-        "outputHidden": false
-      }
+     "data": {
+      "text/plain": [
+       "W&B Run: https://app.wandb.ai/vincent-a-brandon/lambda-ds-424/runs/kt210tgo"
+      ]
+     },
+     "execution_count": 141,
+     "metadata": {},
+     "output_type": "execute_result"
     }
-  ],
-  "metadata": {
-    "kernel_info": {
-      "name": "python3"
-    },
-    "kernelspec": {
-      "name": "python3",
-      "language": "python",
-      "display_name": "Python 3"
-    },
-    "language_info": {
-      "name": "python",
-      "version": "3.7.3",
-      "mimetype": "text/x-python",
-      "codemirror_mode": {
-        "name": "ipython",
-        "version": 3
-      },
-      "pygments_lexer": "ipython3",
-      "nbconvert_exporter": "python",
-      "file_extension": ".py"
+   ],
+   "source": [
+    "from tensorflow import keras\n",
+    "from tensorflow.keras.models import Sequential\n",
+    "from tensorflow.keras.layers import Dense\n",
+    "\n",
+    "# Initialize WANDB\n",
+    "import wandb\n",
+    "from wandb.keras import WandbCallback\n",
+    "wandb.init(project=\"lambda-ds-424\")\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 142,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
+      "Train on 303 samples\n",
+      "Epoch 1/1000\n",
+      "303/303 [==============================] - 0s 324us/sample - loss: 0.7997 - binary_accuracy: 0.4554\n",
+      "Epoch 2/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.5671 - binary_accuracy: 0.4554\n",
+      "Epoch 3/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.5217 - binary_accuracy: 0.4554\n",
+      "Epoch 4/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.4920 - binary_accuracy: 0.5446\n",
+      "Epoch 5/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.4576 - binary_accuracy: 0.5743\n",
+      "Epoch 6/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.4232 - binary_accuracy: 0.5611\n",
+      "Epoch 7/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.3964 - binary_accuracy: 0.5545\n",
+      "Epoch 8/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.3765 - binary_accuracy: 0.5908\n",
+      "Epoch 9/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.3569 - binary_accuracy: 0.7096\n",
+      "Epoch 10/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.3330 - binary_accuracy: 0.7591\n",
+      "Epoch 11/1000\n",
+      "303/303 [==============================] - 0s 124us/sample - loss: 0.3077 - binary_accuracy: 0.7888\n",
+      "Epoch 12/1000\n",
+      "303/303 [==============================] - 0s 133us/sample - loss: 0.2847 - binary_accuracy: 0.8251\n",
+      "Epoch 13/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.2719 - binary_accuracy: 0.8284\n",
+      "Epoch 14/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.2608 - binary_accuracy: 0.8251\n",
+      "Epoch 15/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.2565 - binary_accuracy: 0.8284\n",
+      "Epoch 16/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.2578 - binary_accuracy: 0.8251\n",
+      "Epoch 17/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.2540 - binary_accuracy: 0.8284\n",
+      "Epoch 18/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.2510 - binary_accuracy: 0.8284\n",
+      "Epoch 19/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.2493 - binary_accuracy: 0.8350\n",
+      "Epoch 20/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.2469 - binary_accuracy: 0.8383\n",
+      "Epoch 21/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.2461 - binary_accuracy: 0.8383\n",
+      "Epoch 22/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.2464 - binary_accuracy: 0.8350\n",
+      "Epoch 23/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.2437 - binary_accuracy: 0.8416\n",
+      "Epoch 24/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.2418 - binary_accuracy: 0.8383\n",
+      "Epoch 25/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.2434 - binary_accuracy: 0.8350\n",
+      "Epoch 26/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.2419 - binary_accuracy: 0.8317\n",
+      "Epoch 27/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.2403 - binary_accuracy: 0.8383\n",
+      "Epoch 28/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.2364 - binary_accuracy: 0.8383\n",
+      "Epoch 29/1000\n",
+      "303/303 [==============================] - 0s 124us/sample - loss: 0.2370 - binary_accuracy: 0.8317\n",
+      "Epoch 30/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.2336 - binary_accuracy: 0.8350\n",
+      "Epoch 31/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.2326 - binary_accuracy: 0.8350\n",
+      "Epoch 32/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.2297 - binary_accuracy: 0.8383\n",
+      "Epoch 33/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.2284 - binary_accuracy: 0.8317\n",
+      "Epoch 34/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.2272 - binary_accuracy: 0.8383\n",
+      "Epoch 35/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.2249 - binary_accuracy: 0.8350\n",
+      "Epoch 36/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.2233 - binary_accuracy: 0.8383\n",
+      "Epoch 37/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.2219 - binary_accuracy: 0.8350\n",
+      "Epoch 38/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.2215 - binary_accuracy: 0.8383\n",
+      "Epoch 39/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.2189 - binary_accuracy: 0.8317\n",
+      "Epoch 40/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.2179 - binary_accuracy: 0.8350\n",
+      "Epoch 41/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2141 - binary_accuracy: 0.8317\n",
+      "Epoch 42/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.2123 - binary_accuracy: 0.8350\n",
+      "Epoch 43/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.2108 - binary_accuracy: 0.8383\n",
+      "Epoch 44/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.2085 - binary_accuracy: 0.8350\n",
+      "Epoch 45/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.2134 - binary_accuracy: 0.8284\n",
+      "Epoch 46/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.2077 - binary_accuracy: 0.8284\n",
+      "Epoch 47/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.2078 - binary_accuracy: 0.8317\n",
+      "Epoch 48/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.2027 - binary_accuracy: 0.8350\n",
+      "Epoch 49/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.2014 - binary_accuracy: 0.8350\n",
+      "Epoch 50/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1995 - binary_accuracy: 0.8317\n",
+      "Epoch 51/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1983 - binary_accuracy: 0.8317\n",
+      "Epoch 52/1000\n",
+      "303/303 [==============================] - 0s 123us/sample - loss: 0.1978 - binary_accuracy: 0.8317\n",
+      "Epoch 53/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1953 - binary_accuracy: 0.8317\n",
+      "Epoch 54/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1954 - binary_accuracy: 0.8317\n",
+      "Epoch 55/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1937 - binary_accuracy: 0.8317\n",
+      "Epoch 56/1000\n",
+      "303/303 [==============================] - 0s 130us/sample - loss: 0.1936 - binary_accuracy: 0.8317\n",
+      "Epoch 57/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1936 - binary_accuracy: 0.8350\n",
+      "Epoch 58/1000\n",
+      "303/303 [==============================] - 0s 144us/sample - loss: 0.1941 - binary_accuracy: 0.8284\n",
+      "Epoch 59/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1908 - binary_accuracy: 0.8350\n",
+      "Epoch 60/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1940 - binary_accuracy: 0.8383\n",
+      "Epoch 61/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1921 - binary_accuracy: 0.8317\n",
+      "Epoch 62/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1905 - binary_accuracy: 0.8284\n",
+      "Epoch 63/1000\n",
+      "303/303 [==============================] - 0s 131us/sample - loss: 0.1870 - binary_accuracy: 0.8284\n",
+      "Epoch 64/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1873 - binary_accuracy: 0.8350\n",
+      "Epoch 65/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1855 - binary_accuracy: 0.8317\n",
+      "Epoch 66/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1843 - binary_accuracy: 0.8350\n",
+      "Epoch 67/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1834 - binary_accuracy: 0.8383\n",
+      "Epoch 68/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1831 - binary_accuracy: 0.8350\n",
+      "Epoch 69/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1825 - binary_accuracy: 0.8350\n",
+      "Epoch 70/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1847 - binary_accuracy: 0.8284\n",
+      "Epoch 71/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1834 - binary_accuracy: 0.8317\n",
+      "Epoch 72/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1823 - binary_accuracy: 0.8383\n",
+      "Epoch 73/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1843 - binary_accuracy: 0.8350\n",
+      "Epoch 74/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1841 - binary_accuracy: 0.8317\n",
+      "Epoch 75/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1803 - binary_accuracy: 0.8317\n",
+      "Epoch 76/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1779 - binary_accuracy: 0.8383\n",
+      "Epoch 77/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1777 - binary_accuracy: 0.8383\n",
+      "Epoch 78/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1770 - binary_accuracy: 0.8416\n",
+      "Epoch 79/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1764 - binary_accuracy: 0.8416\n",
+      "Epoch 80/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1762 - binary_accuracy: 0.8416\n",
+      "Epoch 81/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1770 - binary_accuracy: 0.8383\n",
+      "Epoch 82/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1763 - binary_accuracy: 0.8317\n",
+      "Epoch 83/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1745 - binary_accuracy: 0.8416\n",
+      "Epoch 84/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1761 - binary_accuracy: 0.8515\n",
+      "Epoch 85/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1758 - binary_accuracy: 0.8449\n",
+      "Epoch 86/1000\n",
+      "303/303 [==============================] - 0s 125us/sample - loss: 0.1741 - binary_accuracy: 0.8416\n",
+      "Epoch 87/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1728 - binary_accuracy: 0.8482\n",
+      "Epoch 88/1000\n",
+      "303/303 [==============================] - 0s 132us/sample - loss: 0.1725 - binary_accuracy: 0.8449\n",
+      "Epoch 89/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1722 - binary_accuracy: 0.8383\n",
+      "Epoch 90/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1732 - binary_accuracy: 0.8482\n",
+      "Epoch 91/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1721 - binary_accuracy: 0.8482\n",
+      "Epoch 92/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1714 - binary_accuracy: 0.8449\n",
+      "Epoch 93/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1701 - binary_accuracy: 0.8449\n",
+      "Epoch 94/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1697 - binary_accuracy: 0.8449\n",
+      "Epoch 95/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1691 - binary_accuracy: 0.8482\n",
+      "Epoch 96/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1687 - binary_accuracy: 0.8350\n",
+      "Epoch 97/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1689 - binary_accuracy: 0.8416\n",
+      "Epoch 98/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1678 - binary_accuracy: 0.8482\n",
+      "Epoch 99/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1695 - binary_accuracy: 0.8482\n",
+      "Epoch 100/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1686 - binary_accuracy: 0.8482\n",
+      "Epoch 101/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1677 - binary_accuracy: 0.8383\n",
+      "Epoch 102/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1673 - binary_accuracy: 0.8449\n",
+      "Epoch 103/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.1666 - binary_accuracy: 0.8482\n",
+      "Epoch 104/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1665 - binary_accuracy: 0.8416\n",
+      "Epoch 105/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1659 - binary_accuracy: 0.8416\n",
+      "Epoch 106/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1694 - binary_accuracy: 0.8449\n",
+      "Epoch 107/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1666 - binary_accuracy: 0.8515\n",
+      "Epoch 108/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1663 - binary_accuracy: 0.8482\n",
+      "Epoch 109/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1657 - binary_accuracy: 0.8482\n",
+      "Epoch 110/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1644 - binary_accuracy: 0.8614\n",
+      "Epoch 111/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1651 - binary_accuracy: 0.8449\n",
+      "Epoch 112/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1642 - binary_accuracy: 0.8482\n",
+      "Epoch 113/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1640 - binary_accuracy: 0.8482\n",
+      "Epoch 114/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1666 - binary_accuracy: 0.8548\n",
+      "Epoch 115/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1663 - binary_accuracy: 0.8482\n",
+      "Epoch 116/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1651 - binary_accuracy: 0.8515\n",
+      "Epoch 117/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1625 - binary_accuracy: 0.8548\n",
+      "Epoch 118/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1621 - binary_accuracy: 0.8515\n",
+      "Epoch 119/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1624 - binary_accuracy: 0.8515\n",
+      "Epoch 120/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1619 - binary_accuracy: 0.8482\n",
+      "Epoch 121/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1611 - binary_accuracy: 0.8548\n",
+      "Epoch 122/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1609 - binary_accuracy: 0.8548\n",
+      "Epoch 123/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1610 - binary_accuracy: 0.8548\n",
+      "Epoch 124/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1606 - binary_accuracy: 0.8581\n",
+      "Epoch 125/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1614 - binary_accuracy: 0.8515\n",
+      "Epoch 126/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1618 - binary_accuracy: 0.8581\n",
+      "Epoch 127/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1601 - binary_accuracy: 0.8581\n",
+      "Epoch 128/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1597 - binary_accuracy: 0.8548\n",
+      "Epoch 129/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1593 - binary_accuracy: 0.8581\n",
+      "Epoch 130/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1604 - binary_accuracy: 0.8581\n",
+      "Epoch 131/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1594 - binary_accuracy: 0.8581\n",
+      "Epoch 132/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1586 - binary_accuracy: 0.8581\n",
+      "Epoch 133/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1611 - binary_accuracy: 0.8548\n",
+      "Epoch 134/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1601 - binary_accuracy: 0.8548\n",
+      "Epoch 135/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1617 - binary_accuracy: 0.8581\n",
+      "Epoch 136/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1630 - binary_accuracy: 0.8581\n",
+      "Epoch 137/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1602 - binary_accuracy: 0.8581\n",
+      "Epoch 138/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1576 - binary_accuracy: 0.8581\n",
+      "Epoch 139/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1577 - binary_accuracy: 0.8581\n",
+      "Epoch 140/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.1603 - binary_accuracy: 0.8548\n",
+      "Epoch 141/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1623 - binary_accuracy: 0.8581\n",
+      "Epoch 142/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1647 - binary_accuracy: 0.8581\n",
+      "Epoch 143/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1614 - binary_accuracy: 0.8581\n",
+      "Epoch 144/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1594 - binary_accuracy: 0.8581\n",
+      "Epoch 145/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1578 - binary_accuracy: 0.8581\n",
+      "Epoch 146/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1582 - binary_accuracy: 0.8581\n",
+      "Epoch 147/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1578 - binary_accuracy: 0.8515\n",
+      "Epoch 148/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1579 - binary_accuracy: 0.8581\n",
+      "Epoch 149/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1585 - binary_accuracy: 0.8581\n",
+      "Epoch 150/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1569 - binary_accuracy: 0.8581\n",
+      "Epoch 151/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1555 - binary_accuracy: 0.8581\n",
+      "Epoch 152/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1554 - binary_accuracy: 0.8581\n",
+      "Epoch 153/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1550 - binary_accuracy: 0.8581\n",
+      "Epoch 154/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1558 - binary_accuracy: 0.8581\n",
+      "Epoch 155/1000\n",
+      "303/303 [==============================] - 0s 123us/sample - loss: 0.1552 - binary_accuracy: 0.8581\n",
+      "Epoch 156/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1557 - binary_accuracy: 0.8581\n",
+      "Epoch 157/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1558 - binary_accuracy: 0.8581\n",
+      "Epoch 158/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1562 - binary_accuracy: 0.8581\n",
+      "Epoch 159/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1561 - binary_accuracy: 0.8581\n",
+      "Epoch 160/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1582 - binary_accuracy: 0.8581\n",
+      "Epoch 161/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1618 - binary_accuracy: 0.8581\n",
+      "Epoch 162/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1606 - binary_accuracy: 0.8581\n",
+      "Epoch 163/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1555 - binary_accuracy: 0.8581\n",
+      "Epoch 164/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1538 - binary_accuracy: 0.8581\n",
+      "Epoch 165/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1551 - binary_accuracy: 0.8581\n",
+      "Epoch 166/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1537 - binary_accuracy: 0.8581\n",
+      "Epoch 167/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1534 - binary_accuracy: 0.8581\n",
+      "Epoch 168/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1537 - binary_accuracy: 0.8581\n",
+      "Epoch 169/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1529 - binary_accuracy: 0.8581\n",
+      "Epoch 170/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1546 - binary_accuracy: 0.8581\n",
+      "Epoch 171/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1554 - binary_accuracy: 0.8581\n",
+      "Epoch 172/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1525 - binary_accuracy: 0.8581\n",
+      "Epoch 173/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1526 - binary_accuracy: 0.8581\n",
+      "Epoch 174/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1545 - binary_accuracy: 0.8581\n",
+      "Epoch 175/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1531 - binary_accuracy: 0.8581\n",
+      "Epoch 176/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1539 - binary_accuracy: 0.8581\n",
+      "Epoch 177/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1524 - binary_accuracy: 0.8581\n",
+      "Epoch 178/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1531 - binary_accuracy: 0.8581\n",
+      "Epoch 179/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1547 - binary_accuracy: 0.8581\n",
+      "Epoch 180/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1539 - binary_accuracy: 0.8581\n",
+      "Epoch 181/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1521 - binary_accuracy: 0.8548\n",
+      "Epoch 182/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1510 - binary_accuracy: 0.8581\n",
+      "Epoch 183/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1521 - binary_accuracy: 0.8581\n",
+      "Epoch 184/1000\n",
+      "303/303 [==============================] - 0s 135us/sample - loss: 0.1520 - binary_accuracy: 0.8581\n",
+      "Epoch 185/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1518 - binary_accuracy: 0.8581\n",
+      "Epoch 186/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1509 - binary_accuracy: 0.8581\n",
+      "Epoch 187/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1501 - binary_accuracy: 0.8581\n",
+      "Epoch 188/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.1508 - binary_accuracy: 0.8581\n",
+      "Epoch 189/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1524 - binary_accuracy: 0.8581\n",
+      "Epoch 190/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1505 - binary_accuracy: 0.8581\n",
+      "Epoch 191/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1512 - binary_accuracy: 0.8581\n",
+      "Epoch 192/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1513 - binary_accuracy: 0.8614\n",
+      "Epoch 193/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1518 - binary_accuracy: 0.8614\n",
+      "Epoch 194/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1510 - binary_accuracy: 0.8581\n",
+      "Epoch 195/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1502 - binary_accuracy: 0.8581\n",
+      "Epoch 196/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1497 - binary_accuracy: 0.8581\n",
+      "Epoch 197/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1507 - binary_accuracy: 0.8581\n",
+      "Epoch 198/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1504 - binary_accuracy: 0.8614\n",
+      "Epoch 199/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1516 - binary_accuracy: 0.8647\n",
+      "Epoch 200/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1506 - binary_accuracy: 0.8581\n",
+      "Epoch 201/1000\n",
+      "303/303 [==============================] - 0s 145us/sample - loss: 0.1493 - binary_accuracy: 0.8581\n",
+      "Epoch 202/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1495 - binary_accuracy: 0.8614\n",
+      "Epoch 203/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1518 - binary_accuracy: 0.8614\n",
+      "Epoch 204/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1505 - binary_accuracy: 0.8647\n",
+      "Epoch 205/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1502 - binary_accuracy: 0.8647\n",
+      "Epoch 206/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1515 - binary_accuracy: 0.8647\n",
+      "Epoch 207/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1501 - binary_accuracy: 0.8647\n",
+      "Epoch 208/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1519 - binary_accuracy: 0.8581\n",
+      "Epoch 209/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1508 - binary_accuracy: 0.8548\n",
+      "Epoch 210/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1495 - binary_accuracy: 0.8647\n",
+      "Epoch 211/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1519 - binary_accuracy: 0.8647\n",
+      "Epoch 212/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1541 - binary_accuracy: 0.8614\n",
+      "Epoch 213/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1491 - binary_accuracy: 0.8647\n",
+      "Epoch 214/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1478 - binary_accuracy: 0.8548\n",
+      "Epoch 215/1000\n",
+      "303/303 [==============================] - 0s 127us/sample - loss: 0.1477 - binary_accuracy: 0.8581\n",
+      "Epoch 216/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1470 - binary_accuracy: 0.8614\n",
+      "Epoch 217/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1470 - binary_accuracy: 0.8647\n",
+      "Epoch 218/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1473 - binary_accuracy: 0.8647\n",
+      "Epoch 219/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1480 - binary_accuracy: 0.8647\n",
+      "Epoch 220/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1468 - binary_accuracy: 0.8647\n",
+      "Epoch 221/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1473 - binary_accuracy: 0.8581\n",
+      "Epoch 222/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1485 - binary_accuracy: 0.8614\n",
+      "Epoch 223/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1505 - binary_accuracy: 0.8581\n",
+      "Epoch 224/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1501 - binary_accuracy: 0.8548\n",
+      "Epoch 225/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1485 - binary_accuracy: 0.8647\n",
+      "Epoch 226/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1475 - binary_accuracy: 0.8581\n",
+      "Epoch 227/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1480 - binary_accuracy: 0.8581\n",
+      "Epoch 228/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1476 - binary_accuracy: 0.8614\n",
+      "Epoch 229/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1477 - binary_accuracy: 0.8647\n",
+      "Epoch 230/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1469 - binary_accuracy: 0.8548\n",
+      "Epoch 231/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1467 - binary_accuracy: 0.8581\n",
+      "Epoch 232/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1462 - binary_accuracy: 0.8581\n",
+      "Epoch 233/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1475 - binary_accuracy: 0.8581\n",
+      "Epoch 234/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1468 - binary_accuracy: 0.8614\n",
+      "Epoch 235/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1470 - binary_accuracy: 0.8581\n",
+      "Epoch 236/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1464 - binary_accuracy: 0.8581\n",
+      "Epoch 237/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1461 - binary_accuracy: 0.8647\n",
+      "Epoch 238/1000\n",
+      "303/303 [==============================] - 0s 123us/sample - loss: 0.1483 - binary_accuracy: 0.8581\n",
+      "Epoch 239/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.1478 - binary_accuracy: 0.8647\n",
+      "Epoch 240/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1456 - binary_accuracy: 0.8647\n",
+      "Epoch 241/1000\n",
+      "303/303 [==============================] - 0s 124us/sample - loss: 0.1469 - binary_accuracy: 0.8647\n",
+      "Epoch 242/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1452 - binary_accuracy: 0.8647\n",
+      "Epoch 243/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1469 - binary_accuracy: 0.8614\n",
+      "Epoch 244/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1456 - binary_accuracy: 0.8647\n",
+      "Epoch 245/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1453 - binary_accuracy: 0.8647\n",
+      "Epoch 246/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1447 - binary_accuracy: 0.8647\n",
+      "Epoch 247/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1442 - binary_accuracy: 0.8614\n",
+      "Epoch 248/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1455 - binary_accuracy: 0.8614\n",
+      "Epoch 249/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1453 - binary_accuracy: 0.8647\n",
+      "Epoch 250/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1445 - binary_accuracy: 0.8647\n",
+      "Epoch 251/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1445 - binary_accuracy: 0.8647\n",
+      "Epoch 252/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1449 - binary_accuracy: 0.8614\n",
+      "Epoch 253/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1472 - binary_accuracy: 0.8647\n",
+      "Epoch 254/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1468 - binary_accuracy: 0.8647\n",
+      "Epoch 255/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1459 - binary_accuracy: 0.8647\n",
+      "Epoch 256/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1463 - binary_accuracy: 0.8647\n",
+      "Epoch 257/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1449 - binary_accuracy: 0.8647\n",
+      "Epoch 258/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1436 - binary_accuracy: 0.8647\n",
+      "Epoch 259/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1456 - binary_accuracy: 0.8647\n",
+      "Epoch 260/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1452 - binary_accuracy: 0.8647\n",
+      "Epoch 261/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1459 - binary_accuracy: 0.8647\n",
+      "Epoch 262/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1476 - binary_accuracy: 0.8647\n",
+      "Epoch 263/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1451 - binary_accuracy: 0.8647\n",
+      "Epoch 264/1000\n",
+      "303/303 [==============================] - 0s 129us/sample - loss: 0.1444 - binary_accuracy: 0.8647\n",
+      "Epoch 265/1000\n",
+      "303/303 [==============================] - 0s 125us/sample - loss: 0.1463 - binary_accuracy: 0.8647\n",
+      "Epoch 266/1000\n",
+      "303/303 [==============================] - 0s 131us/sample - loss: 0.1450 - binary_accuracy: 0.8647\n",
+      "Epoch 267/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1439 - binary_accuracy: 0.8647\n",
+      "Epoch 268/1000\n",
+      "303/303 [==============================] - 0s 127us/sample - loss: 0.1428 - binary_accuracy: 0.8680\n",
+      "Epoch 269/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1431 - binary_accuracy: 0.8647\n",
+      "Epoch 270/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1427 - binary_accuracy: 0.8647\n",
+      "Epoch 271/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.1457 - binary_accuracy: 0.8647\n",
+      "Epoch 272/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.1444 - binary_accuracy: 0.8647\n",
+      "Epoch 273/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1436 - binary_accuracy: 0.8647\n",
+      "Epoch 274/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.1429 - binary_accuracy: 0.8647\n",
+      "Epoch 275/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1432 - binary_accuracy: 0.8680\n",
+      "Epoch 276/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1429 - binary_accuracy: 0.8647\n",
+      "Epoch 277/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1425 - binary_accuracy: 0.8647\n",
+      "Epoch 278/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1429 - binary_accuracy: 0.8680\n",
+      "Epoch 279/1000\n",
+      "303/303 [==============================] - 0s 125us/sample - loss: 0.1428 - binary_accuracy: 0.8647\n",
+      "Epoch 280/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1430 - binary_accuracy: 0.8647\n",
+      "Epoch 281/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1439 - binary_accuracy: 0.8680\n",
+      "Epoch 282/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1436 - binary_accuracy: 0.8680\n",
+      "Epoch 283/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1425 - binary_accuracy: 0.8647\n",
+      "Epoch 284/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1438 - binary_accuracy: 0.8680\n",
+      "Epoch 285/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1450 - binary_accuracy: 0.8647\n",
+      "Epoch 286/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1428 - binary_accuracy: 0.8680\n",
+      "Epoch 287/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1441 - binary_accuracy: 0.8647\n",
+      "Epoch 288/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1424 - binary_accuracy: 0.8680\n",
+      "Epoch 289/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1417 - binary_accuracy: 0.8680\n",
+      "Epoch 290/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1443 - binary_accuracy: 0.8647\n",
+      "Epoch 291/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1440 - binary_accuracy: 0.8680\n",
+      "Epoch 292/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1438 - binary_accuracy: 0.8680\n",
+      "Epoch 293/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1464 - binary_accuracy: 0.8680\n",
+      "Epoch 294/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1452 - binary_accuracy: 0.8680\n",
+      "Epoch 295/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1432 - binary_accuracy: 0.8680\n",
+      "Epoch 296/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.1435 - binary_accuracy: 0.8680\n",
+      "Epoch 297/1000\n",
+      "303/303 [==============================] - 0s 132us/sample - loss: 0.1415 - binary_accuracy: 0.8680\n",
+      "Epoch 298/1000\n",
+      "303/303 [==============================] - 0s 126us/sample - loss: 0.1409 - binary_accuracy: 0.8680\n",
+      "Epoch 299/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1411 - binary_accuracy: 0.8680\n",
+      "Epoch 300/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1415 - binary_accuracy: 0.8680\n",
+      "Epoch 301/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1420 - binary_accuracy: 0.8680\n",
+      "Epoch 302/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1427 - binary_accuracy: 0.8680\n",
+      "Epoch 303/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1421 - binary_accuracy: 0.8680\n",
+      "Epoch 304/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1418 - binary_accuracy: 0.8680\n",
+      "Epoch 305/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1413 - binary_accuracy: 0.8713\n",
+      "Epoch 306/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1415 - binary_accuracy: 0.8680\n",
+      "Epoch 307/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1406 - binary_accuracy: 0.8680\n",
+      "Epoch 308/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1406 - binary_accuracy: 0.8680\n",
+      "Epoch 309/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1410 - binary_accuracy: 0.8680\n",
+      "Epoch 310/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1412 - binary_accuracy: 0.8680\n",
+      "Epoch 311/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1407 - binary_accuracy: 0.8680\n",
+      "Epoch 312/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1418 - binary_accuracy: 0.8680\n",
+      "Epoch 313/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1458 - binary_accuracy: 0.8680\n",
+      "Epoch 314/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1460 - binary_accuracy: 0.8713\n",
+      "Epoch 315/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1446 - binary_accuracy: 0.8680\n",
+      "Epoch 316/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1428 - binary_accuracy: 0.8680\n",
+      "Epoch 317/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1412 - binary_accuracy: 0.8680\n",
+      "Epoch 318/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1420 - binary_accuracy: 0.8680\n",
+      "Epoch 319/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1406 - binary_accuracy: 0.8713\n",
+      "Epoch 320/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1409 - binary_accuracy: 0.8680\n",
+      "Epoch 321/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1404 - binary_accuracy: 0.8680\n",
+      "Epoch 322/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1410 - binary_accuracy: 0.8680\n",
+      "Epoch 323/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1406 - binary_accuracy: 0.8680\n",
+      "Epoch 324/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1414 - binary_accuracy: 0.8680\n",
+      "Epoch 325/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1412 - binary_accuracy: 0.8680\n",
+      "Epoch 326/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1400 - binary_accuracy: 0.8680\n",
+      "Epoch 327/1000\n",
+      "303/303 [==============================] - 0s 130us/sample - loss: 0.1398 - binary_accuracy: 0.8680\n",
+      "Epoch 328/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1400 - binary_accuracy: 0.8680\n",
+      "Epoch 329/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1450 - binary_accuracy: 0.8713\n",
+      "Epoch 330/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1442 - binary_accuracy: 0.8680\n",
+      "Epoch 331/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1426 - binary_accuracy: 0.8680\n",
+      "Epoch 332/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1416 - binary_accuracy: 0.8680\n",
+      "Epoch 333/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1395 - binary_accuracy: 0.8680\n",
+      "Epoch 334/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1406 - binary_accuracy: 0.8680\n",
+      "Epoch 335/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1417 - binary_accuracy: 0.8713\n",
+      "Epoch 336/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1395 - binary_accuracy: 0.8713\n",
+      "Epoch 337/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1397 - binary_accuracy: 0.8680\n",
+      "Epoch 338/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1387 - binary_accuracy: 0.8680\n",
+      "Epoch 339/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1394 - binary_accuracy: 0.8680\n",
+      "Epoch 340/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1458 - binary_accuracy: 0.8713\n",
+      "Epoch 341/1000\n",
+      "303/303 [==============================] - 0s 96us/sample - loss: 0.1427 - binary_accuracy: 0.8680\n",
+      "Epoch 342/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1446 - binary_accuracy: 0.8647\n",
+      "Epoch 343/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1467 - binary_accuracy: 0.8680\n",
+      "Epoch 344/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1440 - binary_accuracy: 0.8713\n",
+      "Epoch 345/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1409 - binary_accuracy: 0.8713\n",
+      "Epoch 346/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1398 - binary_accuracy: 0.8680\n",
+      "Epoch 347/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1410 - binary_accuracy: 0.8680\n",
+      "Epoch 348/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1386 - binary_accuracy: 0.8680\n",
+      "Epoch 349/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1382 - binary_accuracy: 0.8680\n",
+      "Epoch 350/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1406 - binary_accuracy: 0.8680\n",
+      "Epoch 351/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1425 - binary_accuracy: 0.8713\n",
+      "Epoch 352/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1424 - binary_accuracy: 0.8713\n",
+      "Epoch 353/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1392 - binary_accuracy: 0.8713\n",
+      "Epoch 354/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1380 - binary_accuracy: 0.8713\n",
+      "Epoch 355/1000\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.1382 - binary_accuracy: 0.8680\n",
+      "Epoch 356/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1393 - binary_accuracy: 0.8680\n",
+      "Epoch 357/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1385 - binary_accuracy: 0.8713\n",
+      "Epoch 358/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1377 - binary_accuracy: 0.8713\n",
+      "Epoch 359/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1379 - binary_accuracy: 0.8713\n",
+      "Epoch 360/1000\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.1391 - binary_accuracy: 0.8713\n",
+      "Epoch 361/1000\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.1398 - binary_accuracy: 0.8680\n",
+      "Epoch 362/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1394 - binary_accuracy: 0.8680\n",
+      "Epoch 363/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1430 - binary_accuracy: 0.8713\n",
+      "Epoch 364/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1429 - binary_accuracy: 0.8680\n",
+      "Epoch 365/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1419 - binary_accuracy: 0.8680\n",
+      "Epoch 366/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1378 - binary_accuracy: 0.8680\n",
+      "Epoch 367/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1376 - binary_accuracy: 0.8680\n",
+      "Epoch 368/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1373 - binary_accuracy: 0.8680\n",
+      "Epoch 369/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1381 - binary_accuracy: 0.8680\n",
+      "Epoch 370/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1371 - binary_accuracy: 0.8680\n",
+      "Epoch 371/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1371 - binary_accuracy: 0.8680\n",
+      "Epoch 372/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1384 - binary_accuracy: 0.8680\n",
+      "Epoch 373/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1366 - binary_accuracy: 0.8680\n",
+      "Epoch 374/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1368 - binary_accuracy: 0.8680\n",
+      "Epoch 375/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1371 - binary_accuracy: 0.8680\n",
+      "Epoch 376/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1380 - binary_accuracy: 0.8680\n",
+      "Epoch 377/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1377 - binary_accuracy: 0.8746\n",
+      "Epoch 378/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1359 - binary_accuracy: 0.8680\n",
+      "Epoch 379/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1388 - binary_accuracy: 0.8713\n",
+      "Epoch 380/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1393 - binary_accuracy: 0.8713\n",
+      "Epoch 381/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1391 - binary_accuracy: 0.8680\n",
+      "Epoch 382/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1377 - binary_accuracy: 0.8713\n",
+      "Epoch 383/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1377 - binary_accuracy: 0.8680\n",
+      "Epoch 384/1000\n",
+      "303/303 [==============================] - 0s 123us/sample - loss: 0.1371 - binary_accuracy: 0.8680\n",
+      "Epoch 385/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1363 - binary_accuracy: 0.8680\n",
+      "Epoch 386/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1360 - binary_accuracy: 0.8680\n",
+      "Epoch 387/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1366 - binary_accuracy: 0.8713\n",
+      "Epoch 388/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1367 - binary_accuracy: 0.8713\n",
+      "Epoch 389/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1362 - binary_accuracy: 0.8680\n",
+      "Epoch 390/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1361 - binary_accuracy: 0.8680\n",
+      "Epoch 391/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1365 - binary_accuracy: 0.8713\n",
+      "Epoch 392/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1355 - binary_accuracy: 0.8680\n",
+      "Epoch 393/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1355 - binary_accuracy: 0.8680\n",
+      "Epoch 394/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1370 - binary_accuracy: 0.8713\n",
+      "Epoch 395/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1357 - binary_accuracy: 0.8713\n",
+      "Epoch 396/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1359 - binary_accuracy: 0.8713\n",
+      "Epoch 397/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1379 - binary_accuracy: 0.8680\n",
+      "Epoch 398/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1364 - binary_accuracy: 0.8680\n",
+      "Epoch 399/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1361 - binary_accuracy: 0.8713\n",
+      "Epoch 400/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1352 - binary_accuracy: 0.8680\n",
+      "Epoch 401/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1358 - binary_accuracy: 0.8680\n",
+      "Epoch 402/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1365 - binary_accuracy: 0.8713\n",
+      "Epoch 403/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1352 - binary_accuracy: 0.8713\n",
+      "Epoch 404/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1379 - binary_accuracy: 0.8713\n",
+      "Epoch 405/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1360 - binary_accuracy: 0.8713\n",
+      "Epoch 406/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1365 - binary_accuracy: 0.8713\n",
+      "Epoch 407/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1361 - binary_accuracy: 0.8713\n",
+      "Epoch 408/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1355 - binary_accuracy: 0.8746\n",
+      "Epoch 409/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1355 - binary_accuracy: 0.8713\n",
+      "Epoch 410/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1351 - binary_accuracy: 0.8713\n",
+      "Epoch 411/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1362 - binary_accuracy: 0.8647\n",
+      "Epoch 412/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1360 - binary_accuracy: 0.8647\n",
+      "Epoch 413/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1355 - binary_accuracy: 0.8713\n",
+      "Epoch 414/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1384 - binary_accuracy: 0.8713\n",
+      "Epoch 415/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1377 - binary_accuracy: 0.8713\n",
+      "Epoch 416/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1377 - binary_accuracy: 0.8713\n",
+      "Epoch 417/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1356 - binary_accuracy: 0.8680\n",
+      "Epoch 418/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1358 - binary_accuracy: 0.8713\n",
+      "Epoch 419/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1397 - binary_accuracy: 0.8746\n",
+      "Epoch 420/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1358 - binary_accuracy: 0.8713\n",
+      "Epoch 421/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1373 - binary_accuracy: 0.8713\n",
+      "Epoch 422/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1359 - binary_accuracy: 0.8680\n",
+      "Epoch 423/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1353 - binary_accuracy: 0.8680\n",
+      "Epoch 424/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1355 - binary_accuracy: 0.8680\n",
+      "Epoch 425/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1351 - binary_accuracy: 0.8746\n",
+      "Epoch 426/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1347 - binary_accuracy: 0.8713\n",
+      "Epoch 427/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1359 - binary_accuracy: 0.8680\n",
+      "Epoch 428/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1349 - binary_accuracy: 0.8680\n",
+      "Epoch 429/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1343 - binary_accuracy: 0.8713\n",
+      "Epoch 430/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1342 - binary_accuracy: 0.8680\n",
+      "Epoch 431/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1355 - binary_accuracy: 0.8713\n",
+      "Epoch 432/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1370 - binary_accuracy: 0.8713\n",
+      "Epoch 433/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1347 - binary_accuracy: 0.8713\n",
+      "Epoch 434/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1365 - binary_accuracy: 0.8713\n",
+      "Epoch 435/1000\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.1341 - binary_accuracy: 0.8680\n",
+      "Epoch 436/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1338 - binary_accuracy: 0.8713\n",
+      "Epoch 437/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1334 - binary_accuracy: 0.8713\n",
+      "Epoch 438/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1344 - binary_accuracy: 0.8680\n",
+      "Epoch 439/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1363 - binary_accuracy: 0.8713\n",
+      "Epoch 440/1000\n",
+      "303/303 [==============================] - 0s 124us/sample - loss: 0.1348 - binary_accuracy: 0.8680\n",
+      "Epoch 441/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1344 - binary_accuracy: 0.8713\n",
+      "Epoch 442/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1351 - binary_accuracy: 0.8680\n",
+      "Epoch 443/1000\n",
+      "303/303 [==============================] - 0s 127us/sample - loss: 0.1338 - binary_accuracy: 0.8680\n",
+      "Epoch 444/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1352 - binary_accuracy: 0.8713\n",
+      "Epoch 445/1000\n",
+      "303/303 [==============================] - 0s 136us/sample - loss: 0.1341 - binary_accuracy: 0.8713\n",
+      "Epoch 446/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1339 - binary_accuracy: 0.8713\n",
+      "Epoch 447/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1341 - binary_accuracy: 0.8713\n",
+      "Epoch 448/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1335 - binary_accuracy: 0.8713\n",
+      "Epoch 449/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1328 - binary_accuracy: 0.8713\n",
+      "Epoch 450/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1317 - binary_accuracy: 0.8713\n",
+      "Epoch 451/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1321 - binary_accuracy: 0.8713\n",
+      "Epoch 452/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1325 - binary_accuracy: 0.8713\n",
+      "Epoch 453/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1321 - binary_accuracy: 0.8713\n",
+      "Epoch 454/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1327 - binary_accuracy: 0.8713\n",
+      "Epoch 455/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1327 - binary_accuracy: 0.8713\n",
+      "Epoch 456/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1338 - binary_accuracy: 0.8713\n",
+      "Epoch 457/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1336 - binary_accuracy: 0.8746\n",
+      "Epoch 458/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1322 - binary_accuracy: 0.8713\n",
+      "Epoch 459/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1341 - binary_accuracy: 0.8713\n",
+      "Epoch 460/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1320 - binary_accuracy: 0.8713\n",
+      "Epoch 461/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1321 - binary_accuracy: 0.8746\n",
+      "Epoch 462/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1339 - binary_accuracy: 0.8746\n",
+      "Epoch 463/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1323 - binary_accuracy: 0.8713\n",
+      "Epoch 464/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1341 - binary_accuracy: 0.8713\n",
+      "Epoch 465/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1334 - binary_accuracy: 0.8713\n",
+      "Epoch 466/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1329 - binary_accuracy: 0.8713\n",
+      "Epoch 467/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1329 - binary_accuracy: 0.8713\n",
+      "Epoch 468/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1316 - binary_accuracy: 0.8713\n",
+      "Epoch 469/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1311 - binary_accuracy: 0.8746\n",
+      "Epoch 470/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1331 - binary_accuracy: 0.8713\n",
+      "Epoch 471/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1335 - binary_accuracy: 0.8713\n",
+      "Epoch 472/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1334 - binary_accuracy: 0.8779\n",
+      "Epoch 473/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1330 - binary_accuracy: 0.8713\n",
+      "Epoch 474/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1328 - binary_accuracy: 0.8713\n",
+      "Epoch 475/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1313 - binary_accuracy: 0.8713\n",
+      "Epoch 476/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1314 - binary_accuracy: 0.8713\n",
+      "Epoch 477/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1324 - binary_accuracy: 0.8746\n",
+      "Epoch 478/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1318 - binary_accuracy: 0.8713\n",
+      "Epoch 479/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1317 - binary_accuracy: 0.8713\n",
+      "Epoch 480/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1320 - binary_accuracy: 0.8746\n",
+      "Epoch 481/1000\n",
+      "303/303 [==============================] - 0s 138us/sample - loss: 0.1307 - binary_accuracy: 0.8746\n",
+      "Epoch 482/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1318 - binary_accuracy: 0.8713\n",
+      "Epoch 483/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1324 - binary_accuracy: 0.8713\n",
+      "Epoch 484/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1316 - binary_accuracy: 0.8746\n",
+      "Epoch 485/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1307 - binary_accuracy: 0.8713\n",
+      "Epoch 486/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1312 - binary_accuracy: 0.8746\n",
+      "Epoch 487/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1321 - binary_accuracy: 0.8746\n",
+      "Epoch 488/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1305 - binary_accuracy: 0.8713\n",
+      "Epoch 489/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1305 - binary_accuracy: 0.8746\n",
+      "Epoch 490/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1309 - binary_accuracy: 0.8713\n",
+      "Epoch 491/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1293 - binary_accuracy: 0.8746\n",
+      "Epoch 492/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1302 - binary_accuracy: 0.8746\n",
+      "Epoch 493/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1301 - binary_accuracy: 0.8713\n",
+      "Epoch 494/1000\n",
+      "303/303 [==============================] - 0s 155us/sample - loss: 0.1317 - binary_accuracy: 0.8713\n",
+      "Epoch 495/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1312 - binary_accuracy: 0.8746\n",
+      "Epoch 496/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1301 - binary_accuracy: 0.8713\n",
+      "Epoch 497/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1305 - binary_accuracy: 0.8713\n",
+      "Epoch 498/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1299 - binary_accuracy: 0.8713\n",
+      "Epoch 499/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1313 - binary_accuracy: 0.8713\n",
+      "Epoch 500/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1327 - binary_accuracy: 0.8713\n",
+      "Epoch 501/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1342 - binary_accuracy: 0.8713\n",
+      "Epoch 502/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1325 - binary_accuracy: 0.8779\n",
+      "Epoch 503/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1311 - binary_accuracy: 0.8713\n",
+      "Epoch 504/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1317 - binary_accuracy: 0.8713\n",
+      "Epoch 505/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1356 - binary_accuracy: 0.8746\n",
+      "Epoch 506/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1364 - binary_accuracy: 0.8746\n",
+      "Epoch 507/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1366 - binary_accuracy: 0.8746\n",
+      "Epoch 508/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1336 - binary_accuracy: 0.8746\n",
+      "Epoch 509/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1333 - binary_accuracy: 0.8746\n",
+      "Epoch 510/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1305 - binary_accuracy: 0.8746\n",
+      "Epoch 511/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1300 - binary_accuracy: 0.8746\n",
+      "Epoch 512/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1309 - binary_accuracy: 0.8779\n",
+      "Epoch 513/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1333 - binary_accuracy: 0.8746\n",
+      "Epoch 514/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1314 - binary_accuracy: 0.8713\n",
+      "Epoch 515/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1305 - binary_accuracy: 0.8713\n",
+      "Epoch 516/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1306 - binary_accuracy: 0.8746\n",
+      "Epoch 517/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1315 - binary_accuracy: 0.8746\n",
+      "Epoch 518/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1316 - binary_accuracy: 0.8746\n",
+      "Epoch 519/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1316 - binary_accuracy: 0.8746\n",
+      "Epoch 520/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1323 - binary_accuracy: 0.8746\n",
+      "Epoch 521/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1294 - binary_accuracy: 0.8713\n",
+      "Epoch 522/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1293 - binary_accuracy: 0.8746\n",
+      "Epoch 523/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1288 - binary_accuracy: 0.8746\n",
+      "Epoch 524/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1293 - binary_accuracy: 0.8746\n",
+      "Epoch 525/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1288 - binary_accuracy: 0.8746\n",
+      "Epoch 526/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1295 - binary_accuracy: 0.8746\n",
+      "Epoch 527/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1294 - binary_accuracy: 0.8746\n",
+      "Epoch 528/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1291 - binary_accuracy: 0.8713\n",
+      "Epoch 529/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1297 - binary_accuracy: 0.8746\n",
+      "Epoch 530/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1297 - binary_accuracy: 0.8713\n",
+      "Epoch 531/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1285 - binary_accuracy: 0.8746\n",
+      "Epoch 532/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1295 - binary_accuracy: 0.8746\n",
+      "Epoch 533/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1290 - binary_accuracy: 0.8746\n",
+      "Epoch 534/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1286 - binary_accuracy: 0.8746\n",
+      "Epoch 535/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1285 - binary_accuracy: 0.8746\n",
+      "Epoch 536/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1291 - binary_accuracy: 0.8746\n",
+      "Epoch 537/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1284 - binary_accuracy: 0.8746\n",
+      "Epoch 538/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1296 - binary_accuracy: 0.8746\n",
+      "Epoch 539/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1292 - binary_accuracy: 0.8746\n",
+      "Epoch 540/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1316 - binary_accuracy: 0.8746\n",
+      "Epoch 541/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1311 - binary_accuracy: 0.8746\n",
+      "Epoch 542/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1338 - binary_accuracy: 0.8746\n",
+      "Epoch 543/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1316 - binary_accuracy: 0.8746\n",
+      "Epoch 544/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1304 - binary_accuracy: 0.8746\n",
+      "Epoch 545/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1291 - binary_accuracy: 0.8746\n",
+      "Epoch 546/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1288 - binary_accuracy: 0.8746\n",
+      "Epoch 547/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1289 - binary_accuracy: 0.8746\n",
+      "Epoch 548/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1295 - binary_accuracy: 0.8746\n",
+      "Epoch 549/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1298 - binary_accuracy: 0.8746\n",
+      "Epoch 550/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1292 - binary_accuracy: 0.8779\n",
+      "Epoch 551/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1313 - binary_accuracy: 0.8746\n",
+      "Epoch 552/1000\n",
+      "303/303 [==============================] - 0s 124us/sample - loss: 0.1303 - binary_accuracy: 0.8746\n",
+      "Epoch 553/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1294 - binary_accuracy: 0.8746\n",
+      "Epoch 554/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1326 - binary_accuracy: 0.8746\n",
+      "Epoch 555/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1300 - binary_accuracy: 0.8746\n",
+      "Epoch 556/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1291 - binary_accuracy: 0.8746\n",
+      "Epoch 557/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1281 - binary_accuracy: 0.8746\n",
+      "Epoch 558/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1290 - binary_accuracy: 0.8746\n",
+      "Epoch 559/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1306 - binary_accuracy: 0.8779\n",
+      "Epoch 560/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1315 - binary_accuracy: 0.8746\n",
+      "Epoch 561/1000\n",
+      "303/303 [==============================] - 0s 126us/sample - loss: 0.1285 - binary_accuracy: 0.8746\n",
+      "Epoch 562/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.1301 - binary_accuracy: 0.8746\n",
+      "Epoch 563/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1289 - binary_accuracy: 0.8779\n",
+      "Epoch 564/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1298 - binary_accuracy: 0.8746\n",
+      "Epoch 565/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1306 - binary_accuracy: 0.8746\n",
+      "Epoch 566/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1293 - binary_accuracy: 0.8779\n",
+      "Epoch 567/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1288 - binary_accuracy: 0.8746\n",
+      "Epoch 568/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1283 - binary_accuracy: 0.8746\n",
+      "Epoch 569/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1279 - binary_accuracy: 0.8746\n",
+      "Epoch 570/1000\n",
+      "303/303 [==============================] - 0s 238us/sample - loss: 0.1282 - binary_accuracy: 0.8746\n",
+      "Epoch 571/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1289 - binary_accuracy: 0.8746\n",
+      "Epoch 572/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1294 - binary_accuracy: 0.8746\n",
+      "Epoch 573/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1287 - binary_accuracy: 0.8746\n",
+      "Epoch 574/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1291 - binary_accuracy: 0.8746\n",
+      "Epoch 575/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1324 - binary_accuracy: 0.8746\n",
+      "Epoch 576/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1303 - binary_accuracy: 0.8746\n",
+      "Epoch 577/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1288 - binary_accuracy: 0.8746\n",
+      "Epoch 578/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1296 - binary_accuracy: 0.8746\n",
+      "Epoch 579/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1292 - binary_accuracy: 0.8746\n",
+      "Epoch 580/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1270 - binary_accuracy: 0.8746\n",
+      "Epoch 581/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1271 - binary_accuracy: 0.8746\n",
+      "Epoch 582/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1291 - binary_accuracy: 0.8746\n",
+      "Epoch 583/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1321 - binary_accuracy: 0.8746\n",
+      "Epoch 584/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1285 - binary_accuracy: 0.8746\n",
+      "Epoch 585/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1276 - binary_accuracy: 0.8746\n",
+      "Epoch 586/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1303 - binary_accuracy: 0.8746\n",
+      "Epoch 587/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1289 - binary_accuracy: 0.8746\n",
+      "Epoch 588/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.1283 - binary_accuracy: 0.8746\n",
+      "Epoch 589/1000\n",
+      "303/303 [==============================] - 0s 136us/sample - loss: 0.1270 - binary_accuracy: 0.8746\n",
+      "Epoch 590/1000\n",
+      "303/303 [==============================] - 0s 123us/sample - loss: 0.1270 - binary_accuracy: 0.8746\n",
+      "Epoch 591/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1285 - binary_accuracy: 0.8746\n",
+      "Epoch 592/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1294 - binary_accuracy: 0.8746\n",
+      "Epoch 593/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1282 - binary_accuracy: 0.8779\n",
+      "Epoch 594/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1274 - binary_accuracy: 0.8746\n",
+      "Epoch 595/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1273 - binary_accuracy: 0.8746\n",
+      "Epoch 596/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1278 - binary_accuracy: 0.8779\n",
+      "Epoch 597/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1295 - binary_accuracy: 0.8746\n",
+      "Epoch 598/1000\n",
+      "303/303 [==============================] - 0s 127us/sample - loss: 0.1317 - binary_accuracy: 0.8779\n",
+      "Epoch 599/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1292 - binary_accuracy: 0.8746\n",
+      "Epoch 600/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1277 - binary_accuracy: 0.8746\n",
+      "Epoch 601/1000\n",
+      "303/303 [==============================] - 0s 131us/sample - loss: 0.1295 - binary_accuracy: 0.8746\n",
+      "Epoch 602/1000\n",
+      "303/303 [==============================] - 0s 135us/sample - loss: 0.1285 - binary_accuracy: 0.8779\n",
+      "Epoch 603/1000\n",
+      "303/303 [==============================] - 0s 135us/sample - loss: 0.1304 - binary_accuracy: 0.8845\n",
+      "Epoch 604/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1327 - binary_accuracy: 0.8746\n",
+      "Epoch 605/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.1311 - binary_accuracy: 0.8746\n",
+      "Epoch 606/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1280 - binary_accuracy: 0.8779\n",
+      "Epoch 607/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1274 - binary_accuracy: 0.8746\n",
+      "Epoch 608/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1292 - binary_accuracy: 0.8746\n",
+      "Epoch 609/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1336 - binary_accuracy: 0.8779\n",
+      "Epoch 610/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1364 - binary_accuracy: 0.8812\n",
+      "Epoch 611/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1318 - binary_accuracy: 0.8779\n",
+      "Epoch 612/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1299 - binary_accuracy: 0.8845\n",
+      "Epoch 613/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1288 - binary_accuracy: 0.8779\n",
+      "Epoch 614/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1295 - binary_accuracy: 0.8746\n",
+      "Epoch 615/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1289 - binary_accuracy: 0.8812\n",
+      "Epoch 616/1000\n",
+      "303/303 [==============================] - 0s 127us/sample - loss: 0.1288 - binary_accuracy: 0.8746\n",
+      "Epoch 617/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.1269 - binary_accuracy: 0.8779\n",
+      "Epoch 618/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1281 - binary_accuracy: 0.8779\n",
+      "Epoch 619/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1285 - binary_accuracy: 0.8779\n",
+      "Epoch 620/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1270 - binary_accuracy: 0.8779\n",
+      "Epoch 621/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1274 - binary_accuracy: 0.8812\n",
+      "Epoch 622/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1270 - binary_accuracy: 0.8746\n",
+      "Epoch 623/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1273 - binary_accuracy: 0.8746\n",
+      "Epoch 624/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1270 - binary_accuracy: 0.8779\n",
+      "Epoch 625/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1269 - binary_accuracy: 0.8779\n",
+      "Epoch 626/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1272 - binary_accuracy: 0.8779\n",
+      "Epoch 627/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1266 - binary_accuracy: 0.8812\n",
+      "Epoch 628/1000\n",
+      "303/303 [==============================] - 0s 124us/sample - loss: 0.1277 - binary_accuracy: 0.8779\n",
+      "Epoch 629/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1274 - binary_accuracy: 0.8746\n",
+      "Epoch 630/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1281 - binary_accuracy: 0.8779\n",
+      "Epoch 631/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1288 - binary_accuracy: 0.8746\n",
+      "Epoch 632/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1263 - binary_accuracy: 0.8812\n",
+      "Epoch 633/1000\n",
+      "303/303 [==============================] - 0s 138us/sample - loss: 0.1256 - binary_accuracy: 0.8812\n",
+      "Epoch 634/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.1271 - binary_accuracy: 0.8779\n",
+      "Epoch 635/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1274 - binary_accuracy: 0.8779\n",
+      "Epoch 636/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1305 - binary_accuracy: 0.8779\n",
+      "Epoch 637/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1296 - binary_accuracy: 0.8812\n",
+      "Epoch 638/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1278 - binary_accuracy: 0.8779\n",
+      "Epoch 639/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1290 - binary_accuracy: 0.8779\n",
+      "Epoch 640/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1273 - binary_accuracy: 0.8812\n",
+      "Epoch 641/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1281 - binary_accuracy: 0.8812\n",
+      "Epoch 642/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1276 - binary_accuracy: 0.8779\n",
+      "Epoch 643/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1260 - binary_accuracy: 0.8779\n",
+      "Epoch 644/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1256 - binary_accuracy: 0.8812\n",
+      "Epoch 645/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1261 - binary_accuracy: 0.8812\n",
+      "Epoch 646/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1260 - binary_accuracy: 0.8812\n",
+      "Epoch 647/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1262 - binary_accuracy: 0.8779\n",
+      "Epoch 648/1000\n",
+      "303/303 [==============================] - 0s 226us/sample - loss: 0.1267 - binary_accuracy: 0.8812\n",
+      "Epoch 649/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1265 - binary_accuracy: 0.8779\n",
+      "Epoch 650/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1266 - binary_accuracy: 0.8779\n",
+      "Epoch 651/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1251 - binary_accuracy: 0.8779\n",
+      "Epoch 652/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1260 - binary_accuracy: 0.8812\n",
+      "Epoch 653/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1267 - binary_accuracy: 0.8812\n",
+      "Epoch 654/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1266 - binary_accuracy: 0.8812\n",
+      "Epoch 655/1000\n",
+      "303/303 [==============================] - 0s 123us/sample - loss: 0.1249 - binary_accuracy: 0.8812\n",
+      "Epoch 656/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1250 - binary_accuracy: 0.8812\n",
+      "Epoch 657/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1263 - binary_accuracy: 0.8812\n",
+      "Epoch 658/1000\n",
+      "303/303 [==============================] - 0s 235us/sample - loss: 0.1263 - binary_accuracy: 0.8812\n",
+      "Epoch 659/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1262 - binary_accuracy: 0.8812\n",
+      "Epoch 660/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1262 - binary_accuracy: 0.8779\n",
+      "Epoch 661/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1270 - binary_accuracy: 0.8812\n",
+      "Epoch 662/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1251 - binary_accuracy: 0.8812\n",
+      "Epoch 663/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1248 - binary_accuracy: 0.8812\n",
+      "Epoch 664/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1261 - binary_accuracy: 0.8812\n",
+      "Epoch 665/1000\n",
+      "303/303 [==============================] - 0s 125us/sample - loss: 0.1273 - binary_accuracy: 0.8812\n",
+      "Epoch 666/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1273 - binary_accuracy: 0.8812\n",
+      "Epoch 667/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1270 - binary_accuracy: 0.8812\n",
+      "Epoch 668/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1265 - binary_accuracy: 0.8779\n",
+      "Epoch 669/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1265 - binary_accuracy: 0.8812\n",
+      "Epoch 670/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1254 - binary_accuracy: 0.8812\n",
+      "Epoch 671/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1256 - binary_accuracy: 0.8812\n",
+      "Epoch 672/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1263 - binary_accuracy: 0.8812\n",
+      "Epoch 673/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1256 - binary_accuracy: 0.8812\n",
+      "Epoch 674/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1258 - binary_accuracy: 0.8812\n",
+      "Epoch 675/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1251 - binary_accuracy: 0.8812\n",
+      "Epoch 676/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1244 - binary_accuracy: 0.8812\n",
+      "Epoch 677/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.1257 - binary_accuracy: 0.8812\n",
+      "Epoch 678/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1251 - binary_accuracy: 0.8812\n",
+      "Epoch 679/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1254 - binary_accuracy: 0.8845\n",
+      "Epoch 680/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1258 - binary_accuracy: 0.8845\n",
+      "Epoch 681/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1245 - binary_accuracy: 0.8812\n",
+      "Epoch 682/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1253 - binary_accuracy: 0.8812\n",
+      "Epoch 683/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1252 - binary_accuracy: 0.8845\n",
+      "Epoch 684/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1244 - binary_accuracy: 0.8812\n",
+      "Epoch 685/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1258 - binary_accuracy: 0.8845\n",
+      "Epoch 686/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1255 - binary_accuracy: 0.8812\n",
+      "Epoch 687/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1254 - binary_accuracy: 0.8812\n",
+      "Epoch 688/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1247 - binary_accuracy: 0.8812\n",
+      "Epoch 689/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1250 - binary_accuracy: 0.8845\n",
+      "Epoch 690/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1259 - binary_accuracy: 0.8779\n",
+      "Epoch 691/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1278 - binary_accuracy: 0.8812\n",
+      "Epoch 692/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1280 - binary_accuracy: 0.8812\n",
+      "Epoch 693/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1255 - binary_accuracy: 0.8812\n",
+      "Epoch 694/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1251 - binary_accuracy: 0.8779\n",
+      "Epoch 695/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1246 - binary_accuracy: 0.8812\n",
+      "Epoch 696/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1257 - binary_accuracy: 0.8812\n",
+      "Epoch 697/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1263 - binary_accuracy: 0.8812\n",
+      "Epoch 698/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1266 - binary_accuracy: 0.8812\n",
+      "Epoch 699/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1243 - binary_accuracy: 0.8812\n",
+      "Epoch 700/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1249 - binary_accuracy: 0.8812\n",
+      "Epoch 701/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1269 - binary_accuracy: 0.8812\n",
+      "Epoch 702/1000\n",
+      "303/303 [==============================] - 0s 123us/sample - loss: 0.1262 - binary_accuracy: 0.8845\n",
+      "Epoch 703/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1266 - binary_accuracy: 0.8812\n",
+      "Epoch 704/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.1312 - binary_accuracy: 0.8812\n",
+      "Epoch 705/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1286 - binary_accuracy: 0.8845\n",
+      "Epoch 706/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1258 - binary_accuracy: 0.8845\n",
+      "Epoch 707/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1241 - binary_accuracy: 0.8812\n",
+      "Epoch 708/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1247 - binary_accuracy: 0.8845\n",
+      "Epoch 709/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1262 - binary_accuracy: 0.8812\n",
+      "Epoch 710/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1250 - binary_accuracy: 0.8812\n",
+      "Epoch 711/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1239 - binary_accuracy: 0.8845\n",
+      "Epoch 712/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1257 - binary_accuracy: 0.8845\n",
+      "Epoch 713/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1250 - binary_accuracy: 0.8812\n",
+      "Epoch 714/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1251 - binary_accuracy: 0.8845\n",
+      "Epoch 715/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1276 - binary_accuracy: 0.8812\n",
+      "Epoch 716/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1254 - binary_accuracy: 0.8812\n",
+      "Epoch 717/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1249 - binary_accuracy: 0.8845\n",
+      "Epoch 718/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1253 - binary_accuracy: 0.8845\n",
+      "Epoch 719/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1295 - binary_accuracy: 0.8812\n",
+      "Epoch 720/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1263 - binary_accuracy: 0.8812\n",
+      "Epoch 721/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1241 - binary_accuracy: 0.8845\n",
+      "Epoch 722/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1249 - binary_accuracy: 0.8812\n",
+      "Epoch 723/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1249 - binary_accuracy: 0.8845\n",
+      "Epoch 724/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1239 - binary_accuracy: 0.8845\n",
+      "Epoch 725/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1248 - binary_accuracy: 0.8845\n",
+      "Epoch 726/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1241 - binary_accuracy: 0.8812\n",
+      "Epoch 727/1000\n",
+      "303/303 [==============================] - 0s 130us/sample - loss: 0.1236 - binary_accuracy: 0.8845\n",
+      "Epoch 728/1000\n",
+      "303/303 [==============================] - 0s 134us/sample - loss: 0.1247 - binary_accuracy: 0.8845\n",
+      "Epoch 729/1000\n",
+      "303/303 [==============================] - 0s 127us/sample - loss: 0.1308 - binary_accuracy: 0.8812\n",
+      "Epoch 730/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1305 - binary_accuracy: 0.8845\n",
+      "Epoch 731/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1257 - binary_accuracy: 0.8845\n",
+      "Epoch 732/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1241 - binary_accuracy: 0.8845\n",
+      "Epoch 733/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1232 - binary_accuracy: 0.8812\n",
+      "Epoch 734/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1250 - binary_accuracy: 0.8845\n",
+      "Epoch 735/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1270 - binary_accuracy: 0.8845\n",
+      "Epoch 736/1000\n",
+      "303/303 [==============================] - 0s 131us/sample - loss: 0.1272 - binary_accuracy: 0.8845\n",
+      "Epoch 737/1000\n",
+      "303/303 [==============================] - 0s 127us/sample - loss: 0.1290 - binary_accuracy: 0.8845\n",
+      "Epoch 738/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1262 - binary_accuracy: 0.8845\n",
+      "Epoch 739/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1247 - binary_accuracy: 0.8845\n",
+      "Epoch 740/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1266 - binary_accuracy: 0.8845\n",
+      "Epoch 741/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1250 - binary_accuracy: 0.8845\n",
+      "Epoch 742/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1279 - binary_accuracy: 0.8845\n",
+      "Epoch 743/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1285 - binary_accuracy: 0.8845\n",
+      "Epoch 744/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1250 - binary_accuracy: 0.8845\n",
+      "Epoch 745/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1241 - binary_accuracy: 0.8845\n",
+      "Epoch 746/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1250 - binary_accuracy: 0.8845\n",
+      "Epoch 747/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1259 - binary_accuracy: 0.8845\n",
+      "Epoch 748/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1278 - binary_accuracy: 0.8845\n",
+      "Epoch 749/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1249 - binary_accuracy: 0.8845\n",
+      "Epoch 750/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1251 - binary_accuracy: 0.8845\n",
+      "Epoch 751/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1249 - binary_accuracy: 0.8845\n",
+      "Epoch 752/1000\n",
+      "303/303 [==============================] - 0s 163us/sample - loss: 0.1238 - binary_accuracy: 0.8845\n",
+      "Epoch 753/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1276 - binary_accuracy: 0.8845\n",
+      "Epoch 754/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1268 - binary_accuracy: 0.8845\n",
+      "Epoch 755/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1253 - binary_accuracy: 0.8812\n",
+      "Epoch 756/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1266 - binary_accuracy: 0.8812\n",
+      "Epoch 757/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1246 - binary_accuracy: 0.8845\n",
+      "Epoch 758/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1235 - binary_accuracy: 0.8845\n",
+      "Epoch 759/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1230 - binary_accuracy: 0.8845\n",
+      "Epoch 760/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1231 - binary_accuracy: 0.8845\n",
+      "Epoch 761/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1239 - binary_accuracy: 0.8845\n",
+      "Epoch 762/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1235 - binary_accuracy: 0.8845\n",
+      "Epoch 763/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1242 - binary_accuracy: 0.8845\n",
+      "Epoch 764/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1254 - binary_accuracy: 0.8845\n",
+      "Epoch 765/1000\n",
+      "303/303 [==============================] - 0s 130us/sample - loss: 0.1242 - binary_accuracy: 0.8845\n",
+      "Epoch 766/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1258 - binary_accuracy: 0.8845\n",
+      "Epoch 767/1000\n",
+      "303/303 [==============================] - 0s 125us/sample - loss: 0.1263 - binary_accuracy: 0.8845\n",
+      "Epoch 768/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1228 - binary_accuracy: 0.8845\n",
+      "Epoch 769/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1272 - binary_accuracy: 0.8845\n",
+      "Epoch 770/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1249 - binary_accuracy: 0.8845\n",
+      "Epoch 771/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1253 - binary_accuracy: 0.8845\n",
+      "Epoch 772/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1241 - binary_accuracy: 0.8845\n",
+      "Epoch 773/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1234 - binary_accuracy: 0.8845\n",
+      "Epoch 774/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1240 - binary_accuracy: 0.8845\n",
+      "Epoch 775/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1232 - binary_accuracy: 0.8845\n",
+      "Epoch 776/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1229 - binary_accuracy: 0.8845\n",
+      "Epoch 777/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1232 - binary_accuracy: 0.8845\n",
+      "Epoch 778/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1239 - binary_accuracy: 0.8845\n",
+      "Epoch 779/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1254 - binary_accuracy: 0.8845\n",
+      "Epoch 780/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1253 - binary_accuracy: 0.8845\n",
+      "Epoch 781/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1250 - binary_accuracy: 0.8845\n",
+      "Epoch 782/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1245 - binary_accuracy: 0.8845\n",
+      "Epoch 783/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1250 - binary_accuracy: 0.8845\n",
+      "Epoch 784/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1239 - binary_accuracy: 0.8845\n",
+      "Epoch 785/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1229 - binary_accuracy: 0.8845\n",
+      "Epoch 786/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1228 - binary_accuracy: 0.8845\n",
+      "Epoch 787/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1233 - binary_accuracy: 0.8845\n",
+      "Epoch 788/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1221 - binary_accuracy: 0.8845\n",
+      "Epoch 789/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1221 - binary_accuracy: 0.8845\n",
+      "Epoch 790/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1228 - binary_accuracy: 0.8845\n",
+      "Epoch 791/1000\n",
+      "303/303 [==============================] - 0s 125us/sample - loss: 0.1248 - binary_accuracy: 0.8845\n",
+      "Epoch 792/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1232 - binary_accuracy: 0.8845\n",
+      "Epoch 793/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1230 - binary_accuracy: 0.8845\n",
+      "Epoch 794/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1232 - binary_accuracy: 0.8845\n",
+      "Epoch 795/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1223 - binary_accuracy: 0.8845\n",
+      "Epoch 796/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1224 - binary_accuracy: 0.8845\n",
+      "Epoch 797/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1233 - binary_accuracy: 0.8845\n",
+      "Epoch 798/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1221 - binary_accuracy: 0.8845\n",
+      "Epoch 799/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1261 - binary_accuracy: 0.8845\n",
+      "Epoch 800/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1257 - binary_accuracy: 0.8845\n",
+      "Epoch 801/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1255 - binary_accuracy: 0.8845\n",
+      "Epoch 802/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1230 - binary_accuracy: 0.8845\n",
+      "Epoch 803/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1252 - binary_accuracy: 0.8845\n",
+      "Epoch 804/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1235 - binary_accuracy: 0.8845\n",
+      "Epoch 805/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1220 - binary_accuracy: 0.8845\n",
+      "Epoch 806/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1223 - binary_accuracy: 0.8845\n",
+      "Epoch 807/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 808/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1243 - binary_accuracy: 0.8845\n",
+      "Epoch 809/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1270 - binary_accuracy: 0.8845\n",
+      "Epoch 810/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1224 - binary_accuracy: 0.8845\n",
+      "Epoch 811/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1219 - binary_accuracy: 0.8845\n",
+      "Epoch 812/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 813/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1236 - binary_accuracy: 0.8845\n",
+      "Epoch 814/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1236 - binary_accuracy: 0.8845\n",
+      "Epoch 815/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1220 - binary_accuracy: 0.8845\n",
+      "Epoch 816/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1225 - binary_accuracy: 0.8845\n",
+      "Epoch 817/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1222 - binary_accuracy: 0.8845\n",
+      "Epoch 818/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1235 - binary_accuracy: 0.8845\n",
+      "Epoch 819/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1262 - binary_accuracy: 0.8845\n",
+      "Epoch 820/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1247 - binary_accuracy: 0.8845\n",
+      "Epoch 821/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1243 - binary_accuracy: 0.8845\n",
+      "Epoch 822/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1230 - binary_accuracy: 0.8845\n",
+      "Epoch 823/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1230 - binary_accuracy: 0.8845\n",
+      "Epoch 824/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1219 - binary_accuracy: 0.8845\n",
+      "Epoch 825/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1235 - binary_accuracy: 0.8845\n",
+      "Epoch 826/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1231 - binary_accuracy: 0.8845\n",
+      "Epoch 827/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1221 - binary_accuracy: 0.8845\n",
+      "Epoch 828/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1217 - binary_accuracy: 0.8845\n",
+      "Epoch 829/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 830/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1219 - binary_accuracy: 0.8845\n",
+      "Epoch 831/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1224 - binary_accuracy: 0.8845\n",
+      "Epoch 832/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1212 - binary_accuracy: 0.8845\n",
+      "Epoch 833/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1214 - binary_accuracy: 0.8845\n",
+      "Epoch 834/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1213 - binary_accuracy: 0.8845\n",
+      "Epoch 835/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1240 - binary_accuracy: 0.8845\n",
+      "Epoch 836/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1248 - binary_accuracy: 0.8845\n",
+      "Epoch 837/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1248 - binary_accuracy: 0.8845\n",
+      "Epoch 838/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1242 - binary_accuracy: 0.8845\n",
+      "Epoch 839/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1229 - binary_accuracy: 0.8845\n",
+      "Epoch 840/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1223 - binary_accuracy: 0.8845\n",
+      "Epoch 841/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1211 - binary_accuracy: 0.8845\n",
+      "Epoch 842/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1213 - binary_accuracy: 0.8845\n",
+      "Epoch 843/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1212 - binary_accuracy: 0.8845\n",
+      "Epoch 844/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1225 - binary_accuracy: 0.8845\n",
+      "Epoch 845/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1212 - binary_accuracy: 0.8845\n",
+      "Epoch 846/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1229 - binary_accuracy: 0.8845\n",
+      "Epoch 847/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1223 - binary_accuracy: 0.8845\n",
+      "Epoch 848/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1218 - binary_accuracy: 0.8845\n",
+      "Epoch 849/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1213 - binary_accuracy: 0.8845\n",
+      "Epoch 850/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 851/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1206 - binary_accuracy: 0.8845\n",
+      "Epoch 852/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1212 - binary_accuracy: 0.8845\n",
+      "Epoch 853/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1208 - binary_accuracy: 0.8845\n",
+      "Epoch 854/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1204 - binary_accuracy: 0.8845\n",
+      "Epoch 855/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1221 - binary_accuracy: 0.8845\n",
+      "Epoch 856/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1212 - binary_accuracy: 0.8845\n",
+      "Epoch 857/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1212 - binary_accuracy: 0.8845\n",
+      "Epoch 858/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1223 - binary_accuracy: 0.8845\n",
+      "Epoch 859/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1224 - binary_accuracy: 0.8845\n",
+      "Epoch 860/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1227 - binary_accuracy: 0.8845\n",
+      "Epoch 861/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 862/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1253 - binary_accuracy: 0.8845\n",
+      "Epoch 863/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1240 - binary_accuracy: 0.8845\n",
+      "Epoch 864/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1226 - binary_accuracy: 0.8845\n",
+      "Epoch 865/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1220 - binary_accuracy: 0.8845\n",
+      "Epoch 866/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 867/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1205 - binary_accuracy: 0.8845\n",
+      "Epoch 868/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1222 - binary_accuracy: 0.8845\n",
+      "Epoch 869/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1211 - binary_accuracy: 0.8845\n",
+      "Epoch 870/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1227 - binary_accuracy: 0.8845\n",
+      "Epoch 871/1000\n",
+      "303/303 [==============================] - 0s 128us/sample - loss: 0.1213 - binary_accuracy: 0.8845\n",
+      "Epoch 872/1000\n",
+      "303/303 [==============================] - 0s 119us/sample - loss: 0.1230 - binary_accuracy: 0.8845\n",
+      "Epoch 873/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 874/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1218 - binary_accuracy: 0.8845\n",
+      "Epoch 875/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1217 - binary_accuracy: 0.8845\n",
+      "Epoch 876/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1207 - binary_accuracy: 0.8845\n",
+      "Epoch 877/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1205 - binary_accuracy: 0.8845\n",
+      "Epoch 878/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1212 - binary_accuracy: 0.8845\n",
+      "Epoch 879/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1202 - binary_accuracy: 0.8845\n",
+      "Epoch 880/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.1208 - binary_accuracy: 0.8845\n",
+      "Epoch 881/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1203 - binary_accuracy: 0.8845\n",
+      "Epoch 882/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1209 - binary_accuracy: 0.8845\n",
+      "Epoch 883/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1205 - binary_accuracy: 0.8845\n",
+      "Epoch 884/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1220 - binary_accuracy: 0.8845\n",
+      "Epoch 885/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1222 - binary_accuracy: 0.8845\n",
+      "Epoch 886/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1207 - binary_accuracy: 0.8845\n",
+      "Epoch 887/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1200 - binary_accuracy: 0.8845\n",
+      "Epoch 888/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1222 - binary_accuracy: 0.8845\n",
+      "Epoch 889/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1225 - binary_accuracy: 0.8845\n",
+      "Epoch 890/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1220 - binary_accuracy: 0.8845\n",
+      "Epoch 891/1000\n",
+      "303/303 [==============================] - 0s 116us/sample - loss: 0.1223 - binary_accuracy: 0.8845\n",
+      "Epoch 892/1000\n",
+      "303/303 [==============================] - 0s 120us/sample - loss: 0.1211 - binary_accuracy: 0.8845\n",
+      "Epoch 893/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1200 - binary_accuracy: 0.8845\n",
+      "Epoch 894/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1203 - binary_accuracy: 0.8845\n",
+      "Epoch 895/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1201 - binary_accuracy: 0.8845\n",
+      "Epoch 896/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1210 - binary_accuracy: 0.8845\n",
+      "Epoch 897/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1212 - binary_accuracy: 0.8845\n",
+      "Epoch 898/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1209 - binary_accuracy: 0.8845\n",
+      "Epoch 899/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1229 - binary_accuracy: 0.8845\n",
+      "Epoch 900/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1213 - binary_accuracy: 0.8845\n",
+      "Epoch 901/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1198 - binary_accuracy: 0.8845\n",
+      "Epoch 902/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1204 - binary_accuracy: 0.8845\n",
+      "Epoch 903/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1222 - binary_accuracy: 0.8845\n",
+      "Epoch 904/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1229 - binary_accuracy: 0.8845\n",
+      "Epoch 905/1000\n",
+      "303/303 [==============================] - 0s 117us/sample - loss: 0.1222 - binary_accuracy: 0.8845\n",
+      "Epoch 906/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1205 - binary_accuracy: 0.8845\n",
+      "Epoch 907/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1216 - binary_accuracy: 0.8845\n",
+      "Epoch 908/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.1219 - binary_accuracy: 0.8845\n",
+      "Epoch 909/1000\n",
+      "303/303 [==============================] - 0s 127us/sample - loss: 0.1239 - binary_accuracy: 0.8845\n",
+      "Epoch 910/1000\n",
+      "303/303 [==============================] - 0s 122us/sample - loss: 0.1214 - binary_accuracy: 0.8845\n",
+      "Epoch 911/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1197 - binary_accuracy: 0.8845\n",
+      "Epoch 912/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1241 - binary_accuracy: 0.8845\n",
+      "Epoch 913/1000\n",
+      "303/303 [==============================] - 0s 97us/sample - loss: 0.1213 - binary_accuracy: 0.8845\n",
+      "Epoch 914/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1213 - binary_accuracy: 0.8845\n",
+      "Epoch 915/1000\n",
+      "303/303 [==============================] - 0s 113us/sample - loss: 0.1200 - binary_accuracy: 0.8845\n",
+      "Epoch 916/1000\n",
+      "303/303 [==============================] - 0s 126us/sample - loss: 0.1199 - binary_accuracy: 0.8845\n",
+      "Epoch 917/1000\n",
+      "303/303 [==============================] - 0s 121us/sample - loss: 0.1207 - binary_accuracy: 0.8845\n",
+      "Epoch 918/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 919/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1206 - binary_accuracy: 0.8845\n",
+      "Epoch 920/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1207 - binary_accuracy: 0.8845\n",
+      "Epoch 921/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1209 - binary_accuracy: 0.8845\n",
+      "Epoch 922/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1206 - binary_accuracy: 0.8845\n",
+      "Epoch 923/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1208 - binary_accuracy: 0.8845\n",
+      "Epoch 924/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1216 - binary_accuracy: 0.8845\n",
+      "Epoch 925/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1216 - binary_accuracy: 0.8845\n",
+      "Epoch 926/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1200 - binary_accuracy: 0.8845\n",
+      "Epoch 927/1000\n",
+      "303/303 [==============================] - 0s 130us/sample - loss: 0.1196 - binary_accuracy: 0.8845\n",
+      "Epoch 928/1000\n",
+      "303/303 [==============================] - 0s 118us/sample - loss: 0.1196 - binary_accuracy: 0.8845\n",
+      "Epoch 929/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1191 - binary_accuracy: 0.8845\n",
+      "Epoch 930/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1203 - binary_accuracy: 0.8845\n",
+      "Epoch 931/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1195 - binary_accuracy: 0.8845\n",
+      "Epoch 932/1000\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.1197 - binary_accuracy: 0.8845\n",
+      "Epoch 933/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1211 - binary_accuracy: 0.8845\n",
+      "Epoch 934/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1221 - binary_accuracy: 0.8845\n",
+      "Epoch 935/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1204 - binary_accuracy: 0.8845\n",
+      "Epoch 936/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1206 - binary_accuracy: 0.8845\n",
+      "Epoch 937/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 938/1000\n",
+      "303/303 [==============================] - 0s 97us/sample - loss: 0.1223 - binary_accuracy: 0.8845\n",
+      "Epoch 939/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1200 - binary_accuracy: 0.8845\n",
+      "Epoch 940/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1205 - binary_accuracy: 0.8845\n",
+      "Epoch 941/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1226 - binary_accuracy: 0.8845\n",
+      "Epoch 942/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1258 - binary_accuracy: 0.8845\n",
+      "Epoch 943/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1218 - binary_accuracy: 0.8845\n",
+      "Epoch 944/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1230 - binary_accuracy: 0.8845\n",
+      "Epoch 945/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1194 - binary_accuracy: 0.8845\n",
+      "Epoch 946/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1191 - binary_accuracy: 0.8845\n",
+      "Epoch 947/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1207 - binary_accuracy: 0.8845\n",
+      "Epoch 948/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1202 - binary_accuracy: 0.8845\n",
+      "Epoch 949/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1211 - binary_accuracy: 0.8845\n",
+      "Epoch 950/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1204 - binary_accuracy: 0.8845\n",
+      "Epoch 951/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1208 - binary_accuracy: 0.8845\n",
+      "Epoch 952/1000\n",
+      "303/303 [==============================] - 0s 98us/sample - loss: 0.1209 - binary_accuracy: 0.8845\n",
+      "Epoch 953/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1214 - binary_accuracy: 0.8845\n",
+      "Epoch 954/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1225 - binary_accuracy: 0.8845\n",
+      "Epoch 955/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1233 - binary_accuracy: 0.8845\n",
+      "Epoch 956/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1222 - binary_accuracy: 0.8845\n",
+      "Epoch 957/1000\n",
+      "303/303 [==============================] - 0s 99us/sample - loss: 0.1227 - binary_accuracy: 0.8845\n",
+      "Epoch 958/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1194 - binary_accuracy: 0.8845\n",
+      "Epoch 959/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1198 - binary_accuracy: 0.8845\n",
+      "Epoch 960/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1216 - binary_accuracy: 0.8845\n",
+      "Epoch 961/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1222 - binary_accuracy: 0.8845\n",
+      "Epoch 962/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1192 - binary_accuracy: 0.8845\n",
+      "Epoch 963/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1194 - binary_accuracy: 0.8845\n",
+      "Epoch 964/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1190 - binary_accuracy: 0.8845\n",
+      "Epoch 965/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1206 - binary_accuracy: 0.8845\n",
+      "Epoch 966/1000\n",
+      "303/303 [==============================] - 0s 109us/sample - loss: 0.1213 - binary_accuracy: 0.8845\n",
+      "Epoch 967/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1207 - binary_accuracy: 0.8845\n",
+      "Epoch 968/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1204 - binary_accuracy: 0.8845\n",
+      "Epoch 969/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1214 - binary_accuracy: 0.8845\n",
+      "Epoch 970/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1226 - binary_accuracy: 0.8845\n",
+      "Epoch 971/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1208 - binary_accuracy: 0.8845\n",
+      "Epoch 972/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1214 - binary_accuracy: 0.8845\n",
+      "Epoch 973/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1206 - binary_accuracy: 0.8845\n",
+      "Epoch 974/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1202 - binary_accuracy: 0.8845\n",
+      "Epoch 975/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1210 - binary_accuracy: 0.8845\n",
+      "Epoch 976/1000\n",
+      "303/303 [==============================] - 0s 128us/sample - loss: 0.1210 - binary_accuracy: 0.8845\n",
+      "Epoch 977/1000\n",
+      "303/303 [==============================] - 0s 112us/sample - loss: 0.1186 - binary_accuracy: 0.8845\n",
+      "Epoch 978/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1196 - binary_accuracy: 0.8845\n",
+      "Epoch 979/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1192 - binary_accuracy: 0.8845\n",
+      "Epoch 980/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 981/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1215 - binary_accuracy: 0.8845\n",
+      "Epoch 982/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1189 - binary_accuracy: 0.8845\n",
+      "Epoch 983/1000\n",
+      "303/303 [==============================] - 0s 102us/sample - loss: 0.1186 - binary_accuracy: 0.8845\n",
+      "Epoch 984/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1190 - binary_accuracy: 0.8845\n",
+      "Epoch 985/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1181 - binary_accuracy: 0.8845\n",
+      "Epoch 986/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1189 - binary_accuracy: 0.8845\n",
+      "Epoch 987/1000\n",
+      "303/303 [==============================] - 0s 101us/sample - loss: 0.1184 - binary_accuracy: 0.8845\n",
+      "Epoch 988/1000\n",
+      "303/303 [==============================] - 0s 115us/sample - loss: 0.1185 - binary_accuracy: 0.8845\n",
+      "Epoch 989/1000\n",
+      "303/303 [==============================] - 0s 108us/sample - loss: 0.1182 - binary_accuracy: 0.8845\n",
+      "Epoch 990/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1189 - binary_accuracy: 0.8845\n",
+      "Epoch 991/1000\n",
+      "303/303 [==============================] - 0s 111us/sample - loss: 0.1188 - binary_accuracy: 0.8845\n",
+      "Epoch 992/1000\n",
+      "303/303 [==============================] - 0s 107us/sample - loss: 0.1216 - binary_accuracy: 0.8845\n",
+      "Epoch 993/1000\n",
+      "303/303 [==============================] - 0s 110us/sample - loss: 0.1189 - binary_accuracy: 0.8845\n",
+      "Epoch 994/1000\n",
+      "303/303 [==============================] - 0s 100us/sample - loss: 0.1193 - binary_accuracy: 0.8845\n",
+      "Epoch 995/1000\n",
+      "303/303 [==============================] - 0s 103us/sample - loss: 0.1186 - binary_accuracy: 0.8845\n",
+      "Epoch 996/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1193 - binary_accuracy: 0.8845\n",
+      "Epoch 997/1000\n",
+      "303/303 [==============================] - 0s 104us/sample - loss: 0.1207 - binary_accuracy: 0.8845\n",
+      "Epoch 998/1000\n",
+      "303/303 [==============================] - 0s 105us/sample - loss: 0.1214 - binary_accuracy: 0.8845\n",
+      "Epoch 999/1000\n",
+      "303/303 [==============================] - 0s 106us/sample - loss: 0.1192 - binary_accuracy: 0.8845\n",
+      "Epoch 1000/1000\n",
+      "303/303 [==============================] - 0s 114us/sample - loss: 0.1181 - binary_accuracy: 0.8845\n"
+     ]
     },
-    "nteract": {
-      "version": "0.15.0"
+    {
+     "data": {
+      "text/plain": [
+       "<tensorflow.python.keras.callbacks.History at 0x7fed704cbf60>"
+      ]
+     },
+     "execution_count": 142,
+     "metadata": {},
+     "output_type": "execute_result"
     }
+   ],
+   "source": [
+    "from sklearn.model_selection import RandomizedSearchCV\n",
+    "\n",
+    "# Static Parameters\n",
+    "inputs = X_transformed.shape[1]\n",
+    "wandb.config.epochs = 1000\n",
+    "\n",
+    "# Define model\n",
+    "model = Sequential()\n",
+    "model.add(Dense(13, input_shape=(inputs,)))\n",
+    "model.add(Dense(26, activation='sigmoid'))\n",
+    "model.add(Dense(13, activation='relu'))\n",
+    "model.add(Dense(1))\n",
+    "# Compile Model\n",
+    "model.compile(optimizer='adam', loss='cross-entropy', metrics=['binary_accuracy'])\n",
+    "\n",
+    "# Fit Model\n",
+    "\n",
+    "model.fit(X_transformed, y, \n",
+    "          epochs=wandb.config.epochs, \n",
+    "          callbacks=[WandbCallback()],\n",
+    "          verbose=0,\n",
+    "         )"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernel_info": {
+   "name": "python3"
+  },
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.6.8"
   },
-  "nbformat": 4,
-  "nbformat_minor": 2
-}
\ No newline at end of file
+  "nteract": {
+   "version": "0.15.0"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
diff --git a/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb b/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
index 559d58f..097fa19 100644
--- a/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
+++ b/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
@@ -31,7 +31,97 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[0, 0, 1],\n",
+       "       [0, 1, 1],\n",
+       "       [1, 0, 1],\n",
+       "       [0, 1, 0],\n",
+       "       [1, 0, 0],\n",
+       "       [1, 1, 1],\n",
+       "       [0, 0, 0]])"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/plain": [
+       "array([[0],\n",
+       "       [1],\n",
+       "       [1],\n",
+       "       [1],\n",
+       "       [1],\n",
+       "       [0],\n",
+       "       [0]])"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "# Load Data\n",
+    "import numpy as np\n",
+    "\n",
+    "\n",
+    "X = np.array((\n",
+    "    [0, 0, 1],\n",
+    "    [0, 1, 1],\n",
+    "    [1, 0, 1],\n",
+    "    [0, 1, 0],\n",
+    "    [1, 0, 0],\n",
+    "    [1, 1, 1],\n",
+    "    [0, 0, 0]\n",
+    "))\n",
+    "\n",
+    "y = np.array((\n",
+    "    [0], [1], [1], [1], [1], [0], [0]\n",
+    "))\n",
+    "\n",
+    "display(X, y)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "NameError",
+     "evalue": "name 'NeuralNetwork' is not defined",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-6-53ea8d73d50b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test given network class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Number of Epochs / Iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mNameError\u001b[0m: name 'NeuralNetwork' is not defined"
+     ]
+    }
+   ],
+   "source": [
+    "# Test given network class\n",
+    "nn = NeuralNetwork()\n",
+    "\n",
+    "# Number of Epochs / Iterations\n",
+    "for i in range(10000):\n",
+    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
+    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
+    "        print('Input: \\n', X)\n",
+    "        print('Actual Output: \\n', y)\n",
+    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
+    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
+    "    nn.train(X,y)\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -39,9 +129,686 @@
    },
    "outputs": [],
    "source": [
-    "##### Your Code Here #####"
+    "def gen_random_matrix(shape):\n",
+    "    return np.random.rand(shape[0], shape[1])\n",
+    "\n",
+    "# A lot of thought needs to go setting up the nodes.  Creating their size programatically\n",
+    "#  makes a lot of assumptions about hidden layer size (could be random, arbitrary, etc.).\n",
+    "#  One option would be to add a 'auto' feature that created hidden layers of X.shape + C\n",
+    "#  weights. Another would be to create pass a distribution and have the layer be a generator \n",
+    "#  of sorts whose parameters can be optimized.\n",
+    "\n",
+    "example_network_description = (\n",
+    "    ('input', X),  # row 0 must be input\n",
+    "    ('hidden_1', (X.shape[1], 3), 'simple random'),  # hidden vectors must match input vec\n",
+    "    ('hidden_2', (3, 7), 'simple random'),\n",
+    "    ('output', (7,1), 'simple random'),  # final active row must be output vector. match last hidden vec\n",
+    "    ('target', y) # last row in description must be the target vector\n",
+    ")\n",
+    "\n",
+    "\n",
+    "class LayerFactory():\n",
+    "    def __init__(self):\n",
+    "        return\n",
+    "    \n",
+    "    def initialize_weights(self, shape, algorithm):\n",
+    "        \"\"\"\n",
+    "        Lookup available weight formulas and generate pseudo-random numbers for initial weights\n",
+    "        of specified shape.\n",
+    "        \n",
+    "        \"\"\"\n",
+    "        available_generators = {\n",
+    "            'simple random': gen_random_matrix,\n",
+    "        }\n",
+    "        \n",
+    "        return available_generators[algorithm](shape)\n",
+    "    \n",
+    "    def generate_layers(self, description):\n",
+    "        \"\"\"\n",
+    "        Generate layers based on network description.\n",
+    "        \n",
+    "        ====Parameters====\n",
+    "        description: tuple or list object of layer descriptions ('name', shape)\n",
+    "        \"\"\"\n",
+    "        layers = {}\n",
+    "        layers[0] = Layer()\n",
+    "        layers[0].activated_values = description[0][1]\n",
+    "        for count, row in enumerate(description):\n",
+    "            if row[0] == 'target':\n",
+    "                layers[count] = Layer()\n",
+    "                layers[count].activated_values = description[len(description)-1][1]\n",
+    "                \n",
+    "            elif row[0] != 'input':\n",
+    "                layers[count] = Layer()\n",
+    "                layers[count].weights = self.initialize_weights(shape=row[1], algorithm=row[2])\n",
+    "                layers[count].weighted_sum = 0\n",
+    "                layers[count].activated_values = 0\n",
+    "            \n",
+    "        \n",
+    "        return layers\n",
+    "    \n",
+    "    \n",
+    "class Layer():\n",
+    "    pass\n",
+    "    \n",
+    "\n",
+    "class NeuralNetwork(LayerFactory):\n",
+    "    def __init__(self, description):\n",
+    "        # Set up Architecture of Neural Network\n",
+    "        self.description = description\n",
+    "        self.layers = self.generate_layers(description)\n",
+    "        \n",
+    "    def sigmoid(self, weighted_sum):\n",
+    "        return 1 / (1+np.exp(-weighted_sum))\n",
+    "    \n",
+    "    def sigmoidPrime(self, weighted_sum):\n",
+    "        return weighted_sum * (1 - weighted_sum)\n",
+    "    \n",
+    "    def feed_forward(self, X):\n",
+    "        \"\"\"\n",
+    "        Calculate the NN inference using feed forward.\n",
+    "        aka \"predict\"\n",
+    "        \"\"\"\n",
+    "        for i in range(1, len(self.layers)-1):\n",
+    "            # Weighted sum of inputs\n",
+    "            #  Check if first layer (required to use feed_forward method as Predict)\n",
+    "            if i == 1:\n",
+    "                self.layers[i].weighted_sum = np.dot(X, self.layers[i].weights)\n",
+    "                # Activated values (local outputs)\n",
+    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
+    "            else:\n",
+    "                self.layers[i].weighted_sum = np.dot(self.layers[i-1].activated_values, self.layers[i].weights)\n",
+    "                # Activated values (local outputs)\n",
+    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
+    "\n",
+    "        return self.layers[len(self.layers)-2].activated_values\n",
+    "        \n",
+    "    def backward(self, X, y, net_output, learning_rate):\n",
+    "        \"\"\"\n",
+    "        Backward propagate through the network\n",
+    "        \"\"\"\n",
+    "        # Step 1: Calculate errors and delta shifts for each layer (backward)\n",
+    "        back_prop_pos = 0\n",
+    "        for i in range(len(self.layers)-2, 0, -1):\n",
+    "            # Error in local output\n",
+    "            #   Check if first backprop\n",
+    "            if back_prop_pos == 0:\n",
+    "                self.layers[i].error = self.layers[i+1].activated_values - net_output\n",
+    "                # Apply Derivative of Sigmoid to error\n",
+    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(net_output)\n",
+    "            else:\n",
+    "                self.layers[i].error = self.layers[i+1].delta.dot(self.layers[i+1].weights.T)\n",
+    "                # Apply Derivative of Sigmoid to error\n",
+    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(\n",
+    "                    self.layers[i].activated_values)*learning_rate\n",
+    "                \n",
+    "            back_prop_pos += 1\n",
+    "            \n",
+    "        # Step 2: Calculate adjustments and apply to each layer (forward)\n",
+    "        for i in range(1, len(self.layers)-1):\n",
+    "            self.layers[i].weights += self.layers[i-1].activated_values.T.dot(self.layers[i].delta)\n",
+    "        \n",
+    "    def train(self, X, y, learning_rate):\n",
+    "        net_output = self.feed_forward(X)\n",
+    "        self.backward(X, y, net_output, learning_rate)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "NameError",
+     "evalue": "name 'layers' is not defined",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-9-1bdf468a0e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Testing block.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
+     ]
+    }
+   ],
+   "source": [
+    "# Testing block.\n",
+    "for i in range(len(layers)-2, 0, -1):\n",
+    "    print(i)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 10,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "{0: <__main__.Layer at 0x7f5a3cef6470>,\n",
+       " 1: <__main__.Layer at 0x7f5a3cef66d8>,\n",
+       " 2: <__main__.Layer at 0x7f5a3cef65c0>,\n",
+       " 3: <__main__.Layer at 0x7f5a3cef6c18>,\n",
+       " 4: <__main__.Layer at 0x7f5a3cef6630>}"
+      ]
+     },
+     "execution_count": 10,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# Test LayerFactory\n",
+    "factory = LayerFactory()\n",
+    "layers = factory.generate_layers(description=example_network_description)\n",
+    "layers"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "metadata": {
+    "scrolled": true
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[0.62710878, 0.95262447, 0.80739135, 0.37436599, 0.42188375,\n",
+       "        0.02954279, 0.76865005],\n",
+       "       [0.778027  , 0.51607266, 0.87361569, 0.35606138, 0.00713691,\n",
+       "        0.77170409, 0.34532938],\n",
+       "       [0.10597956, 0.44483607, 0.55169943, 0.90002817, 0.33284319,\n",
+       "        0.26650898, 0.40674867]])"
+      ]
+     },
+     "execution_count": 11,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "layers[2].weights"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "+---------EPOCH 1---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.90001542]\n",
+      " [0.90943558]\n",
+      " [0.90684305]\n",
+      " [0.8997136 ]\n",
+      " [0.89520821]\n",
+      " [0.91293224]\n",
+      " [0.88283682]]\n",
+      "Loss: \n",
+      " 0.35154181530121204\n",
+      "+---------EPOCH 2---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.77460858]\n",
+      " [0.78528419]\n",
+      " [0.78241465]\n",
+      " [0.77425127]\n",
+      " [0.76964302]\n",
+      " [0.78951931]\n",
+      " [0.75671251]]\n",
+      "Loss: \n",
+      " 0.2847780148880342\n",
+      "+---------EPOCH 3---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.52184464]\n",
+      " [0.5233401 ]\n",
+      " [0.52332309]\n",
+      " [0.52171843]\n",
+      " [0.52196283]\n",
+      " [0.52432127]\n",
+      " [0.52013619]]\n",
+      "Loss: \n",
+      " 0.24706780520505797\n",
+      "+---------EPOCH 4---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.62776169]\n",
+      " [0.63412271]\n",
+      " [0.63262421]\n",
+      " [0.62750785]\n",
+      " [0.62533207]\n",
+      " [0.63691294]\n",
+      " [0.61787375]]\n",
+      "Loss: \n",
+      " 0.2470669193516853\n",
+      "+---------EPOCH 5---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.52870734]\n",
+      " [0.53053361]\n",
+      " [0.53042199]\n",
+      " [0.52857262]\n",
+      " [0.52866415]\n",
+      " [0.531642  ]\n",
+      " [0.52644707]]\n",
+      "Loss: \n",
+      " 0.24637494842869323\n",
+      "+---------EPOCH 1000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.57265846]\n",
+      " [0.57510173]\n",
+      " [0.57995036]\n",
+      " [0.57328448]\n",
+      " [0.58087405]\n",
+      " [0.57863114]\n",
+      " [0.56706002]]\n",
+      "Loss: \n",
+      " 0.24272024326609046\n",
+      "+---------EPOCH 2000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.4040769 ]\n",
+      " [0.59320448]\n",
+      " [0.65186825]\n",
+      " [0.61962146]\n",
+      " [0.69956202]\n",
+      " [0.69478392]\n",
+      " [0.37953314]]\n",
+      "Loss: \n",
+      " 0.18738248179055378\n",
+      "+---------EPOCH 3000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.10092761]\n",
+      " [0.48940541]\n",
+      " [0.56865713]\n",
+      " [0.8634187 ]\n",
+      " [0.8746409 ]\n",
+      " [0.34653972]\n",
+      " [0.0452725 ]]\n",
+      "Loss: \n",
+      " 0.08763694524949416\n",
+      "+---------EPOCH 4000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[0.08091188]\n",
+      " [0.8734761 ]\n",
+      " [0.88304562]\n",
+      " [0.97397462]\n",
+      " [0.95350244]\n",
+      " [0.1628851 ]\n",
+      " [0.0017191 ]]\n",
+      "Loss: \n",
+      " 0.009372458605793227\n",
+      "+---------EPOCH 5000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[4.30170919e-02]\n",
+      " [9.31355568e-01]\n",
+      " [9.38678693e-01]\n",
+      " [9.83203035e-01]\n",
+      " [9.66586221e-01]\n",
+      " [8.92664319e-02]\n",
+      " [4.02783372e-04]]\n",
+      "Loss: \n",
+      " 0.0028128725309214094\n",
+      "+---------EPOCH 6000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[2.99132380e-02]\n",
+      " [9.51022930e-01]\n",
+      " [9.57444951e-01]\n",
+      " [9.87180890e-01]\n",
+      " [9.73258204e-01]\n",
+      " [6.42122687e-02]\n",
+      " [2.07938663e-04]]\n",
+      "Loss: \n",
+      " 0.0014438856107908897\n",
+      "+---------EPOCH 7000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[2.34180313e-02]\n",
+      " [9.60906009e-01]\n",
+      " [9.66690859e-01]\n",
+      " [9.89452644e-01]\n",
+      " [9.77320645e-01]\n",
+      " [5.15125696e-02]\n",
+      " [1.38852037e-04]]\n",
+      "Loss: \n",
+      " 0.00092362959351063\n",
+      "+---------EPOCH 8000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[1.95161844e-02]\n",
+      " [9.66933204e-01]\n",
+      " [9.72232462e-01]\n",
+      " [9.90937205e-01]\n",
+      " [9.80075351e-01]\n",
+      " [4.37160773e-02]\n",
+      " [1.04445311e-04]]\n",
+      "Loss: \n",
+      " 0.0006622232624017801\n",
+      "+---------EPOCH 9000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[1.68933903e-02]\n",
+      " [9.71038848e-01]\n",
+      " [9.75953927e-01]\n",
+      " [9.91991258e-01]\n",
+      " [9.82082707e-01]\n",
+      " [3.83793639e-02]\n",
+      " [8.40121026e-05]]\n",
+      "Loss: \n",
+      " 0.0005086429369579763\n",
+      "+---------EPOCH 10000---------+\n",
+      "Input: \n",
+      " [[0 0 1]\n",
+      " [0 1 1]\n",
+      " [1 0 1]\n",
+      " [0 1 0]\n",
+      " [1 0 0]\n",
+      " [1 1 1]\n",
+      " [0 0 0]]\n",
+      "Actual Output: \n",
+      " [[0]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [1]\n",
+      " [0]\n",
+      " [0]]\n",
+      "Predicted Output: \n",
+      " [[1.49971830e-02]\n",
+      " [9.74041114e-01]\n",
+      " [9.78643390e-01]\n",
+      " [9.92783070e-01]\n",
+      " [9.83621338e-01]\n",
+      " [3.44625973e-02]\n",
+      " [7.05106756e-05]]\n",
+      "Loss: \n",
+      " 0.00040898633108132074\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Ensure layers being created correctly in NN\n",
+    "nn = NeuralNetwork(description=example_network_description)\n",
+    "# Number of Epochs / Iterations\n",
+    "for i in range(10000):\n",
+    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
+    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
+    "        print('Input: \\n', X)\n",
+    "        print('Actual Output: \\n', y)\n",
+    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
+    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
+    "    nn.train(X,y,0.1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 128,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[0.57969291, 0.88623463, 0.82354341, 0.28966033, 0.25362587,\n",
+       "        0.90120771, 0.7845741 ],\n",
+       "       [0.67906782, 0.29905237, 0.27333864, 0.60150628, 0.25300745,\n",
+       "        0.6969853 , 0.02095351],\n",
+       "       [0.16736435, 0.54543392, 0.53045375, 0.86785617, 0.26756957,\n",
+       "        0.06520021, 0.35142303]])"
+      ]
+     },
+     "execution_count": 128,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "nn.layers[1].weights"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Building Flexible Perceptron Network"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 158,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# SEE ABOVE!! WOOT!!"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
   {
    "cell_type": "markdown",
    "metadata": {
@@ -73,7 +840,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 39,
+   "execution_count": 146,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -84,7 +851,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 147,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -94,31 +861,31 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 31,
+   "execution_count": 148,
    "metadata": {},
    "outputs": [],
    "source": [
     "# the data, split between train and test sets\n",
-    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
+    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 32,
+   "execution_count": 149,
    "metadata": {},
    "outputs": [],
    "source": [
-    "x_train = x_train.reshape(x_train.shape[0], img_rows * img_cols)\n",
-    "x_test = x_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
+    "X_train = X_train.reshape(X_train.shape[0], img_rows * img_cols)\n",
+    "X_test = X_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
     "\n",
     "# Normalize Our Data\n",
-    "x_train = x_train / 255\n",
-    "x_test = x_test / 255"
+    "X_train = X_train / 255\n",
+    "X_test = X_test / 255"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 16,
+   "execution_count": 150,
    "metadata": {},
    "outputs": [
     {
@@ -127,7 +894,7 @@
        "(60000, 784)"
       ]
      },
-     "execution_count": 16,
+     "execution_count": 150,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -147,7 +914,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 33,
+   "execution_count": 151,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -164,35 +931,682 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 40,
+   "execution_count": 152,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
-       "array([0., 1., 0., ..., 0., 0., 0.])"
+       "array([[0.],\n",
+       "       [1.],\n",
+       "       [0.],\n",
+       "       ...,\n",
+       "       [0.],\n",
+       "       [0.],\n",
+       "       [0.]])"
       ]
      },
-     "execution_count": 40,
+     "execution_count": 152,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "# A Nice Binary target for ya to work with\n",
+    "y_train = y_train.reshape(-1,1)\n",
     "y_train"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 161,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
+       "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
+       "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
+       "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
+       "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
+       "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
+       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
+       "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
+       "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
+       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
+       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
+       "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
+       "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
+       "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
+       "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
+       "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
+       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
+       "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
+       "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
+       "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
+       "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
+       "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
+       "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
+       "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
+       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
+       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
+       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
+       "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
+       "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
+       "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
+       "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
+       "       0.        , 0.        , 0.        , 0.        ])"
+      ]
+     },
+     "execution_count": 161,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "X_train[0]"
+   ]
+  },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "### Estimating Your `net"
+    "### Balance the class"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 268,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>0</th>\n",
+       "      <th>1</th>\n",
+       "      <th>2</th>\n",
+       "      <th>3</th>\n",
+       "      <th>4</th>\n",
+       "      <th>5</th>\n",
+       "      <th>6</th>\n",
+       "      <th>7</th>\n",
+       "      <th>8</th>\n",
+       "      <th>9</th>\n",
+       "      <th>...</th>\n",
+       "      <th>775</th>\n",
+       "      <th>776</th>\n",
+       "      <th>777</th>\n",
+       "      <th>778</th>\n",
+       "      <th>779</th>\n",
+       "      <th>780</th>\n",
+       "      <th>781</th>\n",
+       "      <th>782</th>\n",
+       "      <th>783</th>\n",
+       "      <th>target</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.0</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "<p>2 rows × 785 columns</p>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "     0    1    2    3    4    5    6    7    8    9  ...  775  776  777  778  \\\n",
+       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
+       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
+       "\n",
+       "   779  780  781  782  783  target  \n",
+       "0  0.0  0.0  0.0  0.0  0.0     0.0  \n",
+       "1  0.0  0.0  0.0  0.0  0.0     1.0  \n",
+       "\n",
+       "[2 rows x 785 columns]"
+      ]
+     },
+     "execution_count": 268,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "import pandas as pd\n",
+    "\n",
+    "# concatenate the data\n",
+    "df_train = pd.DataFrame(X_train)\n",
+    "df_train['target'] = y_train\n",
+    "df_train.head(2)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 274,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>0</th>\n",
+       "      <th>1</th>\n",
+       "      <th>2</th>\n",
+       "      <th>3</th>\n",
+       "      <th>4</th>\n",
+       "      <th>5</th>\n",
+       "      <th>6</th>\n",
+       "      <th>7</th>\n",
+       "      <th>8</th>\n",
+       "      <th>9</th>\n",
+       "      <th>...</th>\n",
+       "      <th>775</th>\n",
+       "      <th>776</th>\n",
+       "      <th>777</th>\n",
+       "      <th>778</th>\n",
+       "      <th>779</th>\n",
+       "      <th>780</th>\n",
+       "      <th>781</th>\n",
+       "      <th>782</th>\n",
+       "      <th>783</th>\n",
+       "      <th>target</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>count</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>11846.000000</td>\n",
+       "      <td>11846.000000</td>\n",
+       "      <td>11846.000000</td>\n",
+       "      <td>11846.000000</td>\n",
+       "      <td>11846.000000</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.0</td>\n",
+       "      <td>11846.000000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>mean</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.000185</td>\n",
+       "      <td>0.000030</td>\n",
+       "      <td>0.000014</td>\n",
+       "      <td>0.000081</td>\n",
+       "      <td>0.000021</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.500000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>std</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.011510</td>\n",
+       "      <td>0.003279</td>\n",
+       "      <td>0.001549</td>\n",
+       "      <td>0.008792</td>\n",
+       "      <td>0.002234</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.500021</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>min</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.000000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>25%</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.000000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>50%</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.500000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>75%</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.000000</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.000000</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>max</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>...</td>\n",
+       "      <td>0.996078</td>\n",
+       "      <td>0.356863</td>\n",
+       "      <td>0.168627</td>\n",
+       "      <td>0.956863</td>\n",
+       "      <td>0.243137</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>1.000000</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "<p>8 rows × 785 columns</p>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "             0        1        2        3        4        5        6        7  \\\n",
+       "count  11846.0  11846.0  11846.0  11846.0  11846.0  11846.0  11846.0  11846.0   \n",
+       "mean       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "std        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "min        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "25%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "50%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "75%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "max        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
+       "\n",
+       "             8        9  ...           775           776           777  \\\n",
+       "count  11846.0  11846.0  ...  11846.000000  11846.000000  11846.000000   \n",
+       "mean       0.0      0.0  ...      0.000185      0.000030      0.000014   \n",
+       "std        0.0      0.0  ...      0.011510      0.003279      0.001549   \n",
+       "min        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
+       "25%        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
+       "50%        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
+       "75%        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
+       "max        0.0      0.0  ...      0.996078      0.356863      0.168627   \n",
+       "\n",
+       "                778           779      780      781      782      783  \\\n",
+       "count  11846.000000  11846.000000  11846.0  11846.0  11846.0  11846.0   \n",
+       "mean       0.000081      0.000021      0.0      0.0      0.0      0.0   \n",
+       "std        0.008792      0.002234      0.0      0.0      0.0      0.0   \n",
+       "min        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
+       "25%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
+       "50%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
+       "75%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
+       "max        0.956863      0.243137      0.0      0.0      0.0      0.0   \n",
+       "\n",
+       "             target  \n",
+       "count  11846.000000  \n",
+       "mean       0.500000  \n",
+       "std        0.500021  \n",
+       "min        0.000000  \n",
+       "25%        0.000000  \n",
+       "50%        0.500000  \n",
+       "75%        1.000000  \n",
+       "max        1.000000  \n",
+       "\n",
+       "[8 rows x 785 columns]"
+      ]
+     },
+     "execution_count": 274,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "from sklearn.utils import resample\n",
+    "# try to get a balanced sample\n",
+    "\n",
+    "# Separate majority/minority classes\n",
+    "df_majority = df_train[df_train.target == 0]\n",
+    "df_minority = df_train[df_train.target == 1]\n",
+    "\n",
+    "\n",
+    "# Downsample majority class\n",
+    "df_majority_downsampled = resample(\n",
+    "                                    df_majority,\n",
+    "                                    replace=False,\n",
+    "                                    n_samples=5923,\n",
+    "                                    random_state=42\n",
+    "                                  )\n",
+    "\n",
+    "df_downsampled = pd.concat([\n",
+    "    df_majority_downsampled,\n",
+    "    df_minority\n",
+    "])\n",
+    "\n",
+    "df_downsampled.describe()"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Estimating Your  Net"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 283,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -201,7 +1615,222 @@
    "outputs": [],
    "source": [
     "##### Your Code Here #####\n",
-    "\n"
+    "\n",
+    "# Use balanced training set\n",
+    "X = df_downsampled.drop(columns='target')\n",
+    "y = df_downsampled.target.to_numpy().reshape(-1,1)\n",
+    "\n",
+    "network_description = (\n",
+    "    ('input', X),  # row 0 must be input\n",
+    "    ('hidden_1', (X.shape[1], 768), 'simple random'),  # hidden vectors must match input vec\n",
+    "    ('hidden_2', (768, 50), 'simple random'),\n",
+    "    ('hidden_3', (50, 10), 'simple random'),\n",
+    "    ('output', (10,1), 'simple random'),  # final active row must be output vector. match last hidden vec\n",
+    "    ('target', y) # last row in description must be the target vector\n",
+    ")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 284,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "(11846, 784)"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/plain": [
+       "(11846, 1)"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "display(X.shape, y.shape)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 286,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "+---------EPOCH 1---------+\n",
+      "Loss: \n",
+      " 0.4796212892485052\n",
+      "+---------EPOCH 2---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 3---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 4---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 5---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 100---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 200---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 300---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 400---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 500---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 600---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 700---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 800---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 900---------+\n",
+      "Loss: \n",
+      " 0.5\n",
+      "+---------EPOCH 1000---------+\n",
+      "Loss: \n",
+      " 0.5\n"
+     ]
+    }
+   ],
+   "source": [
+    "network = NeuralNetwork(description=network_description)\n",
+    "# Number of Epochs / Iterations\n",
+    "losses = []\n",
+    "for i in range(1000):\n",
+    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 100 ==0):\n",
+    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
+    "        loss = np.mean(np.square(y - network.feed_forward(X)))\n",
+    "        losses.append(loss)\n",
+    "        print(\"Loss: \\n\", str(np.mean(np.square(y - network.feed_forward(X)))))\n",
+    "    network.train(X, y, 0.1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 253,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "1.0"
+      ]
+     },
+     "execution_count": 253,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "np.mean(network.layers[len(network.layers)-1].activated_values == y_train)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 254,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "image/png": "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\n",
+      "text/plain": [
+       "<Figure size 432x288 with 1 Axes>"
+      ]
+     },
+     "metadata": {
+      "needs_background": "light"
+     },
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "# Visualize losses\n",
+    "import matplotlib.pyplot as plt\n",
+    "\n",
+    "plt.plot(losses)\n",
+    "plt.show();"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 255,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "0.902"
+      ]
+     },
+     "execution_count": 255,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# Calculate on test set\n",
+    "np.mean(network.feed_forward(X_test) == y_test.reshape(-1,1))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 259,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "0.09871666666666666"
+      ]
+     },
+     "execution_count": 259,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# What is the majority class?\n",
+    "np.mean(y_train)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# It's probably guessing everything is a one.  Would yield similar efficiency.  Does not\n",
+    "#  Beat majority classifier."
    ]
   },
   {
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
index ca65dc6..2eefc23 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Assignment.ipynb
@@ -91,9 +91,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.6.8"
   }
  },
  "nbformat": 4,
- "nbformat_minor": 2
+ "nbformat_minor": 4
 }
