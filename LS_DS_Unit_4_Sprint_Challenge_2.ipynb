{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:  A node in network consisting of a set of weights to be applied to inputs and optionally a transform (activation function) dictating output.\n",
    "- **Input Layer:  The processed data layer - often an unweighted ingest of normalized/scaled data to the first hidden layer.\n",
    "- **Hidden Layer:  A layer that lies between the input and output layers.\n",
    "- **Output Layer:  A final set of nodes in the shape of desired output.  May use alternate activation functions to get explicity output ranges.\n",
    "- **Activation:  Output transformation at the neuron level - f(x) where x = sum(weights*inputs). \n",
    "- **Backpropagation:  Propogation of errors from the output layer backward through the network calculating the partial dependence on weights at each layer/node.  The backbone of neural networks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5,           chocolate         gummy           ate\n",
       " count  10000.000000  10000.000000  10000.000000\n",
       " mean       0.499100      0.499300      0.500000\n",
       " std        0.500024      0.500025      0.500025\n",
       " min        0.000000      0.000000      0.000000\n",
       " 25%        0.000000      0.000000      0.000000\n",
       " 50%        0.000000      0.000000      0.500000\n",
       " 75%        1.000000      1.000000      1.000000\n",
       " max        1.000000      1.000000      1.000000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Majority Class\n",
    "\n",
    "candy.ate.mean(), candy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "\n",
    "y = candy['ate'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (10000, 2), (10000, 1))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom implementation of flexible dense network with numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def gen_random_matrix(shape):\n",
    "    np.random.seed()\n",
    "    return np.random.rand(shape[0], shape[1])\n",
    "\n",
    "# A lot of thought needs to go setting up the nodes.  Creating their size programatically\n",
    "#  makes a lot of assumptions about hidden layer size (could be random, arbitrary, etc.).\n",
    "#  One option would be to add a 'auto' feature that created hidden layers of X.shape + C\n",
    "#  weights. Another would be to create pass a distribution and have the layer be a generator \n",
    "#  of sorts whose parameters can be optimized.\n",
    "\n",
    "example_network_description = (\n",
    "    ('input', X),  # row 0 must be input\n",
    "    ('hidden_1', (X.shape[1], 3), 'simple random'),  # hidden vectors must match input vec\n",
    "    ('hidden_2', (3, 7), 'simple random'),\n",
    "    ('output', (7,1), 'simple random'),  # final active row must be output vector. match last hidden vec\n",
    "    ('target', y) # last row in description must be the target vector\n",
    ")\n",
    "\n",
    "\n",
    "class LayerFactory():\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def initialize_weights(self, shape, algorithm):\n",
    "        \"\"\"\n",
    "        Lookup available weight formulas and generate pseudo-random numbers for initial weights\n",
    "        of specified shape.\n",
    "        \n",
    "        \"\"\"\n",
    "        available_generators = {\n",
    "            'simple random': gen_random_matrix,\n",
    "        }\n",
    "        \n",
    "        return available_generators[algorithm](shape)\n",
    "    \n",
    "    def generate_layers(self, description):\n",
    "        \"\"\"\n",
    "        Generate layers based on network description.\n",
    "        \n",
    "        ====Parameters====\n",
    "        description: tuple or list object of layer descriptions ('name', shape)\n",
    "        \"\"\"\n",
    "        layers = {}\n",
    "        layers[0] = Layer()\n",
    "        layers[0].activated_values = description[0][1]\n",
    "        for count, row in enumerate(description):\n",
    "            if row[0] == 'target':\n",
    "                layers[count] = Layer()\n",
    "                layers[count].activated_values = description[len(description)-1][1]\n",
    "                \n",
    "            elif row[0] != 'input':\n",
    "                layers[count] = Layer()\n",
    "                layers[count].weights = self.initialize_weights(shape=row[1], algorithm=row[2])\n",
    "                layers[count].weighted_sum = 0\n",
    "                layers[count].activated_values = 0\n",
    "            \n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    \n",
    "class Layer():\n",
    "    pass\n",
    "    \n",
    "\n",
    "class NeuralNetwork(LayerFactory):\n",
    "    def __init__(self, description):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.description = description\n",
    "        self.layers = self.generate_layers(description)\n",
    "        \n",
    "    def sigmoid(self, weighted_sum):\n",
    "        return 1 / (1+np.exp(-weighted_sum))\n",
    "    \n",
    "    def sigmoidPrime(self, weighted_sum):\n",
    "        return weighted_sum * (1 - weighted_sum)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        for i in range(1, len(self.layers)-1):\n",
    "            # Weighted sum of inputs\n",
    "            #  Check if first layer (required to use feed_forward method as Predict)\n",
    "            if i == 1:\n",
    "                self.layers[i].weighted_sum = np.dot(X, self.layers[i].weights)\n",
    "                # Activated values (local outputs)\n",
    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
    "            else:\n",
    "                self.layers[i].weighted_sum = np.dot(self.layers[i-1].activated_values, self.layers[i].weights)\n",
    "                # Activated values (local outputs)\n",
    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
    "\n",
    "        return self.layers[len(self.layers)-2].activated_values\n",
    "        \n",
    "    def backward(self, X, y, net_output, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        # Step 1: Calculate errors and delta shifts for each layer (backward)\n",
    "        back_prop_pos = 0\n",
    "        for i in range(len(self.layers)-2, 0, -1):\n",
    "            # Error in local output\n",
    "            #   Check if first backprop\n",
    "            if back_prop_pos == 0:\n",
    "                self.layers[i].error = y - net_output\n",
    "                # Apply Derivative of Sigmoid to error\n",
    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(net_output) * learning_rate\n",
    "            else:\n",
    "                self.layers[i].error = self.layers[i+1].delta.dot(self.layers[i+1].weights.T)\n",
    "                # Apply Derivative of Sigmoid to error\n",
    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(\n",
    "                    self.layers[i].activated_values)\n",
    "                \n",
    "            back_prop_pos += 1\n",
    "            \n",
    "        # Step 2: Calculate adjustments and apply to each layer (forward)\n",
    "        for i in range(1, len(self.layers)-1):\n",
    "            self.layers[i].weights += self.layers[i-1].activated_values.T.dot(self.layers[i].delta)\n",
    "        \n",
    "    def train(self, X, y, learning_rate):\n",
    "        net_output = self.feed_forward(X)\n",
    "        self.backward(X, y, net_output, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Weights: \n",
      " [[0.47432446]\n",
      " [0.16792762]]\n",
      "Loss: \n",
      " 0.2590575177989399\n",
      "+---------EPOCH 5---------+\n",
      "Weights: \n",
      " [[-13.89552077]\n",
      " [ -8.49463178]]\n",
      "Loss: \n",
      " 0.5487530406363103\n",
      "+---------EPOCH 1000---------+\n",
      "Weights: \n",
      " [[-19.54241453]\n",
      " [  2.8912209 ]]\n",
      "Loss: \n",
      " 0.32526107670787996\n",
      "+---------EPOCH 2000---------+\n",
      "Weights: \n",
      " [[-19.54082341]\n",
      " [  2.89122091]]\n",
      "Loss: \n",
      " 0.32526107670281657\n",
      "+---------EPOCH 3000---------+\n",
      "Weights: \n",
      " [[-19.53922975]\n",
      " [  2.89122091]]\n",
      "Loss: \n",
      " 0.3252610766977371\n",
      "+---------EPOCH 4000---------+\n",
      "Weights: \n",
      " [[-19.53763355]\n",
      " [  2.89122091]]\n",
      "Loss: \n",
      " 0.32526107669264137\n",
      "+---------EPOCH 5000---------+\n",
      "Weights: \n",
      " [[-19.5360348 ]\n",
      " [  2.89122091]]\n",
      "Loss: \n",
      " 0.3252610766875294\n",
      "+---------EPOCH 6000---------+\n",
      "Weights: \n",
      " [[-19.53443349]\n",
      " [  2.89122091]]\n",
      "Loss: \n",
      " 0.325261076682401\n",
      "+---------EPOCH 7000---------+\n",
      "Weights: \n",
      " [[-19.53282961]\n",
      " [  2.89122092]]\n",
      "Loss: \n",
      " 0.3252610766772561\n",
      "+---------EPOCH 8000---------+\n",
      "Weights: \n",
      " [[-19.53122315]\n",
      " [  2.89122092]]\n",
      "Loss: \n",
      " 0.32526107667209464\n",
      "+---------EPOCH 9000---------+\n",
      "Weights: \n",
      " [[-19.5296141 ]\n",
      " [  2.89122092]]\n",
      "Loss: \n",
      " 0.3252610766669166\n",
      "+---------EPOCH 10000---------+\n",
      "Weights: \n",
      " [[-19.52800247]\n",
      " [  2.89122092]]\n",
      "Loss: \n",
      " 0.3252610766617218\n"
     ]
    }
   ],
   "source": [
    "# Describe single perceptron network\n",
    "single_layer =(\n",
    "    ('input', X),  # row 0 must be input\n",
    "    ('output', (X.shape[1], 1), 'simple random'),  # shape of first layer of (input_dim, #nodes)\n",
    "    ('target', y) # last row in description must be the target vector\n",
    ")\n",
    "\n",
    "nn_singlet = NeuralNetwork(single_layer)\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1, 5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Weights: \\n', nn_singlet.layers[1].weights)\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn_singlet.feed_forward(X)))))\n",
    "    nn_singlet.train(X, y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7229"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check actual accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(nn_singlet.feed_forward(X).round(), y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Better Than Majority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A single Perceptron** in with only two weights and a sigmoid activation function is highly limited.  Translating these values back to binary via simple round can vary error significantly.  Simply adding more nodes to allow for cooperative interpretation of weights might be helpful, but another layer is required to transform the output into single probability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Loss: \n",
      " 0.262119387525705\n",
      "+---------EPOCH 5---------+\n",
      "Loss: \n",
      " 0.2537678148345916\n",
      "+---------EPOCH 1000---------+\n",
      "Loss: \n",
      " 0.11284325264785669\n",
      "+---------EPOCH 2000---------+\n",
      "Loss: \n",
      " 0.11016799791348786\n",
      "+---------EPOCH 3000---------+\n",
      "Loss: \n",
      " 0.10124714221423707\n",
      "+---------EPOCH 4000---------+\n",
      "Loss: \n",
      " 0.1011770248536761\n",
      "+---------EPOCH 5000---------+\n",
      "Loss: \n",
      " 0.10115736297390696\n",
      "+---------EPOCH 6000---------+\n",
      "Loss: \n",
      " 0.10114816527332042\n",
      "+---------EPOCH 7000---------+\n",
      "Loss: \n",
      " 0.10114284685639229\n",
      "+---------EPOCH 8000---------+\n",
      "Loss: \n",
      " 0.10113938474650397\n",
      "+---------EPOCH 9000---------+\n",
      "Loss: \n",
      " 0.10113695284348583\n",
      "+---------EPOCH 10000---------+\n",
      "Loss: \n",
      " 0.10113515142281249\n"
     ]
    }
   ],
   "source": [
    "# Build a simple multi layer network with one node/input in the first hidden layer and an output layer to convert back to single probability of ate/not ate\n",
    "multi_layer =(\n",
    "    ('input', X),  # row 0 must be input\n",
    "    ('hidden_1', (X.shape[1], 2), 'simple random'),  # shape of first layer of (input_dim, #nodes)\n",
    "    ('hidden_2', (2, 2), 'simple random'),\n",
    "    ('output', (2, 1), 'simple random'),\n",
    "    ('target', y) # last row in description must be the target vector\n",
    ")\n",
    "\n",
    "nn_multi = NeuralNetwork(multi_layer)\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1, 5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "#         print('Weights: \\n', nn_multi.layers[1].weights)\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn_multi.feed_forward(X)))))\n",
    "    nn_multi.train(X, y, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.499946</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.947680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.499946</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.056076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_pred  y_test\n",
       "0  0.499946       1\n",
       "1  0.947680       1\n",
       "2  0.499946       1\n",
       "3  0.056076       0\n",
       "4  0.056416       0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compat = pd.DataFrame.from_dict({\n",
    "    'y_pred': nn_multi.feed_forward(X).reshape(1,-1)[0],\n",
    "    'y_test': y.reshape(1,-1)[0],\n",
    "})\n",
    "compat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7229"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check actual accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(nn_multi.feed_forward(X).round(), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9458"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can give a little more confidence manually with a stop function at ~50%\n",
    "\n",
    "def give_confidence(x):\n",
    "    if x > 0.45:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "compat.y_pred = compat.y_pred.apply(give_confidence)\n",
    "accuracy_score(compat.y_pred, compat.y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "source": [
    "**Boom!** ~95%.  For some reason, the network isn't able to push values further away from 0.5 for true positives.  Maybe more layers, bias optimization, or other techniques could help with that.  With a little confidence, this spunky network is doing ok!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "171   48    1   1       110   229    0        1      168      0      1.0   \n",
       "254   59    1   3       160   273    0        0      125      0      0.0   \n",
       "64    58    1   2       140   211    1        0      165      0      0.0   \n",
       "126   47    1   0       112   204    0        1      143      0      0.1   \n",
       "6     56    0   1       140   294    0        0      153      0      1.3   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "171      0   0     3       0  \n",
       "254      2   0     2       0  \n",
       "64       2   0     2       1  \n",
       "126      2   0     2       1  \n",
       "6        1   0     2       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Some of these variables are continuous, others categorical and already encoded.\n",
    "\n",
    "Summary:\n",
    "* Categorical: sex, cp, fbs, restecg, exang, slope, ca, thal\n",
    "* Continuous: everything else\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='target')\n",
    "y = np.array(df.target).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.235160</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.335616</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.412214</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.194064</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.717557</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.549618</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625954</td>\n",
       "      <td>0</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  sex  cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "171  0.395833    1   1  0.150943  0.235160    0        1  0.740458      0   \n",
       "254  0.625000    1   3  0.622642  0.335616    0        0  0.412214      0   \n",
       "64   0.604167    1   2  0.433962  0.194064    1        0  0.717557      0   \n",
       "126  0.375000    1   0  0.169811  0.178082    0        1  0.549618      0   \n",
       "6    0.562500    0   1  0.433962  0.383562    0        0  0.625954      0   \n",
       "\n",
       "      oldpeak  slope  ca  thal  \n",
       "171  0.161290      0   0     3  \n",
       "254  0.000000      2   0     2  \n",
       "64   0.000000      2   0     2  \n",
       "126  0.016129      2   0     2  \n",
       "6    0.209677      1   0     2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "categorical_vars = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "continuous_vars = list(set(X.columns) - set(categorical_vars))\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_transformed = X.copy()\n",
    "X_transformed[continuous_vars] = scaler.fit_transform(X_transformed[continuous_vars]) \n",
    "\n",
    "X_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_transformed to np.array\n",
    "X_transformed = np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/vincent-a-brandon/lambda-ds-424/runs/onyatt3i\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "            in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/vincent-a-brandon/lambda-ds-424/runs/onyatt3i"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "\n",
    "# Initialize WANDB\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"lambda-ds-424\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 303 samples\n",
      "Epoch 1/100\n",
      "303/303 [==============================] - 1s 2ms/sample - loss: 6.2166 - accuracy: 0.4554 - mean_pred: 0.0149\n",
      "Epoch 2/100\n",
      "303/303 [==============================] - 0s 144us/sample - loss: 1.0862 - accuracy: 0.3696 - mean_pred: 0.3458\n",
      "Epoch 3/100\n",
      "303/303 [==============================] - 0s 136us/sample - loss: 0.8461 - accuracy: 0.3069 - mean_pred: 0.5144\n",
      "Epoch 4/100\n",
      "303/303 [==============================] - 0s 131us/sample - loss: 0.8244 - accuracy: 0.3696 - mean_pred: 0.5606\n",
      "Epoch 5/100\n",
      "303/303 [==============================] - 0s 126us/sample - loss: 0.8013 - accuracy: 0.3630 - mean_pred: 0.5448\n",
      "Epoch 6/100\n",
      "303/303 [==============================] - 0s 133us/sample - loss: 0.7847 - accuracy: 0.3828 - mean_pred: 0.5315\n",
      "Epoch 7/100\n",
      "303/303 [==============================] - 0s 130us/sample - loss: 0.7693 - accuracy: 0.3993 - mean_pred: 0.5210\n",
      "Epoch 8/100\n",
      "303/303 [==============================] - 0s 123us/sample - loss: 0.7537 - accuracy: 0.4323 - mean_pred: 0.5175\n",
      "Epoch 9/100\n",
      "303/303 [==============================] - 0s 136us/sample - loss: 0.7395 - accuracy: 0.4851 - mean_pred: 0.5309\n",
      "Epoch 10/100\n",
      "303/303 [==============================] - 0s 131us/sample - loss: 0.7246 - accuracy: 0.5083 - mean_pred: 0.5290\n",
      "Epoch 11/100\n",
      "303/303 [==============================] - 0s 129us/sample - loss: 0.7102 - accuracy: 0.5413 - mean_pred: 0.5309\n",
      "Epoch 12/100\n",
      "303/303 [==============================] - 0s 137us/sample - loss: 0.6956 - accuracy: 0.5809 - mean_pred: 0.5403\n",
      "Epoch 13/100\n",
      "303/303 [==============================] - 0s 130us/sample - loss: 0.6804 - accuracy: 0.6139 - mean_pred: 0.5250\n",
      "Epoch 14/100\n",
      "303/303 [==============================] - 0s 137us/sample - loss: 0.6647 - accuracy: 0.6370 - mean_pred: 0.5294\n",
      "Epoch 15/100\n",
      "303/303 [==============================] - 0s 140us/sample - loss: 0.6494 - accuracy: 0.6568 - mean_pred: 0.5372\n",
      "Epoch 16/100\n",
      "303/303 [==============================] - 0s 183us/sample - loss: 0.6368 - accuracy: 0.6700 - mean_pred: 0.5235\n",
      "Epoch 17/100\n",
      "303/303 [==============================] - 0s 136us/sample - loss: 0.6210 - accuracy: 0.6898 - mean_pred: 0.5272\n",
      "Epoch 18/100\n",
      "303/303 [==============================] - 0s 135us/sample - loss: 0.6075 - accuracy: 0.7195 - mean_pred: 0.5216\n",
      "Epoch 19/100\n",
      "303/303 [==============================] - 0s 131us/sample - loss: 0.5915 - accuracy: 0.7261 - mean_pred: 0.5192\n",
      "Epoch 20/100\n",
      "303/303 [==============================] - 0s 129us/sample - loss: 0.5757 - accuracy: 0.7426 - mean_pred: 0.5171\n",
      "Epoch 21/100\n",
      "303/303 [==============================] - 0s 131us/sample - loss: 0.5618 - accuracy: 0.7624 - mean_pred: 0.5121\n",
      "Epoch 22/100\n",
      "303/303 [==============================] - 0s 132us/sample - loss: 0.5472 - accuracy: 0.7558 - mean_pred: 0.5007\n",
      "Epoch 23/100\n",
      "303/303 [==============================] - 0s 126us/sample - loss: 0.5341 - accuracy: 0.7657 - mean_pred: 0.5141\n",
      "Epoch 24/100\n",
      "303/303 [==============================] - 0s 139us/sample - loss: 0.5209 - accuracy: 0.7690 - mean_pred: 0.4896\n",
      "Epoch 25/100\n",
      "303/303 [==============================] - 0s 127us/sample - loss: 0.5070 - accuracy: 0.7822 - mean_pred: 0.5020\n",
      "Epoch 26/100\n",
      "303/303 [==============================] - 0s 127us/sample - loss: 0.4946 - accuracy: 0.7888 - mean_pred: 0.5118\n",
      "Epoch 27/100\n",
      "303/303 [==============================] - 0s 168us/sample - loss: 0.4832 - accuracy: 0.7987 - mean_pred: 0.5100\n",
      "Epoch 28/100\n",
      "303/303 [==============================] - 0s 143us/sample - loss: 0.4738 - accuracy: 0.7954 - mean_pred: 0.5073\n",
      "Epoch 29/100\n",
      "303/303 [==============================] - 0s 147us/sample - loss: 0.4648 - accuracy: 0.8020 - mean_pred: 0.5196\n",
      "Epoch 30/100\n",
      "303/303 [==============================] - 0s 149us/sample - loss: 0.4555 - accuracy: 0.8020 - mean_pred: 0.5202\n",
      "Epoch 31/100\n",
      "303/303 [==============================] - 0s 131us/sample - loss: 0.4492 - accuracy: 0.8053 - mean_pred: 0.5024\n",
      "Epoch 32/100\n",
      "303/303 [==============================] - 0s 130us/sample - loss: 0.4437 - accuracy: 0.8053 - mean_pred: 0.5339\n",
      "Epoch 33/100\n",
      "303/303 [==============================] - 0s 132us/sample - loss: 0.4381 - accuracy: 0.8053 - mean_pred: 0.5358\n",
      "Epoch 34/100\n",
      "303/303 [==============================] - 0s 133us/sample - loss: 0.4318 - accuracy: 0.8020 - mean_pred: 0.5137\n",
      "Epoch 35/100\n",
      "303/303 [==============================] - 0s 134us/sample - loss: 0.4276 - accuracy: 0.8086 - mean_pred: 0.5160\n",
      "Epoch 36/100\n",
      "303/303 [==============================] - 0s 150us/sample - loss: 0.4242 - accuracy: 0.8053 - mean_pred: 0.5071\n",
      "Epoch 37/100\n",
      "303/303 [==============================] - 0s 148us/sample - loss: 0.4169 - accuracy: 0.8086 - mean_pred: 0.5182\n",
      "Epoch 38/100\n",
      "303/303 [==============================] - 0s 157us/sample - loss: 0.4132 - accuracy: 0.8086 - mean_pred: 0.5073\n",
      "Epoch 39/100\n",
      "303/303 [==============================] - 0s 149us/sample - loss: 0.4133 - accuracy: 0.8020 - mean_pred: 0.5332\n",
      "Epoch 40/100\n",
      "303/303 [==============================] - 0s 143us/sample - loss: 0.4076 - accuracy: 0.8053 - mean_pred: 0.4833\n",
      "Epoch 41/100\n",
      "303/303 [==============================] - 0s 150us/sample - loss: 0.4053 - accuracy: 0.8086 - mean_pred: 0.5129\n",
      "Epoch 42/100\n",
      "303/303 [==============================] - 0s 146us/sample - loss: 0.4014 - accuracy: 0.8152 - mean_pred: 0.5172\n",
      "Epoch 43/100\n",
      "303/303 [==============================] - 0s 135us/sample - loss: 0.3993 - accuracy: 0.8218 - mean_pred: 0.5018\n",
      "Epoch 44/100\n",
      "303/303 [==============================] - 0s 132us/sample - loss: 0.3972 - accuracy: 0.8020 - mean_pred: 0.5448\n",
      "Epoch 45/100\n",
      "303/303 [==============================] - 0s 137us/sample - loss: 0.4033 - accuracy: 0.8218 - mean_pred: 0.4694\n",
      "Epoch 46/100\n",
      "303/303 [==============================] - 0s 136us/sample - loss: 0.3926 - accuracy: 0.8053 - mean_pred: 0.5446\n",
      "Epoch 47/100\n",
      "303/303 [==============================] - 0s 140us/sample - loss: 0.3919 - accuracy: 0.8152 - mean_pred: 0.5361\n",
      "Epoch 48/100\n",
      "303/303 [==============================] - 0s 128us/sample - loss: 0.3862 - accuracy: 0.8152 - mean_pred: 0.5181\n",
      "Epoch 49/100\n",
      "303/303 [==============================] - 0s 131us/sample - loss: 0.3973 - accuracy: 0.8119 - mean_pred: 0.5544\n",
      "Epoch 50/100\n",
      "303/303 [==============================] - 0s 131us/sample - loss: 0.4510 - accuracy: 0.8251 - mean_pred: 0.4224\n",
      "Epoch 51/100\n",
      "303/303 [==============================] - 0s 160us/sample - loss: 0.4366 - accuracy: 0.8152 - mean_pred: 0.4977\n",
      "Epoch 52/100\n",
      "303/303 [==============================] - 0s 137us/sample - loss: 0.3871 - accuracy: 0.8185 - mean_pred: 0.5730\n",
      "Epoch 53/100\n",
      "303/303 [==============================] - 0s 126us/sample - loss: 0.3797 - accuracy: 0.8218 - mean_pred: 0.5313\n",
      "Epoch 54/100\n",
      "303/303 [==============================] - 0s 127us/sample - loss: 0.3782 - accuracy: 0.8185 - mean_pred: 0.5185\n",
      "Epoch 55/100\n",
      "303/303 [==============================] - 0s 131us/sample - loss: 0.3791 - accuracy: 0.8185 - mean_pred: 0.5458\n",
      "Epoch 56/100\n",
      "303/303 [==============================] - 0s 126us/sample - loss: 0.3727 - accuracy: 0.8218 - mean_pred: 0.5429\n",
      "Epoch 57/100\n",
      "303/303 [==============================] - 0s 124us/sample - loss: 0.3770 - accuracy: 0.8185 - mean_pred: 0.4965\n",
      "Epoch 58/100\n",
      "303/303 [==============================] - 0s 130us/sample - loss: 0.3771 - accuracy: 0.8218 - mean_pred: 0.5404\n",
      "Epoch 59/100\n",
      "303/303 [==============================] - 0s 134us/sample - loss: 0.3746 - accuracy: 0.8152 - mean_pred: 0.5245\n",
      "Epoch 60/100\n",
      "303/303 [==============================] - 0s 131us/sample - loss: 0.3715 - accuracy: 0.8119 - mean_pred: 0.5252\n",
      "Epoch 61/100\n",
      "303/303 [==============================] - 0s 129us/sample - loss: 0.3759 - accuracy: 0.8317 - mean_pred: 0.5448\n",
      "Epoch 62/100\n",
      "303/303 [==============================] - 0s 129us/sample - loss: 0.3720 - accuracy: 0.8119 - mean_pred: 0.5267\n",
      "Epoch 63/100\n",
      "303/303 [==============================] - 0s 133us/sample - loss: 0.3697 - accuracy: 0.8284 - mean_pred: 0.5332\n",
      "Epoch 64/100\n",
      "303/303 [==============================] - 0s 141us/sample - loss: 0.3676 - accuracy: 0.8218 - mean_pred: 0.5312\n",
      "Epoch 65/100\n",
      "303/303 [==============================] - 0s 135us/sample - loss: 0.3676 - accuracy: 0.8284 - mean_pred: 0.5300\n",
      "Epoch 66/100\n",
      "303/303 [==============================] - 0s 129us/sample - loss: 0.3652 - accuracy: 0.8284 - mean_pred: 0.5397\n",
      "Epoch 67/100\n",
      "303/303 [==============================] - 0s 138us/sample - loss: 0.3710 - accuracy: 0.8152 - mean_pred: 0.5271\n",
      "Epoch 68/100\n",
      "303/303 [==============================] - 0s 134us/sample - loss: 0.3653 - accuracy: 0.8317 - mean_pred: 0.5431\n",
      "Epoch 69/100\n",
      "303/303 [==============================] - 0s 135us/sample - loss: 0.3639 - accuracy: 0.8284 - mean_pred: 0.5405\n",
      "Epoch 70/100\n",
      "303/303 [==============================] - 0s 142us/sample - loss: 0.3627 - accuracy: 0.8218 - mean_pred: 0.5238\n",
      "Epoch 71/100\n",
      "303/303 [==============================] - 0s 135us/sample - loss: 0.3621 - accuracy: 0.8218 - mean_pred: 0.5295\n",
      "Epoch 72/100\n",
      "303/303 [==============================] - 0s 136us/sample - loss: 0.3616 - accuracy: 0.8218 - mean_pred: 0.5297\n",
      "Epoch 73/100\n",
      "303/303 [==============================] - 0s 130us/sample - loss: 0.4081 - accuracy: 0.8383 - mean_pred: 0.5785\n",
      "Epoch 74/100\n",
      "303/303 [==============================] - 0s 132us/sample - loss: 0.4725 - accuracy: 0.7954 - mean_pred: 0.4115\n",
      "Epoch 75/100\n",
      "303/303 [==============================] - 0s 135us/sample - loss: 0.4699 - accuracy: 0.8086 - mean_pred: 0.5326\n",
      "Epoch 76/100\n",
      "303/303 [==============================] - 0s 133us/sample - loss: 0.3842 - accuracy: 0.8218 - mean_pred: 0.6166\n",
      "Epoch 77/100\n",
      "303/303 [==============================] - 0s 123us/sample - loss: 0.3667 - accuracy: 0.8449 - mean_pred: 0.5210\n",
      "Epoch 78/100\n",
      "303/303 [==============================] - 0s 131us/sample - loss: 0.3654 - accuracy: 0.8449 - mean_pred: 0.5305\n",
      "Epoch 79/100\n",
      "303/303 [==============================] - 0s 138us/sample - loss: 0.3603 - accuracy: 0.8449 - mean_pred: 0.5504\n",
      "Epoch 80/100\n",
      "303/303 [==============================] - 0s 128us/sample - loss: 0.3577 - accuracy: 0.8449 - mean_pred: 0.5425\n",
      "Epoch 81/100\n",
      "303/303 [==============================] - 0s 135us/sample - loss: 0.3573 - accuracy: 0.8416 - mean_pred: 0.5447\n",
      "Epoch 82/100\n",
      "303/303 [==============================] - 0s 132us/sample - loss: 0.3551 - accuracy: 0.8449 - mean_pred: 0.5350\n",
      "Epoch 83/100\n",
      "303/303 [==============================] - 0s 121us/sample - loss: 0.3687 - accuracy: 0.8383 - mean_pred: 0.5288\n",
      "Epoch 84/100\n",
      "303/303 [==============================] - 0s 137us/sample - loss: 0.4794 - accuracy: 0.7855 - mean_pred: 0.6180\n",
      "Epoch 85/100\n",
      "303/303 [==============================] - 0s 136us/sample - loss: 0.6235 - accuracy: 0.7624 - mean_pred: 0.7658\n",
      "Epoch 86/100\n",
      "303/303 [==============================] - 0s 135us/sample - loss: 0.4224 - accuracy: 0.8053 - mean_pred: 0.4750\n",
      "Epoch 87/100\n",
      "303/303 [==============================] - 0s 140us/sample - loss: 0.4724 - accuracy: 0.8218 - mean_pred: 0.4252\n",
      "Epoch 88/100\n",
      "303/303 [==============================] - 0s 132us/sample - loss: 0.3882 - accuracy: 0.8251 - mean_pred: 0.5585\n",
      "Epoch 89/100\n",
      "303/303 [==============================] - 0s 129us/sample - loss: 0.3882 - accuracy: 0.8119 - mean_pred: 0.6096\n",
      "Epoch 90/100\n",
      "303/303 [==============================] - 0s 127us/sample - loss: 0.3772 - accuracy: 0.8251 - mean_pred: 0.5663\n",
      "Epoch 91/100\n",
      "303/303 [==============================] - 0s 134us/sample - loss: 0.3732 - accuracy: 0.8383 - mean_pred: 0.5299\n",
      "Epoch 92/100\n",
      "303/303 [==============================] - 0s 129us/sample - loss: 0.3675 - accuracy: 0.8317 - mean_pred: 0.5379\n",
      "Epoch 93/100\n",
      "303/303 [==============================] - 0s 124us/sample - loss: 0.3641 - accuracy: 0.8350 - mean_pred: 0.5587\n",
      "Epoch 94/100\n",
      "303/303 [==============================] - 0s 134us/sample - loss: 0.3611 - accuracy: 0.8350 - mean_pred: 0.5574\n",
      "Epoch 95/100\n",
      "303/303 [==============================] - 0s 146us/sample - loss: 0.3593 - accuracy: 0.8350 - mean_pred: 0.5391\n",
      "Epoch 96/100\n",
      "303/303 [==============================] - 0s 143us/sample - loss: 0.3635 - accuracy: 0.8350 - mean_pred: 0.5663\n",
      "Epoch 97/100\n",
      "303/303 [==============================] - 0s 143us/sample - loss: 0.3544 - accuracy: 0.8416 - mean_pred: 0.5476\n",
      "Epoch 98/100\n",
      "303/303 [==============================] - 0s 139us/sample - loss: 0.3557 - accuracy: 0.8449 - mean_pred: 0.5355\n",
      "Epoch 99/100\n",
      "303/303 [==============================] - 0s 132us/sample - loss: 0.3524 - accuracy: 0.8416 - mean_pred: 0.5557\n",
      "Epoch 100/100\n",
      "303/303 [==============================] - 0s 137us/sample - loss: 0.3518 - accuracy: 0.8416 - mean_pred: 0.5480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f47345bc748>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Static Parameters\n",
    "inputs = X_transformed.shape[1]\n",
    "wandb.config.epochs = 100\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(13, input_shape=(inputs,)))\n",
    "model.add(Dense(26, activation='sigmoid'))\n",
    "model.add(Dense(13, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# Compile Model\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return keras.backend.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', mean_pred])\n",
    "\n",
    "# Fit Model\n",
    "\n",
    "model.fit(X_transformed, y, \n",
    "          epochs=wandb.config.epochs, \n",
    "          callbacks=[WandbCallback()],\n",
    "          verbose=1,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing randomsearchcv\n",
    "def build_keras_base(hidden_layers = [13, 26, 26], dropout_rate = 0,\n",
    "                     n_input = 13, n_class = 2):\n",
    "\n",
    "    model = Sequential()   \n",
    "    for index, layers in enumerate(hidden_layers):       \n",
    "        if not index:\n",
    "            # specify the input_dim to be the number of features for the first layer\n",
    "            model.add(Dense(layers, input_dim=n_input, activation='sigmoid'))\n",
    "        else:\n",
    "            model.add(Dense(layers, activation='sigmoid'))\n",
    "    \n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    \n",
    "    # the loss for binary and muti-class classification is different \n",
    "    loss = 'binary_crossentropy'\n",
    "    if n_class > 2:\n",
    "        loss = 'categorical_crossentropy'\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pass in fixed parameters n_input and n_class\n",
    "model_keras = keras.wrappers.scikit_learn.KerasClassifier(\n",
    "    build_fn = build_keras_base,\n",
    "    n_input = X_transformed.shape[1],\n",
    "    n_class = 1,\n",
    "    verbose = 0,\n",
    ")\n",
    "\n",
    "# random search's parameter:\n",
    "# specify the options and store them inside the dictionary\n",
    "hidden_layers_opts = [[13, 13, 13, 13], [32, 32, 32, 32, 32], [13, 26, 13], [26, 13, 19]]\n",
    "\n",
    "some_dict = {'epochs': 10}\n",
    "\n",
    "keras_param_options = {\n",
    "    'hidden_layers': hidden_layers_opts,\n",
    "    'epochs': wandb.config.epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7adb8bed6384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#           callbacks=[WandbCallback()],)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best score: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parameters:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_score'"
     ]
    }
   ],
   "source": [
    "# code courtesy of: https://ethen8181.github.io/machine-learning/keras/nn_keras_hyperparameter_tuning.html\n",
    "rs_model = RandomizedSearchCV(\n",
    "    model_keras,\n",
    "    param_distributions=keras_param_options,\n",
    "    n_iter=3,\n",
    "    cv=3,\n",
    "    n_jobs=1,\n",
    "    verbose=20\n",
    ")\n",
    "\n",
    "rs_model.fit(X=X_transformed, y=y)\n",
    "#           epochs=wandb.config.epochs, \n",
    "#           callbacks=[WandbCallback()],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.5445544719696045\n",
      "parameters:\n",
      "\thidden_layers: [13, 26, 13]\n",
      "\tepochs: 50\n"
     ]
    }
   ],
   "source": [
    "print('Best score: {0}'.format(rs_model.best_score_))\n",
    "print('parameters:')\n",
    "for param, value in rs_model.best_params_.items():\n",
    "    print('\\t{}: {}'.format(param, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
