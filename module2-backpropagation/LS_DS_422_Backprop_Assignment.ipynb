{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Backpropagation Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array((\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 0]\n",
    "))\n",
    "\n",
    "y = np.array((\n",
    "    [0], [1], [1], [1], [1], [0], [0]\n",
    "))\n",
    "\n",
    "display(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NeuralNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-53ea8d73d50b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test given network class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Number of Epochs / Iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NeuralNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "# Test given network class\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "def gen_random_matrix(shape):\n",
    "    return np.random.rand(shape[0], shape[1])\n",
    "\n",
    "# A lot of thought needs to go setting up the nodes.  Creating their size programatically\n",
    "#  makes a lot of assumptions about hidden layer size (could be random, arbitrary, etc.).\n",
    "#  One option would be to add a 'auto' feature that created hidden layers of X.shape + C\n",
    "#  weights. Another would be to create pass a distribution and have the layer be a generator \n",
    "#  of sorts whose parameters can be optimized.\n",
    "\n",
    "example_network_description = (\n",
    "    ('input', X),  # row 0 must be input\n",
    "    ('hidden_1', (X.shape[1], 3), 'simple random'),  # hidden vectors must match input vec\n",
    "    ('hidden_2', (3, 7), 'simple random'),\n",
    "    ('output', (7,1), 'simple random'),  # final active row must be output vector. match last hidden vec\n",
    "    ('target', y) # last row in description must be the target vector\n",
    ")\n",
    "\n",
    "\n",
    "class LayerFactory():\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def initialize_weights(self, shape, algorithm):\n",
    "        \"\"\"\n",
    "        Lookup available weight formulas and generate pseudo-random numbers for initial weights\n",
    "        of specified shape.\n",
    "        \n",
    "        \"\"\"\n",
    "        available_generators = {\n",
    "            'simple random': gen_random_matrix,\n",
    "        }\n",
    "        \n",
    "        return available_generators[algorithm](shape)\n",
    "    \n",
    "    def generate_layers(self, description):\n",
    "        \"\"\"\n",
    "        Generate layers based on network description.\n",
    "        \n",
    "        ====Parameters====\n",
    "        description: tuple or list object of layer descriptions ('name', shape)\n",
    "        \"\"\"\n",
    "        layers = {}\n",
    "        layers[0] = Layer()\n",
    "        layers[0].activated_values = description[0][1]\n",
    "        for count, row in enumerate(description):\n",
    "            if row[0] == 'target':\n",
    "                layers[count] = Layer()\n",
    "                layers[count].activated_values = description[len(description)-1][1]\n",
    "                \n",
    "            elif row[0] != 'input':\n",
    "                layers[count] = Layer()\n",
    "                layers[count].weights = self.initialize_weights(shape=row[1], algorithm=row[2])\n",
    "                layers[count].weighted_sum = 0\n",
    "                layers[count].activated_values = 0\n",
    "            \n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    \n",
    "class Layer():\n",
    "    pass\n",
    "    \n",
    "\n",
    "class NeuralNetwork(LayerFactory):\n",
    "    def __init__(self, description):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.description = description\n",
    "        self.layers = self.generate_layers(description)\n",
    "        \n",
    "    def sigmoid(self, weighted_sum):\n",
    "        return 1 / (1+np.exp(-weighted_sum))\n",
    "    \n",
    "    def sigmoidPrime(self, weighted_sum):\n",
    "        return weighted_sum * (1 - weighted_sum)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        for i in range(1, len(self.layers)-1):\n",
    "            # Weighted sum of inputs\n",
    "            #  Check if first layer (required to use feed_forward method as Predict)\n",
    "            if i == 1:\n",
    "                self.layers[i].weighted_sum = np.dot(X, self.layers[i].weights)\n",
    "                # Activated values (local outputs)\n",
    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
    "            else:\n",
    "                self.layers[i].weighted_sum = np.dot(self.layers[i-1].activated_values, self.layers[i].weights)\n",
    "                # Activated values (local outputs)\n",
    "                self.layers[i].activated_values = self.sigmoid(self.layers[i].weighted_sum)\n",
    "\n",
    "        return self.layers[len(self.layers)-2].activated_values\n",
    "        \n",
    "    def backward(self, X, y, net_output, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        # Step 1: Calculate errors and delta shifts for each layer (backward)\n",
    "        back_prop_pos = 0\n",
    "        for i in range(len(self.layers)-2, 0, -1):\n",
    "            # Error in local output\n",
    "            #   Check if first backprop\n",
    "            if back_prop_pos == 0:\n",
    "                self.layers[i].error = self.layers[i+1].activated_values - net_output\n",
    "                # Apply Derivative of Sigmoid to error\n",
    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(net_output)\n",
    "            else:\n",
    "                self.layers[i].error = self.layers[i+1].delta.dot(self.layers[i+1].weights.T)\n",
    "                # Apply Derivative of Sigmoid to error\n",
    "                self.layers[i].delta = self.layers[i].error * self.sigmoidPrime(\n",
    "                    self.layers[i].activated_values)*learning_rate\n",
    "                \n",
    "            back_prop_pos += 1\n",
    "            \n",
    "        # Step 2: Calculate adjustments and apply to each layer (forward)\n",
    "        for i in range(1, len(self.layers)-1):\n",
    "            self.layers[i].weights += self.layers[i-1].activated_values.T.dot(self.layers[i].delta)\n",
    "        \n",
    "    def train(self, X, y, learning_rate):\n",
    "        net_output = self.feed_forward(X)\n",
    "        self.backward(X, y, net_output, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1bdf468a0e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Testing block.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing block.\n",
    "for i in range(len(layers)-2, 0, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <__main__.Layer at 0x7f5a3cef6470>,\n",
       " 1: <__main__.Layer at 0x7f5a3cef66d8>,\n",
       " 2: <__main__.Layer at 0x7f5a3cef65c0>,\n",
       " 3: <__main__.Layer at 0x7f5a3cef6c18>,\n",
       " 4: <__main__.Layer at 0x7f5a3cef6630>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test LayerFactory\n",
    "factory = LayerFactory()\n",
    "layers = factory.generate_layers(description=example_network_description)\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62710878, 0.95262447, 0.80739135, 0.37436599, 0.42188375,\n",
       "        0.02954279, 0.76865005],\n",
       "       [0.778027  , 0.51607266, 0.87361569, 0.35606138, 0.00713691,\n",
       "        0.77170409, 0.34532938],\n",
       "       [0.10597956, 0.44483607, 0.55169943, 0.90002817, 0.33284319,\n",
       "        0.26650898, 0.40674867]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[0.90001542]\n",
      " [0.90943558]\n",
      " [0.90684305]\n",
      " [0.8997136 ]\n",
      " [0.89520821]\n",
      " [0.91293224]\n",
      " [0.88283682]]\n",
      "Loss: \n",
      " 0.35154181530121204\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[0.77460858]\n",
      " [0.78528419]\n",
      " [0.78241465]\n",
      " [0.77425127]\n",
      " [0.76964302]\n",
      " [0.78951931]\n",
      " [0.75671251]]\n",
      "Loss: \n",
      " 0.2847780148880342\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[0.52184464]\n",
      " [0.5233401 ]\n",
      " [0.52332309]\n",
      " [0.52171843]\n",
      " [0.52196283]\n",
      " [0.52432127]\n",
      " [0.52013619]]\n",
      "Loss: \n",
      " 0.24706780520505797\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[0.62776169]\n",
      " [0.63412271]\n",
      " [0.63262421]\n",
      " [0.62750785]\n",
      " [0.62533207]\n",
      " [0.63691294]\n",
      " [0.61787375]]\n",
      "Loss: \n",
      " 0.2470669193516853\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[0.52870734]\n",
      " [0.53053361]\n",
      " [0.53042199]\n",
      " [0.52857262]\n",
      " [0.52866415]\n",
      " [0.531642  ]\n",
      " [0.52644707]]\n",
      "Loss: \n",
      " 0.24637494842869323\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[0.57265846]\n",
      " [0.57510173]\n",
      " [0.57995036]\n",
      " [0.57328448]\n",
      " [0.58087405]\n",
      " [0.57863114]\n",
      " [0.56706002]]\n",
      "Loss: \n",
      " 0.24272024326609046\n",
      "+---------EPOCH 2000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[0.4040769 ]\n",
      " [0.59320448]\n",
      " [0.65186825]\n",
      " [0.61962146]\n",
      " [0.69956202]\n",
      " [0.69478392]\n",
      " [0.37953314]]\n",
      "Loss: \n",
      " 0.18738248179055378\n",
      "+---------EPOCH 3000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[0.10092761]\n",
      " [0.48940541]\n",
      " [0.56865713]\n",
      " [0.8634187 ]\n",
      " [0.8746409 ]\n",
      " [0.34653972]\n",
      " [0.0452725 ]]\n",
      "Loss: \n",
      " 0.08763694524949416\n",
      "+---------EPOCH 4000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[0.08091188]\n",
      " [0.8734761 ]\n",
      " [0.88304562]\n",
      " [0.97397462]\n",
      " [0.95350244]\n",
      " [0.1628851 ]\n",
      " [0.0017191 ]]\n",
      "Loss: \n",
      " 0.009372458605793227\n",
      "+---------EPOCH 5000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[4.30170919e-02]\n",
      " [9.31355568e-01]\n",
      " [9.38678693e-01]\n",
      " [9.83203035e-01]\n",
      " [9.66586221e-01]\n",
      " [8.92664319e-02]\n",
      " [4.02783372e-04]]\n",
      "Loss: \n",
      " 0.0028128725309214094\n",
      "+---------EPOCH 6000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[2.99132380e-02]\n",
      " [9.51022930e-01]\n",
      " [9.57444951e-01]\n",
      " [9.87180890e-01]\n",
      " [9.73258204e-01]\n",
      " [6.42122687e-02]\n",
      " [2.07938663e-04]]\n",
      "Loss: \n",
      " 0.0014438856107908897\n",
      "+---------EPOCH 7000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[2.34180313e-02]\n",
      " [9.60906009e-01]\n",
      " [9.66690859e-01]\n",
      " [9.89452644e-01]\n",
      " [9.77320645e-01]\n",
      " [5.15125696e-02]\n",
      " [1.38852037e-04]]\n",
      "Loss: \n",
      " 0.00092362959351063\n",
      "+---------EPOCH 8000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[1.95161844e-02]\n",
      " [9.66933204e-01]\n",
      " [9.72232462e-01]\n",
      " [9.90937205e-01]\n",
      " [9.80075351e-01]\n",
      " [4.37160773e-02]\n",
      " [1.04445311e-04]]\n",
      "Loss: \n",
      " 0.0006622232624017801\n",
      "+---------EPOCH 9000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[1.68933903e-02]\n",
      " [9.71038848e-01]\n",
      " [9.75953927e-01]\n",
      " [9.91991258e-01]\n",
      " [9.82082707e-01]\n",
      " [3.83793639e-02]\n",
      " [8.40121026e-05]]\n",
      "Loss: \n",
      " 0.0005086429369579763\n",
      "+---------EPOCH 10000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Predicted Output: \n",
      " [[1.49971830e-02]\n",
      " [9.74041114e-01]\n",
      " [9.78643390e-01]\n",
      " [9.92783070e-01]\n",
      " [9.83621338e-01]\n",
      " [3.44625973e-02]\n",
      " [7.05106756e-05]]\n",
      "Loss: \n",
      " 0.00040898633108132074\n"
     ]
    }
   ],
   "source": [
    "# Ensure layers being created correctly in NN\n",
    "nn = NeuralNetwork(description=example_network_description)\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57969291, 0.88623463, 0.82354341, 0.28966033, 0.25362587,\n",
       "        0.90120771, 0.7845741 ],\n",
       "       [0.67906782, 0.29905237, 0.27333864, 0.60150628, 0.25300745,\n",
       "        0.6969853 , 0.02095351],\n",
       "       [0.16736435, 0.54543392, 0.53045375, 0.86785617, 0.26756957,\n",
       "        0.06520021, 0.35142303]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.layers[1].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Flexible Perceptron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE ABOVE!! WOOT!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n",
    "\n",
    "\n",
    "### Parts\n",
    "1. Gathering & Transforming the Data\n",
    "2. Making MNIST a Binary Problem\n",
    "3. Estimating your Neural Network (the part you focus on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering the Data \n",
    "\n",
    "`keras` has a handy method to pull the mnist dataset for you. You'll notice that each observation is a 28x28 arrary which represents an image. Although most Neural Network frameworks can handle higher dimensional data, that is more overhead than necessary for us. We need to flatten the image to one long row which will be 784 values (28X28). Basically, you will be appending each row to one another to make on really long row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], img_rows * img_cols)\n",
    "X_test = X_test.reshape(x_test.shape[0], img_rows * img_cols)\n",
    "\n",
    "# Normalize Our Data\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the data should be in a format you're more familiar with\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making MNIST a Binary Problem \n",
    "MNIST is multiclass classification problem; however we haven't covered all the necessary techniques to handle this yet. You would need to one-hot encode the target, use a different loss metric, and use softmax activations for the last layer. This is all stuff we'll cover later this week, but let us simply the problem for now: Zero or all else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_temp = np.zeros(y_train.shape)\n",
    "y_temp[np.where(y_train == 0.0)[0]] = 1\n",
    "y_train = y_temp\n",
    "\n",
    "y_temp = np.zeros(y_test.shape)\n",
    "y_temp[np.where(y_test == 0.0)[0]] = 1\n",
    "y_test = y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Nice Binary target for ya to work with\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
       "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
       "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
       "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
       "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
       "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
       "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
       "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
       "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
       "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
       "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
       "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
       "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  775  776  777  778  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   779  780  781  782  783  target  \n",
       "0  0.0  0.0  0.0  0.0  0.0     0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0     1.0  \n",
       "\n",
       "[2 rows x 785 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# concatenate the data\n",
    "df_train = pd.DataFrame(X_train)\n",
    "df_train['target'] = y_train\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11846.000000</td>\n",
       "      <td>11846.000000</td>\n",
       "      <td>11846.000000</td>\n",
       "      <td>11846.000000</td>\n",
       "      <td>11846.000000</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.0</td>\n",
       "      <td>11846.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.956863</td>\n",
       "      <td>0.243137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1        2        3        4        5        6        7  \\\n",
       "count  11846.0  11846.0  11846.0  11846.0  11846.0  11846.0  11846.0  11846.0   \n",
       "mean       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "             8        9  ...           775           776           777  \\\n",
       "count  11846.0  11846.0  ...  11846.000000  11846.000000  11846.000000   \n",
       "mean       0.0      0.0  ...      0.000185      0.000030      0.000014   \n",
       "std        0.0      0.0  ...      0.011510      0.003279      0.001549   \n",
       "min        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
       "25%        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
       "50%        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
       "75%        0.0      0.0  ...      0.000000      0.000000      0.000000   \n",
       "max        0.0      0.0  ...      0.996078      0.356863      0.168627   \n",
       "\n",
       "                778           779      780      781      782      783  \\\n",
       "count  11846.000000  11846.000000  11846.0  11846.0  11846.0  11846.0   \n",
       "mean       0.000081      0.000021      0.0      0.0      0.0      0.0   \n",
       "std        0.008792      0.002234      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
       "25%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
       "50%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
       "75%        0.000000      0.000000      0.0      0.0      0.0      0.0   \n",
       "max        0.956863      0.243137      0.0      0.0      0.0      0.0   \n",
       "\n",
       "             target  \n",
       "count  11846.000000  \n",
       "mean       0.500000  \n",
       "std        0.500021  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.500000  \n",
       "75%        1.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "# try to get a balanced sample\n",
    "\n",
    "# Separate majority/minority classes\n",
    "df_majority = df_train[df_train.target == 0]\n",
    "df_minority = df_train[df_train.target == 1]\n",
    "\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(\n",
    "                                    df_majority,\n",
    "                                    replace=False,\n",
    "                                    n_samples=5923,\n",
    "                                    random_state=42\n",
    "                                  )\n",
    "\n",
    "df_downsampled = pd.concat([\n",
    "    df_majority_downsampled,\n",
    "    df_minority\n",
    "])\n",
    "\n",
    "df_downsampled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Your  Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "\n",
    "# Use balanced training set\n",
    "X = df_downsampled.drop(columns='target')\n",
    "y = df_downsampled.target.to_numpy().reshape(-1,1)\n",
    "\n",
    "network_description = (\n",
    "    ('input', X),  # row 0 must be input\n",
    "    ('hidden_1', (X.shape[1], 768), 'simple random'),  # hidden vectors must match input vec\n",
    "    ('hidden_2', (768, 50), 'simple random'),\n",
    "    ('hidden_3', (50, 10), 'simple random'),\n",
    "    ('output', (10,1), 'simple random'),  # final active row must be output vector. match last hidden vec\n",
    "    ('target', y) # last row in description must be the target vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11846, 784)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(11846, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Loss: \n",
      " 0.4796212892485052\n",
      "+---------EPOCH 2---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 3---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 4---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 5---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 100---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 200---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 300---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 400---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 500---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 600---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 700---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 800---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 900---------+\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 1000---------+\n",
      "Loss: \n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork(description=network_description)\n",
    "# Number of Epochs / Iterations\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 100 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        loss = np.mean(np.square(y - network.feed_forward(X)))\n",
    "        losses.append(loss)\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - network.feed_forward(X)))))\n",
    "    network.train(X, y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(network.layers[len(network.layers)-1].activated_values == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate on test set\n",
    "np.mean(network.feed_forward(X_test) == y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09871666666666666"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the majority class?\n",
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's probably guessing everything is a one.  Would yield similar efficiency.  Does not\n",
    "#  Beat majority classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Make MNIST a multiclass problem using cross entropy & soft-max\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
